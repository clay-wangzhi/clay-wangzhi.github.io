<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Nginx code 常用状态码学习小结]]></title>
    <url>%2F2019%2F03%2F20%2FNginx%20code%E5%B8%B8%E7%94%A8%E7%8A%B6%E6%80%81%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Nginx Code Status 状态码分为五类： 100-199 用于指定客户端应相应的某些动作。 200-299 用于表示请求成功。 300-399 用于已经移动的文件并且常被包含在定位头信息中指定新的地址信息。 400-499 用于指出客户端的错误。 500-599 用于指出服务器错误。 200 （成功） 服务器已成功处理了请求。 通常，这表示服务器提供了请求的网页。 201 （已创建） 请求成功并且服务器创建了新的资源。 202 （已接受） 服务器已接受请求，但尚未处理。 203 （非授权信息） 服务器已成功处理了请求，但返回的信息可能来自另一来源。 204 （无内容） 服务器成功处理了请求，但没有返回任何内容。 205 （重置内容） 服务器成功处理了请求，但没有返回任何内容。 206 （部分内容） 服务器成功处理了部分 GET 请求。 300 （多种选择） 针对请求，服务器可执行多种操作。 服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 301 （永久移动） 请求的网页已永久移动到新位置。 服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。 302 （临时移动） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 303 （查看其他位置） 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。 304 （未修改） 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。 305 （使用代理） 请求者只能使用代理访问请求的网页。 如果服务器返回此响应，还表示请求者应使用代理。 307 （临时重定向） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 400 （错误请求） 服务器不理解请求的语法。 401 （未授权） 请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 403 （禁止） 服务器拒绝请求。 404 （未找到） 服务器找不到请求的网页。 405 （方法禁用） 禁用请求中指定的方法。 406 （不接受） 无法使用请求的内容特性响应请求的网页。 407 （需要代理授权） 此状态代码与 401（未授权）类似，但指定请求者应当授权使用代理。 408 （请求超时） 服务器等候请求时发生超时。 409 （冲突） 服务器在完成请求时发生冲突。 服务器必须在响应中包含有关冲突的信息。 410 （已删除） 如果请求的资源已永久删除，服务器就会返回此响应。 411 （需要有效长度） 服务器不接受不含有效内容长度标头字段的请求。 412 （未满足前提条件） 服务器未满足请求者在请求中设置的其中一个前提条件。 413 （请求实体过大） 服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 414 （请求的 URI 过长） 请求的 URI（通常为网址）过长，服务器无法处理。 415 （不支持的媒体类型） 请求的格式不受请求页面的支持。 416 （请求范围不符合要求） 如果页面无法提供请求的范围，则服务器会返回此状态代码。 417 （未满足期望值） 服务器未满足&quot;期望&quot;请求标头字段的要求。 499 客户端主动断开了连接。 500 （服务器内部错误） 服务器遇到错误，无法完成请求。 501 （尚未实施） 服务器不具备完成请求的功能。 例如，服务器无法识别请求方法时可能会返回此代码。 502 （错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。 503 （服务不可用） 服务器目前无法使用（由于超载或停机维护）。 通常，这只是暂时状态。 504 （网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505 （HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。 http请求流程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu网络连接显示设备未托管（解决办法）]]></title>
    <url>%2F2018%2F10%2F16%2Fubuntu%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E6%98%BE%E7%A4%BA%E8%AE%BE%E5%A4%87%E6%9C%AA%E6%89%98%E7%AE%A1%EF%BC%88%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%EF%BC%89%2F</url>
    <content type="text"><![CDATA[转载自：https://blog.csdn.net/wildbison/article/details/6824024 ubuntu11.10 开机进入ubuntu系统后右上角的网络连接显示设备未托管网络不能使用了。 搜集网络资料解决如下： 修改文件： /etc/NetworkManager/NetworkManager.conf[ifupdown]managed=true 重启即可。。。。 原因： Linux里面有两套管理网络连接的方案： 1、/etc/network/interfaces（/etc/init.d/networking）2、Network-Manager 两套方案是冲突的，不能同时共存。第一个方案适用于没有X的环境，如：服务器；或者那些完全不需要改动连接的场合。第二套方案使用于有桌面的环境，特别是笔记本，搬来搬去，网络连接情况随时会变的。 －－－－－－－－－－－－－他们两个为了避免冲突，又能共享配置，就有了下面的解决方案：1、当Network-Manager发现/etc/network/interfaces被改动的时候，则关闭自己（显示为未托管），除非managed设置成真。2、当managed设置成真时，/etc/network/interfaces，则不生效。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（57）-wget]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8857%EF%BC%89-wget%2F</url>
    <content type="text"><![CDATA[Linux系统中的wget是一个下载文件的工具，它用在命令行下。对于Linux用户是必不可少的工具，我们经常要下载一些软件或从远程服务器恢复备份到本地服务器。wget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。 wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。 wget 非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。 语法1wget [参数] [URL地址] 参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485启动参数：-V, –version 显示wget的版本后退出-h, –help 打印语法帮助-b, –background 启动后转入后台执行-e, –execute=COMMAND 执行`.wgetrc’格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc记录和输入文件参数：-o, –output-file=FILE 把记录写到FILE文件中-a, –append-output=FILE 把记录追加到FILE文件中-d, –debug 打印调试输出-q, –quiet 安静模式(没有输出)-v, –verbose 冗长模式(这是缺省设置)-nv, –non-verbose 关掉冗长模式，但不是安静模式-i, –input-file=FILE 下载在FILE文件中出现的URLs-F, –force-html 把输入文件当作HTML格式文件对待-B, –base=URL 将URL作为在-F -i参数指定的文件中出现的相对链接的前缀–sslcertfile=FILE 可选客户端证书–sslcertkey=KEYFILE 可选客户端证书的KEYFILE–egd-file=FILE 指定EGD socket的文件名下载参数：–bind-address=ADDRESS 指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)-t, –tries=NUMBER 设定最大尝试链接次数(0 表示无限制).-O –output-document=FILE 把文档写到FILE文件中-nc, –no-clobber 不要覆盖存在的文件或使用.#前缀-c, –continue 接着下载没下载完的文件–progress=TYPE 设定进程条标记-N, –timestamping 不要重新下载文件除非比本地文件新-S, –server-response 打印服务器的回应–spider 不下载任何东西-T, –timeout=SECONDS 设定响应超时的秒数-w, –wait=SECONDS 两次尝试之间间隔SECONDS秒–waitretry=SECONDS 在重新链接之间等待1…SECONDS秒–random-wait 在下载之间等待0…2*WAIT秒-Y, –proxy=on/off 打开或关闭代理-Q, –quota=NUMBER 设置下载的容量限制–limit-rate=RATE 限定下载输率目录参数：-nd –no-directories 不创建目录-x, –force-directories 强制创建目录-nH, –no-host-directories 不创建主机目录-P, –directory-prefix=PREFIX 将文件保存到目录 PREFIX/…–cut-dirs=NUMBER 忽略 NUMBER层远程目录HTTP 选项参数：–http-user=USER 设定HTTP用户名为 USER.–http-passwd=PASS 设定http密码为 PASS-C, –cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许)-E, –html-extension 将所有text/html文档以.html扩展名保存–ignore-length 忽略 `Content-Length’头域–header=STRING 在headers中插入字符串 STRING–proxy-user=USER 设定代理的用户名为 USER–proxy-passwd=PASS 设定代理的密码为 PASS–referer=URL 在HTTP请求中包含 `Referer: URL’头-s, –save-headers 保存HTTP头到文件-U, –user-agent=AGENT 设定代理的名称为 AGENT而不是 Wget/VERSION–no-http-keep-alive 关闭 HTTP活动链接 (永远链接)–cookies=off 不使用 cookies–load-cookies=FILE 在开始会话前从文件 FILE中加载cookie–save-cookies=FILE 在会话结束后将 cookies保存到 FILE文件中FTP 选项参数：-nr, –dont-remove-listing 不移走 `.listing’文件-g, –glob=on/off 打开或关闭文件名的 globbing机制–passive-ftp 使用被动传输模式 (缺省值).–active-ftp 使用主动传输模式–retr-symlinks 在递归的时候，将链接指向文件(而不是目录)递归下载参数：-r, –recursive 递归下载－－慎用!-l, –level=NUMBER 最大递归深度 (inf 或 0 代表无穷)–delete-after 在现在完毕后局部删除文件-k, –convert-links 转换非相对链接为相对链接-K, –backup-converted 在转换文件X之前，将之备份为 X.orig-m, –mirror 等价于 -r -N -l inf -nr-p, –page-requisites 下载显示HTML文件的所有图片递归下载中的包含和不包含(accept/reject)：-A, –accept=LIST 分号分隔的被接受扩展名的列表-R, –reject=LIST 分号分隔的不被接受的扩展名的列表-D, –domains=LIST 分号分隔的被接受域的列表–exclude-domains=LIST 分号分隔的不被接受的域的列表–follow-ftp 跟踪HTML文档中的FTP链接–follow-tags=LIST 分号分隔的被跟踪的HTML标签的列表-G, –ignore-tags=LIST 分号分隔的被忽略的HTML标签的列表-H, –span-hosts 当递归时转到外部主机-L, –relative 仅仅跟踪相对链接-I, –include-directories=LIST 允许目录的列表-X, –exclude-directories=LIST 不被包含目录的列表-np, –no-parent 不要追溯到父目录wget -S –spider url 不下载只显示过程 功能用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget虽然功能强大，但是使用起来还是比较简单： 1）支持断点下传功能；这一点，也是网络蚂蚁和FlashGet当年最大的卖点，现在，Wget也可以使用此功能，那些网络不是太好的用户可以放心了； 2）同时支持FTP和HTTP下载方式；尽管现在大部分软件可以使用HTTP方式下载，但是，有些时候，仍然需要使用FTP方式下载软件； 3）支持代理服务器；对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能； 4）设置方便简单；可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标； 5）程序小，完全免费；程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。 实例使用wget下载单个文件 1wget http://www.linuxde.net/testfile.zip 以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。 下载并以不同的文件名保存 1wget -O wordpress.zip http://www.linuxde.net/download.aspx?id=1080 wget默认会以最后一个符合/的后面的字符来命令，对于动态链接的下载通常文件名会不正确。 错误：下面的例子会下载一个文件并以名称download.aspx?id=1080保存: 1wget http://www.linuxde.net/download?id=1 即使下载的文件是zip格式，它仍然以download.php?id=1080命令。 正确：为了解决这个问题，我们可以使用参数-O来指定一个文件名： 1wget -O wordpress.zip http://www.linuxde.net/download.aspx?id=1080 wget限速下载 1wget --limit-rate=300k http://www.linuxde.net/testfile.zip 当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。 使用wget断点续传 1wget -c http://www.linuxde.net/testfile.zip 使用wget -c重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c参数。 使用wget后台下载 1234wget -b http://www.linuxde.net/testfile.zipContinuing in background, pid 1840.Output will be written to `wget-log&apos;. 对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载，你可以使用以下命令来察看下载进度： 1tail -f wget-log 伪装代理名称下载 1wget --user-agent=&quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16&quot; http://www.linuxde.net/testfile.zip 有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过--user-agent参数伪装。 测试下载链接 当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加--spider参数进行检查。 1wget --spider URL 如果下载链接正确，将会显示: 12345Spider mode enabled. Check if remote file exists.HTTP request sent, awaiting response... 200 OKLength: unspecified [text/html]Remote file exists and could contain further links,but recursion is disabled -- not retrieving. 这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误: 1234wget --spider urlSpider mode enabled. Check if remote file exists.HTTP request sent, awaiting response... 404 Not FoundRemote file does not exist -- broken link!!! 你可以在以下几种情况下使用--spider参数： 定时下载之前进行检查 间隔检测网站是否可用 检查网站页面的死链接 增加重试次数 1wget --tries=40 URL 如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用--tries增加重试次数。 下载多个文件 1wget -i filelist.txt 首先，保存一份下载链接文件： 12345cat &gt; filelist.txturl1url2url3url4 接着使用这个文件和参数-i下载。 镜像网站 1wget --mirror -p --convert-links -P ./LOCAL URL 下载整个网站到本地。 --miror开户镜像下载。 -p下载所有为了html页面显示正常的文件。 --convert-links下载后，转换成本地的链接。 -P ./LOCAL保存所有文件和目录到本地指定目录。 过滤指定格式下载 1wget --reject=gif ur 下载一个网站，但你不希望下载图片，可以使用这条命令。 把下载信息存入日志文件 1wget -o download.log URL 不希望下载信息直接显示在终端而是在一个日志文件，可以使用。 限制总下载文件大小 1wget -Q5m -i filelist.txt 当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。 下载指定格式文件 1wget -r -A.pdf url 可以在以下情况使用该功能： 下载一个网站的所有图片。 下载一个网站的所有视频。 下载一个网站的所有PDF文件。 FTP下载 12wget ftp-urlwget --ftp-user=USERNAME --ftp-password=PASSWORD url 可以使用wget来完成ftp链接的下载。 使用wget匿名ftp下载： 1wget ftp-url 使用wget用户名和密码认证的ftp下载： 1wget --ftp-user=USERNAME --ftp-password=PASSWORD url 参考链接： http://www.cnblogs.com/peida/archive/2013/03/18/2965369.html http://man.linuxde.net/wget]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（56）-scp]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8856%EF%BC%89-scp%2F</url>
    <content type="text"><![CDATA[scp命令用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读read only system时，用scp可以帮你把文件移出来。另外，scp还非常不占资源，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 语法1scp(选项)(参数) 选项12345678910111213-1：使用ssh协议版本1；-2：使用ssh协议版本2；-4：使用ipv4；-6：使用ipv6；-B：以批处理模式运行；-C：使用压缩；-F：指定ssh配置文件；-l：指定宽带限制；-o：指定使用的ssh选项；-P：指定远程主机的端口号；-p：保留文件的最后修改时间，最后访问时间和权限模式；-q：不显示复制进度；-r：以递归方式复制。 参数 源文件：指定要复制的源文件。 目标文件：目标文件。格式为user@host：filename（文件名为目标文件的名称）。 实例从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。 从远处复制文件到本地目录 1scp root@10.10.10.10:/opt/soft/nginx-0.5.38.tar.gz /opt/soft/ 从10.10.10.10机器上的/opt/soft/的目录中下载nginx-0.5.38.tar.gz 文件到本地/opt/soft/目录中。 从远处复制到本地 1scp -r root@10.10.10.10:/opt/soft/mongodb /opt/soft/ 从10.10.10.10机器上的/opt/soft/中下载mongodb目录到本地的/opt/soft/目录来。 上传本地文件到远程机器指定目录 1scp /opt/soft/nginx-0.5.38.tar.gz root@10.10.10.10:/opt/soft/scptest 复制本地/opt/soft/目录下的文件nginx-0.5.38.tar.gz到远程机器10.10.10.10的opt/soft/scptest目录。 上传本地目录到远程机器指定目录 1scp -r /opt/soft/mongodb root@10.10.10.10:/opt/soft/scptest 上传本地目录/opt/soft/mongodb到远程机器10.10.10.10上/opt/soft/scptest的目录中去。 转载链接： http://man.linuxde.net/scp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（55）-telnet]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8855%EF%BC%89-telnet%2F</url>
    <content type="text"><![CDATA[telnet命令用于登录远程主机，对远程主机进行管理。telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式了。但仍然有很多别的系统可能采用了telnet方式来提供远程登录，因此弄清楚telnet客户端的使用方式仍是很有必要的。 语法1telnet(选项)(参数) 选项123456789101112131415161718-8：允许使用8位字符资料，包括输入与输出；-a：尝试自动登入远端系统；-b&lt;主机别名&gt;：使用别名指定远端主机名称；-c：不读取用户专属目录里的.telnetrc文件；-d：启动排错模式；-e&lt;脱离字符&gt;：设置脱离字符；-E：滤除脱离字符；-f：此参数的效果和指定&quot;-F&quot;参数相同；-F：使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机；-k&lt;域名&gt;：使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名；-K：不自动登入远端主机；-l&lt;用户名称&gt;：指定要登入远端主机的用户名称；-L：允许输出8位字符资料；-n&lt;记录文件&gt;：指定文件记录相关信息；-r：使用类似rlogin指令的用户界面；-S&lt;服务类型&gt;：设置telnet连线所需的ip TOS信息；-x：假设主机有支持数据加密的功能，就使用它；-X&lt;认证形态&gt;：关闭指定的认证形态。 参数 远程主机：指定要登录进行管理的远程主机； 端口：指定TELNET协议使用的端口号。 实例12345678910telnet 192.168.2.10Trying 192.168.2.10...Connected to 192.168.2.10 (192.168.2.10).Escape character is &apos;^]&apos;. localhost (Linux release 2.6.18-274.18.1.el5 #1 SMP Thu Feb 9 12:45:44 EST 2012) (1)login: rootPassword: Login incorrect 转载链接： http://man.linuxde.net/telnet]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（54）-netstat]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8854%EF%BC%89-netstat%2F</url>
    <content type="text"><![CDATA[用于显示各种网络相关信息，如网络连接，路由表，接口状态 (Interface Statistics)，masquerade 连接，多播成员 (Multicast Memberships) 等等。 语法netstat [-acCeFghilMnNoprstuvVwx] 选项1234567-a 或–all 显示所有连线中的Socket-c 或–continuous 持续列出网络状态-h 或–help 在线帮助-l 或–listening 显示监控中的服务器的Socket-n 或–numeric 不解析主机名-t 或–tcp 显示TCP 传输协议的连线状况-u或–udp 显示UDP传输协议的连线状况 注意：LISTEN和LISTENING的状态只有用-a或者-l才能看到 网络连接状态详解共有12中可能的状态，前面11种是按照TCP连接建立的三次握手和TCP连接断开的四次挥手过程来描述的。1)、LISTEN:首先服务端需要打开一个socket进行监听，状态为LISTEN./ The socket is listening for incoming connections. 侦听来自远方TCP端口的连接请求 / 2)、 SYN_SENT:客户端通过应用程序调用connect进行active open.于是客户端tcp发送一个SYN以请求建立一个连接.之后状态置为SYN_SENT./The socket is actively attempting to establish a connection. 在发送连接请求后等待匹配的连接请求 / 3)、 SYN_RECV:服务端应发出ACK确认客户端的 SYN,同时自己向客户端发送一个SYN. 之后状态置为SYN_RECV/ A connection request has been received from the network. 在收到和发送一个连接请求后等待对连接请求的确认 / 4)、ESTABLISHED: 代表一个打开的连接，双方可以进行或已经在数据交互了。/ The socket has an established connection. 代表一个打开的连接，数据可以传送给用户 / 5)、 FIN_WAIT1:主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态./ The socket is closed, and the connection is shutting down. 等待远程TCP的连接中断请求，或先前的连接中断请求的确认 / 6)、CLOSE_WAIT:被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序),并进入CLOSE_WAIT./ The remote end has shut down, waiting for the socket to close. 等待从本地用户发来的连接中断请求 / 7)、FIN_WAIT2:主动关闭端接到ACK后，就进入了 FIN-WAIT-2 ./ Connection is closed, and the socket is waiting for a shutdown from the remote end. 从远程TCP等待连接中断请求 / 8)、LAST_ACK:被动关闭端一段时间后，接收到文件结束符的应用程 序将调用CLOSE关闭连接。这导致它的TCP也发送一个 FIN,等待对方的ACK.就进入了LAST-ACK ./ The remote end has shut down, and the socket is closed. Waiting for acknowledgement. 等待原来发向远程TCP的连接中断请求的确认 / 9)、TIME_WAIT:在主动关闭端接收到FIN后，TCP 就发送ACK包，并进入TIME-WAIT状态。/ The socket is waiting after close to handle packets still in the network.等待足够的时间以确保远程TCP接收到连接中断请求的确认 / 10)、CLOSING: 比较少见./ Both sockets are shut down but we still don’t have all our data sent. 等待远程TCP对连接中断的确认 / 11)、CLOSED: 被动关闭端在接受到ACK包后，就进入了closed的状态。连接结束./ The socket is not being used. 没有任何连接状态 / 12)、UNKNOWN: 未知的Socket状态。/ The state of the socket is unknown. / SYN: (同步序列编号,Synchronize Sequence Numbers)该标志仅在三次握手建立TCP连接时有效。表示一个新的TCP连接请求。ACK: (确认编号,Acknowledgement Number)是对TCP请求的确认标志,同时提示对端系统已经成功接收所有数据。FIN: (结束标志,FINish)用来结束一个TCP回话.但对应端口仍处于开放状态,准备接收后续数据。 常用实例1） 查看TCP的连接状态 123netstat -natlp | awk &apos;&#123;print $6&#125;&apos; | sort | uniq -c |sort -rnnetstat -n | awk &apos;/^tcp/ &#123;++S[$NF]&#125;;END &#123;for(a in S) print a,S[a]&#125;&apos;netstat -n | awk &apos;/^tcp/ &#123;print $NF&#125;&apos; | sort |uniq -c | sort -rn 2） 查找请求数较多的前20个IP（常用于查找攻来源） 123netstat -anpl | grep 80 | grep tcp | awk &apos;&#123;print $5&#125;&apos; | awk -F: &apos;&#123;print $1&#125;&apos; | sort | uniq -c | sort -nr | head -20netstat -ant | awk &apos;/:80/&#123;split($5,ip,&quot;:&quot;)&apos;;++A[ip[1]]&#125;END&#123;for(i in A) print A[i],i&#125;&apos; | sort -rn |head -20 tcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F&quot;.&quot; &apos;&#123;print $1&quot;.&quot;$2&quot;.&quot;$3&quot;.&quot;$4&#125;&apos; | sort | uniq -c | sort -nr | head -20]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（53）-traceroute]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8853%EF%BC%89-traceroute%2F</url>
    <content type="text"><![CDATA[通过traceroute我们可以知道信息从你的计算机到互联网另一端的主机是走的什么路径。当然每次数据包由某一同样的出发点（source）到达某一同样的目的地(destination)走的路径可能会不一样，但基本上来说大部分时候所走的路由是相同的。linux系统中，我们称之为traceroute,在MS Windows中为tracert。 traceroute通过发送小的数据包到目的设备直到其返回，来测量其需要多长时间。一条路径上的每个设备traceroute要测3次。输出结果中包括每次测试的时间(ms)和设备的名称（如有的话）及其IP地址。 在大多数情况下，我们会在linux主机系统下，直接执行命令行： traceroute hostname 而在Windows系统下是执行tracert的命令： tracert hostname 语法traceroute(选项)(参数) 选项123456789101112131415-d：使用Socket层级的排错功能；-f&lt;存活数值&gt;：设置第一个检测数据包的存活数值TTL的大小；-F：设置勿离断位；-g&lt;网关&gt;：设置来源路由网关，最多可设置8个；-i&lt;网络界面&gt;：使用指定的网络界面送出数据包；-I：使用ICMP回应取代UDP资料信息；-m&lt;存活数值&gt;：设置检测数据包的最大存活数值TTL的大小；-n：直接使用IP地址而非主机名称；-p&lt;通信端口&gt;：设置UDP传输协议的通信端口；-r：忽略普通的Routing Table，直接将数据包送到远端主机上。-s&lt;来源地址&gt;：设置本地主机送出数据包的IP地址；-t&lt;服务类型&gt;：设置检测数据包的TOS数值；-v：详细显示指令的执行过程；-w&lt;超时秒数&gt;：设置等待远端主机回报的时间；-x：开启或关闭数据包的正确性检验。 参数主机：指定目的主机IP地址或主机名。 功能traceroute指令让你追踪网络数据包的路由途径，预设数据包大小是40Bytes，用户可另行设置。 具体参数格式：traceroute [-dFlnrvx][-f&lt;存活数值&gt;][-g&lt;网关&gt;…][-i&lt;网络界面&gt;][-m&lt;存活数值&gt;][-p&lt;通信端口&gt;][-s&lt;来源地址&gt;][-t&lt;服务类型&gt;][-w&lt;超时秒数&gt;][主机名称或IP地址][数据包大小] 常用实例1）traceroute 用法简单、最常用的用法 123456789101112# traceroute www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets 1 192.168.74.2 (192.168.74.2) 2.606 ms 2.771 ms 2.950 ms 2 211.151.56.57 (211.151.56.57) 0.596 ms 0.598 ms 0.591 ms 3 211.151.227.206 (211.151.227.206) 0.546 ms 0.544 ms 0.538 ms 4 210.77.139.145 (210.77.139.145) 0.710 ms 0.748 ms 0.801 ms 5 202.106.42.101 (202.106.42.101) 6.759 ms 6.945 ms 7.107 ms 6 61.148.154.97 (61.148.154.97) 718.908 ms * bt-228-025.bta.net.cn (202.106.228.25) 5.177 ms 7 124.65.58.213 (124.65.58.213) 4.343 ms 4.336 ms 4.367 ms 8 202.106.35.190 (202.106.35.190) 1.795 ms 61.148.156.138 (61.148.156.138) 1.899 ms 1.951 ms 9 * * *30 * * * 说明： 记录按序列号从1开始，每个纪录就是一跳 ，每跳表示一个网关，我们看到每行有三个时间，单位是 ms，其实就是-q的默认参数。探测数据包向每个网关发送三个数据包后，网关响应后返回的时间；如果您用 traceroute -q 4 www.58.com ，表示向每个网关发送4个数据包。 有时我们traceroute 一台主机时，会看到有一些行是以星号表示的。出现这样的情况，可能是防火墙封掉了ICMP的返回信息，所以我们得不到什么相关的数据包返回数据。 有时我们在某一网关处延时比较长，有可能是某台网关比较阻塞，也可能是物理设备本身的原因。当然如果某台DNS出现问题时，不能解析主机名、域名时，也会 有延时长的现象；您可以加-n 参数来避免DNS解析，以IP格式输出数据。 如果在局域网中的不同网段之间，我们可以通过traceroute 来排查问题所在，是主机的问题还是网关的问题。如果我们通过远程来访问某台服务器遇到问题时，我们用到traceroute 追踪数据包所经过的网关，提交IDC服务商，也有助于解决问题；但目前看来在国内解决这样的问题是比较困难的，就是我们发现问题所在，IDC服务商也不可能帮助我们解决。 2）跳数设置 123456789101112# traceroute -m 10 www.baidu.comtraceroute to www.baidu.com (61.135.169.105), 10 hops max, 40 byte packets 1 192.168.74.2 (192.168.74.2) 1.534 ms 1.775 ms 1.961 ms 2 211.151.56.1 (211.151.56.1) 0.508 ms 0.514 ms 0.507 ms 3 211.151.227.206 (211.151.227.206) 0.571 ms 0.558 ms 0.550 ms 4 210.77.139.145 (210.77.139.145) 0.708 ms 0.729 ms 0.785 ms 5 202.106.42.101 (202.106.42.101) 7.978 ms 8.155 ms 8.311 ms 6 bt-228-037.bta.net.cn (202.106.228.37) 772.460 ms bt-228-025.bta.net.cn (202.106.228.25) 2.152 ms 61.148.154.97 (61.148.154.97) 772.107 ms 7 124.65.58.221 (124.65.58.221) 4.875 ms 61.148.146.29 (61.148.146.29) 2.124 ms 124.65.58.221 (124.65.58.221) 4.854 ms 8 123.126.6.198 (123.126.6.198) 2.944 ms 61.148.156.6 (61.148.156.6) 3.505 ms 123.126.6.198 (123.126.6.198) 2.885 ms 9 * * *10 * * * 3）显示IP地址，不查主机名 123456789101112# traceroute -n www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets 1 211.151.74.2 5.430 ms 5.636 ms 5.802 ms 2 211.151.56.57 0.627 ms 0.625 ms 0.617 ms 3 211.151.227.206 0.575 ms 0.584 ms 0.576 ms 4 210.77.139.145 0.703 ms 0.754 ms 0.806 ms 5 202.106.42.101 23.683 ms 23.869 ms 23.998 ms 6 202.106.228.37 247.101 ms * * 7 61.148.146.29 5.256 ms 124.65.58.213 4.386 ms 4.373 ms 8 202.106.35.190 1.610 ms 61.148.156.138 1.786 ms 61.148.3.34 2.089 ms 9 * * *30 * * * 4）探测包使用的基本UDP端口设置6888 12345678# traceroute -p 6888 www.baidu.comtraceroute to www.baidu.com (220.181.111.147), 30 hops max, 40 byte packets 1 211.151.74.2 (211.151.74.2) 4.927 ms 5.121 ms 5.298 ms 2 211.151.56.1 (211.151.56.1) 0.500 ms 0.499 ms 0.509 ms 3 211.151.224.90 (211.151.224.90) 0.637 ms 0.631 ms 0.641 ms 4 * * * 5 220.181.70.98 (220.181.70.98) 5.050 ms 5.313 ms 5.596 ms 6 220.181.17.94 (220.181.17.94) 1.665 ms !X * * 5）把探测包的个数设置为值4 123456789101112# traceroute -q 4 www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets 1 211.151.74.2 (211.151.74.2) 40.633 ms 40.819 ms 41.004 ms 41.188 ms 2 211.151.56.57 (211.151.56.57) 0.637 ms 0.633 ms 0.627 ms 0.619 ms 3 211.151.227.206 (211.151.227.206) 0.505 ms 0.580 ms 0.571 ms 0.569 ms 4 210.77.139.145 (210.77.139.145) 0.753 ms 0.800 ms 0.853 ms 0.904 ms 5 202.106.42.101 (202.106.42.101) 7.449 ms 7.543 ms 7.738 ms 7.893 ms 6 61.148.154.97 (61.148.154.97) 316.817 ms bt-228-025.bta.net.cn (202.106.228.25) 3.695 ms 3.672 ms * 7 124.65.58.213 (124.65.58.213) 3.056 ms 2.993 ms 2.960 ms 61.148.146.29 (61.148.146.29) 2.837 ms 8 61.148.3.34 (61.148.3.34) 2.179 ms 2.295 ms 2.442 ms 202.106.35.190 (202.106.35.190) 7.136 ms 9 * * * *30 * * * * 6）绕过正常的路由表，直接发送到网络相连的主机 123# traceroute -r www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packetsconnect: 网络不可达 7）把对外发探测包的等待响应时间设置为3秒 123456789101112# traceroute -w 3 www.baidu.comtraceroute to www.baidu.com (61.135.169.105), 30 hops max, 40 byte packets 1 211.151.74.2 (211.151.74.2) 2.306 ms 2.469 ms 2.650 ms 2 211.151.56.1 (211.151.56.1) 0.621 ms 0.613 ms 0.603 ms 3 211.151.227.206 (211.151.227.206) 0.557 ms 0.560 ms 0.552 ms 4 210.77.139.145 (210.77.139.145) 0.708 ms 0.761 ms 0.817 ms 5 202.106.42.101 (202.106.42.101) 7.520 ms 7.774 ms 7.902 ms 6 bt-228-025.bta.net.cn (202.106.228.25) 2.890 ms 2.369 ms 61.148.154.97 (61.148.154.97) 471.961 ms 7 124.65.58.221 (124.65.58.221) 4.490 ms 4.483 ms 4.472 ms 8 123.126.6.198 (123.126.6.198) 2.948 ms 61.148.156.6 (61.148.156.6) 7.688 ms 7.756 ms 9 * * *30 * * * 说明： Traceroute的工作原理： Traceroute最简单的基本用法是：traceroute hostname Traceroute程序的设计是利用ICMP及IP header的TTL（Time To Live）栏位（field）。首先，traceroute送出一个TTL是1的IP datagram（其实，每次送出的为3个40字节的包，包括源地址，目的地址和包发出的时间标签）到目的地，当路径上的第一个路由器（router）收到这个datagram时，它将TTL减1。此时，TTL变为0了，所以该路由器会将此datagram丢掉，并送回一个「ICMP time exceeded」消息（包括发IP包的源地址，IP包的所有内容及路由器的IP地址），traceroute 收到这个消息后，便知道这个路由器存在于这个路径上，接着traceroute 再送出另一个TTL是2 的datagram，发现第2 个路由器…… traceroute 每次将送出的datagram的TTL 加1来发现另一个路由器，这个重复的动作一直持续到某个datagram 抵达目的地。当datagram到达目的地后，该主机并不会送回ICMP time exceeded消息，因为它已是目的地了，那么traceroute如何得知目的地到达了呢？ Traceroute在送出UDP datagrams到目的地时，它所选择送达的port number 是一个一般应用程序都不会用的号码（30000 以上），所以当此UDP datagram 到达目的地后该主机会送回一个「ICMP port unreachable」的消息，而当traceroute 收到这个消息时，便知道目的地已经到达了。所以traceroute 在Server端也是没有所谓的Daemon 程式。 Traceroute提取发 ICMP TTL到期消息设备的IP地址并作域名解析。每次 ，Traceroute都打印出一系列数据,包括所经过的路由设备的域名及 IP地址,三个包每次来回所花时间。 windows之tracert: 格式： ​ tracert [-d] [-h maximum_hops] [-j host-list] [-w timeout] target_name 参数说明： tracert [-d] [-h maximum_hops] [-j computer-list] [-w timeout] target_name 该诊断实用程序通过向目的地发送具有不同生存时间 (TL) 的 Internet 控制信息协议 (CMP) 回应报文，以确定至目的地的路由。路径上的每个路由器都要在转发该 ICMP 回应报文之前将其 TTL 值至少减 1，因此 TTL 是有效的跳转计数。当报文的 TTL 值减少到 0 时，路由器向源系统发回 ICMP 超时信息。通过发送 TTL 为 1 的第一个回应报文并且在随后的发送中每次将 TTL 值加 1，直到目标响应或达到最大 TTL 值，Tracert 可以确定路由。通过检查中间路由器发发回的 ICMP 超时 (ime Exceeded) 信息，可以确定路由器。注意，有些路由器“安静”地丢弃生存时间 (TLS) 过期的报文并且对 tracert 无效。 参数： -d 指定不对计算机名解析地址。 -h maximum_hops 指定查找目标的跳转的最大数目。 -jcomputer-list 指定在 computer-list 中松散源路由。 -w timeout 等待由 timeout 对每个应答指定的毫秒数。 target_name 目标计算机的名称。 实例： 1234567891011121314151617181920212223242526272829C:\Users\Administrator&gt;tracert www.58.comTracing route to www.58.com [221.187.111.30]over a maximum of 30 hops: 1 1 ms 1 ms 1 ms 10.58.156.1 2 1 ms &lt;1 ms &lt;1 ms 10.10.10.1 3 1 ms 1 ms 1 ms 211.103.193.129 4 2 ms 2 ms 2 ms 10.255.109.129 5 1 ms 1 ms 3 ms 124.205.98.205 6 2 ms 2 ms 2 ms 124.205.98.253 7 2 ms 6 ms 1 ms 202.99.1.125 8 5 ms 6 ms 5 ms 118.186.0.113 9 207 ms * * 118.186.0.106 10 8 ms 6 ms 11 ms 124.238.226.201 11 6 ms 7 ms 6 ms 219.148.19.177 12 12 ms 12 ms 16 ms 219.148.18.117 13 14 ms 17 ms 16 ms 219.148.19.125 14 13 ms 13 ms 12 ms 202.97.80.113 15 * * * Request timed out. 16 12 ms 12 ms 17 ms bj141-147-82.bjtelecom.net [219.141.147.82] 17 13 ms 13 ms 12 ms 202.97.48.2 18 * * * Request timed out. 19 14 ms 14 ms 12 ms 221.187.224.85 20 15 ms 13 ms 12 ms 221.187.104.2 21 * * * Request timed out. 22 15 ms 17 ms 18 ms 221.187.111.30Trace complete. 参考链接： http://www.cnblogs.com/peida/archive/2013/03/07/2947326.html http://man.linuxde.net/traceroute]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（52）-ping]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8852%EF%BC%89-ping%2F</url>
    <content type="text"><![CDATA[Linux系统的ping命令是常用的网络命令，它通常用来测试与目标主机的连通性，我们经常会说“ping一下某机器，看是不是开着”、不能打开网页时会说“你先ping网关地址192.168.1.1试试”。它通过发送ICMP ECHO_REQUEST数据包到网络主机（send ICMP ECHO_REQUEST to network hosts），并显示响应情况，这样我们就可以根据它输出的信息来确定目标主机是否可访问（但这不是绝对的）。有些服务器为了防止通过ping探测到，通过防火墙设置了禁止ping或者在内核参数中禁止ping，这样就不能通过ping确定该主机是否还处于开启状态。 linux下的ping和windows下的ping稍有区别,linux下ping不会自动终止,需要按ctrl+c终止或者用参数-c指定要求完成的回应次数。 语法ping(选项)(参数) 选项1234567891011121314-d：使用Socket的SO_DEBUG功能；-c&lt;完成次数&gt;：设置完成要求回应的次数；-f：极限检测；-i&lt;间隔秒数&gt;：指定收发信息的间隔时间；-I&lt;网络界面&gt;：使用指定的网络界面送出数据包；-l&lt;前置载入&gt;：设置在送出要求信息之前，先行发出的数据包；-n：只输出数值；-p&lt;范本样式&gt;：设置填满数据包的范本样式；-q：不显示指令执行过程，开头和结尾的相关信息除外；-r：忽略普通的Routing Table，直接将数据包送到远端主机上；-R：记录路由过程；-s&lt;数据包大小&gt;：设置数据包的大小；-t&lt;存活数值&gt;：设置存活数值TTL的大小；-v：详细显示指令的执行过程。 参数目的主机：指定发送ICMP报文的目的主机。 功能ping命令用于：确定网络和各外部主机的状态；跟踪和隔离硬件和软件问题；测试、评估和管理网络。如果主机正在运行并连在网上，它就对回送信号进行响应。每个回送信号请求包含一个网际协议（IP）和 ICMP 头，后面紧跟一个 tim 结构，以及来填写这个信息包的足够的字节。缺省情况是连续发送回送信号请求直到接收到中断信号（Ctrl-C）。 ping 命令每秒发送一个数据报并且为每个接收到的响应打印一行输出。ping 命令计算信号往返时间和(信息)包丢失情况的统计信息，并且在完成之后显示一个简要总结。ping 命令在程序超时或当接收到 SIGINT 信号时结束。Host 参数或者是一个有效的主机名或者是因特网地址。 常用实例1）ping的通的情况 1234567# ping 192.168.120.205PING 192.168.120.205 (192.168.120.205) 56(84) bytes of data.64 bytes from 192.168.120.205: icmp_seq=1 ttl=64 time=0.720 ms64 bytes from 192.168.120.205: icmp_seq=2 ttl=64 time=0.181 ms--- 192.168.120.205 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4000msrtt min/avg/max/mdev = 0.181/0.293/0.720/0.214 ms 2）ping不同的情况 1234567# ping 192.168.120.202PING 192.168.120.202 (192.168.120.202) 56(84) bytes of data.From 192.168.120.204 icmp_seq=1 Destination Host UnreachableFrom 192.168.120.204 icmp_seq=2 Destination Host Unreachable-- 192.168.120.202 ping statistics ---8 packets transmitted, 0 received, +6 errors, 100% packet loss, time 7005ms, pipe 4 3）ping指定次数 12345678910111213141516# ping -c 10 192.168.120.206PING 192.168.120.206 (192.168.120.206) 56(84) bytes of data.64 bytes from 192.168.120.206: icmp_seq=1 ttl=64 time=1.25 ms64 bytes from 192.168.120.206: icmp_seq=2 ttl=64 time=0.260 ms64 bytes from 192.168.120.206: icmp_seq=3 ttl=64 time=0.242 ms64 bytes from 192.168.120.206: icmp_seq=4 ttl=64 time=0.271 ms64 bytes from 192.168.120.206: icmp_seq=5 ttl=64 time=0.274 ms64 bytes from 192.168.120.206: icmp_seq=6 ttl=64 time=0.295 ms64 bytes from 192.168.120.206: icmp_seq=7 ttl=64 time=0.269 ms64 bytes from 192.168.120.206: icmp_seq=8 ttl=64 time=0.270 ms64 bytes from 192.168.120.206: icmp_seq=9 ttl=64 time=0.253 ms64 bytes from 192.168.120.206: icmp_seq=10 ttl=64 time=0.289 ms--- 192.168.120.206 ping statistics ---10 packets transmitted, 10 received, 0% packet loss, time 9000msrtt min/avg/max/mdev = 0.242/0.367/1.251/0.295 ms 4）时间间隔和次数限制的ping 12345678910111213141516# ping -c 10 -i 0.5 192.168.120.206PING 192.168.120.206 (192.168.120.206) 56(84) bytes of data.64 bytes from 192.168.120.206: icmp_seq=1 ttl=64 time=1.24 ms64 bytes from 192.168.120.206: icmp_seq=2 ttl=64 time=0.235 ms64 bytes from 192.168.120.206: icmp_seq=3 ttl=64 time=0.244 ms64 bytes from 192.168.120.206: icmp_seq=4 ttl=64 time=0.300 ms64 bytes from 192.168.120.206: icmp_seq=5 ttl=64 time=0.255 ms64 bytes from 192.168.120.206: icmp_seq=6 ttl=64 time=0.264 ms64 bytes from 192.168.120.206: icmp_seq=7 ttl=64 time=0.263 ms64 bytes from 192.168.120.206: icmp_seq=8 ttl=64 time=0.331 ms64 bytes from 192.168.120.206: icmp_seq=9 ttl=64 time=0.247 ms64 bytes from 192.168.120.206: icmp_seq=10 ttl=64 time=0.244 ms--- 192.168.120.206 ping statistics ---10 packets transmitted, 10 received, 0% packet loss, time 4499msrtt min/avg/max/mdev = 0.235/0.362/1.241/0.294 ms 5）多参数使用 123456789101112131415161718192021# ping -i 3 -s 1024 -t 255 192.168.120.206PING 192.168.120.206 (192.168.120.206) 1024(1052) bytes of data.1032 bytes from 192.168.120.206: icmp_seq=1 ttl=64 time=1.99 ms1032 bytes from 192.168.120.206: icmp_seq=2 ttl=64 time=0.694 ms1032 bytes from 192.168.120.206: icmp_seq=3 ttl=64 time=0.300 ms1032 bytes from 192.168.120.206: icmp_seq=4 ttl=64 time=0.481 ms1032 bytes from 192.168.120.206: icmp_seq=5 ttl=64 time=0.415 ms1032 bytes from 192.168.120.206: icmp_seq=6 ttl=64 time=0.600 ms1032 bytes from 192.168.120.206: icmp_seq=7 ttl=64 time=0.411 ms1032 bytes from 192.168.120.206: icmp_seq=8 ttl=64 time=0.281 ms1032 bytes from 192.168.120.206: icmp_seq=9 ttl=64 time=0.318 ms1032 bytes from 192.168.120.206: icmp_seq=10 ttl=64 time=0.362 ms1032 bytes from 192.168.120.206: icmp_seq=11 ttl=64 time=0.408 ms1032 bytes from 192.168.120.206: icmp_seq=12 ttl=64 time=0.445 ms1032 bytes from 192.168.120.206: icmp_seq=13 ttl=64 time=0.397 ms1032 bytes from 192.168.120.206: icmp_seq=14 ttl=64 time=0.406 ms1032 bytes from 192.168.120.206: icmp_seq=15 ttl=64 time=0.458 ms--- 192.168.120.206 ping statistics ---15 packets transmitted, 15 received, 0% packet loss, time 41999msrtt min/avg/max/mdev = 0.281/0.531/1.993/0.404 ms 说明： -i 3 发送周期为 3秒 -s 设置发送包的大小为1024 -t 设置TTL值为 255 参考链接： http://man.linuxde.net/ping http://www.cnblogs.com/peida/archive/2013/03/06/2945407.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（51）-ldd]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8851%EF%BC%89-ldd%2F</url>
    <content type="text"><![CDATA[ldd用于打印程序或者库文件所依赖的共享库列表。 语法ldd(选项)(参数) 选项123456--version：打印指令版本号；-v：详细信息模式，打印所有相关信息；-u：打印未使用的直接依赖；-d：执行重定位和报告任何丢失的对象；-r：执行数据对象和函数的重定位，并且报告任何丢失的对象和函数；--help：显示帮助信息。 参数文件：指定可执行程序或者文库。 其他1) ldd不是一个可执行程序，而只是一个shell脚本 ldd能够显示可执行模块的dependency(所属)(所属)，其原理是通过设置一系列的环境变量，如下：LD_TRACE_LOADED_OBJECTS、LD_WARN、LD_BIND_NOW、LD_LIBRARY_VERSION、LD_VERBOSE等。当LD_TRACE_LOADED_OBJECTS环境变量不为空时，任何可执行程序在运行时，它都会只显示模块的dependency(所属)，而程序并不真正执行。要不你可以在shell终端测试一下，如下： export LD_TRACE_LOADED_OBJECTS=1 再执行任何的程序，如ls等，看看程序的运行结果。 2) ldd显示可执行模块的dependency(所属)的工作原理，其实质是通过ld-linux.so（elf动态库的装载器）来实现的。我们知道，ld-linux.so模块会先于executable模块程序工作，并获得控制权，因此当上述的那些环境变量被设置时，ld-linux.so选择了显示可执行模块的dependency(所属)。 实际上可以直接执行ld-linux.so模块，如：/lib/ld-linux.so.2 –list program（这相当于ldd program）。 转载链接： https://www.cnblogs.com/Spiro-K/p/6378576.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（50）-ifconfig]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8850%EF%BC%89-ifconfig%2F</url>
    <content type="text"><![CDATA[许多windows非常熟悉ipconfig命令行工具，它被用来获取网络接口配置信息并对此进行修改。Linux系统拥有一个类似的工具，也就是ifconfig(interfaces config)。通常需要以root身份登录或使用sudo以便在Linux机器上使用ifconfig工具。依赖于ifconfig命令中使用一些选项属性，ifconfig工具不仅可以被用来简单地获取网络接口配置信息，还可以修改这些配置。 语法ifconfig(参数) 参数123456789101112131415161718add&lt;地址&gt;：设置网络设备IPv6的ip地址；del&lt;地址&gt;：删除网络设备IPv6的IP地址；down：关闭指定的网络设备；&lt;hw&lt;网络设备类型&gt;&lt;硬件地址&gt;：设置网络设备的类型与硬件地址；io_addr&lt;I/O地址&gt;：设置网络设备的I/O地址；irq&lt;IRQ地址&gt;：设置网络设备的IRQ；media&lt;网络媒介类型&gt;：设置网络设备的媒介类型；mem_start&lt;内存地址&gt;：设置网络设备在主内存所占用的起始地址；metric&lt;数目&gt;：指定在计算数据包的转送次数时，所要加上的数目；mtu&lt;字节&gt;：设置网络设备的MTU；netmask&lt;子网掩码&gt;：设置网络设备的子网掩码；tunnel&lt;地址&gt;：建立IPv4与IPv6之间的隧道通信地址；up：启动指定的网络设备；-broadcast&lt;地址&gt;：将要送往指定地址的数据包当成广播数据包来处理；-pointopoint&lt;地址&gt;：与指定地址的网络设备建立直接连线，此模式具有保密功能；-promisc：关闭或启动指定网络设备的promiscuous模式；IP地址：指定网络设备的IP地址；网络设备：指定网络设备的名称。 常用实例1）显示网络设备信息（激活状态的） 12345678910111213141516# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:20 inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB) 说明： eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是 00:50:56:BF:26:20 inet addr 用来表示网卡的IP地址，此网卡的 IP地址是 192.168.120.204，广播地址， Bcast:192.168.120.255，掩码地址Mask:255.255.255.0 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 HTTPD服务器的指定到回坏地址，在浏览器输入 127.0.0.1 就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址） 第二行：网卡的IP地址、子网、掩码 第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节 第四、五行：接收、发送数据包情况统计 第七行：接收、发送数据字节数统计信息。 2）启动关闭指定网卡 12# ifconfig eth0 up# ifconfig eth0 down 说明： ifconfig eth0 up 为启动网卡eth0 ；ifconfig eth0 down 为关闭网卡eth0。ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 3） 为网卡配置和删除IPv6地址： 12# ifconfig eth0 add 33ffe:3240:800:1005::2/64 #为网卡eth0配置IPv6地址# ifconfig eth0 del 33ffe:3240:800:1005::2/64 #为网卡eth0删除IPv6地址 4） 用ifconfig修改MAC地址： 1ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE 5）配置IP地址 123# ifconfig eth0 192.168.120.56 # ifconfig eth0 192.168.120.56 netmask 255.255.255.0 # ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255 说明： ifconfig eth0 192.168.120.56 给eth0网卡配置IP地：192.168.120.56 ifconfig eth0 192.168.120.56 netmask 255.255.255.0 给eth0网卡配置IP地址：192.168.120.56 ，并加上子掩码：255.255.255.0 ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255 /给eth0网卡配置IP地址：192.168.120.56，加上子掩码：255.255.255.0，加上个广播地址： 192.168.120.255 6）启用和关闭ARP协议 12# ifconfig eth0 arp # ifconfig eth0 -arp 7）设置最大传输单元 1# ifconfig eth0 mtu 1500 备注：用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。 参考链接： http://www.cnblogs.com/peida/archive/2013/02/27/2934525.html https://man.linuxde.net/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（49）-lsof]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8849%EF%BC%89-lsof%2F</url>
    <content type="text"><![CDATA[lsof（list open files）是一个列出当前系统打开文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。 语法lsof(选项) 选项123456789101112-a：列出打开文件存在的进程；-c&lt;进程名&gt;：列出指定进程所打开的文件；-g：列出GID号进程详情；-d&lt;文件号&gt;：列出占用该文件号的进程；+d&lt;目录&gt;：列出目录下被打开的文件；+D&lt;目录&gt;：递归列出目录下被打开的文件；-n&lt;目录&gt;：列出使用NFS的文件；-i&lt;条件&gt;：列出符合条件的进程。（4、6、协议、:端口、 @ip ）-p&lt;进程号&gt;：列出指定进程号所打开的文件；-u：列出UID号进程详情；-h：显示帮助信息；-v：显示版本信息。 命令功能用于查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP)。找回/恢复删除的文件。是十分方便的系统监视工具，因为 lsof 需要访问核心内存和各种文件，所以需要root用户执行。 lsof打开的文件可以是： 1.普通文件 2.目录 3.网络文件系统的文件 4.字符或设备文件 5.(函数)共享库 6.管道，命名管道 7.符号链接 8.网络文件（例如：NFS file、网络socket，unix域名socket） 9.还有其它类型的文件，等等 常用实例1）无任何参数 123456789101112# lsofcommand PID USER FD type DEVICE SIZE NODE NAMEinit 1 root cwd DIR 8,2 4096 2 /init 1 root rtd DIR 8,2 4096 2 /init 1 root txt REG 8,2 43496 6121706 /sbin/initinit 1 root mem REG 8,2 143600 7823908 /lib64/ld-2.5.soinit 1 root mem REG 8,2 1722304 7823915 /lib64/libc-2.5.soinit 1 root mem REG 8,2 23360 7823919 /lib64/libdl-2.5.soinit 1 root mem REG 8,2 95464 7824116 /lib64/libselinux.so.1init 1 root mem REG 8,2 247496 7823947 /lib64/libsepol.so.1init 1 root 10u FIFO 0,17 1233 /dev/initctlmigration 2 root cwd DIR 8,2 4096 2 / 说明： lsof输出各列信息的意义如下： COMMAND：进程的名称 PID：进程标识符 PPID：父进程标识符（需要指定-R参数） USER：进程所有者 PGID：进程所属组 FD：文件描述符，应用程序通过文件描述符识别该文件。 文件描述符列表： cwd：表示current work dirctory，即：应用程序的当前工作目录，这是该应用程序启动的目录，除非它本身对这个目录进行更改 txt：该类型的文件是程序代码，如应用程序二进制文件本身或共享库，如上列表中显示的 /sbin/init 程序 lnn：library references (AIX); er：FD information error (see NAME column); jld：jail directory (FreeBSD); ltx：shared library text (code and data); mxx ：hex memory-mapped type number xx. m86：DOS Merge mapped file; mem：memory-mapped file; mmap：memory-mapped device; pd：parent directory; rtd：root directory; tr：kernel trace file (OpenBSD); v86 VP/ix mapped file; 0：表示标准输出 1：表示标准输入 2：表示标准错误 一般在标准输出、标准错误、标准输入后还跟着文件状态模式： u：表示该文件被打开并处于读取/写入模式。 r：表示该文件被打开并处于只读模式。 w：表示该文件被打开并处于。 空格：表示该文件的状态模式为unknow，且没有锁定。 -：表示该文件的状态模式为unknow，且被锁定。 同时在文件状态模式后面，还跟着相关的锁： N：for a Solaris NFS lock of unknown type; r：for read lock on part of the file; R：for a read lock on the entire file; w：for a write lock on part of the file;（文件的部分写锁） W：for a write lock on the entire file;（整个文件的写锁） u：for a read and write lock of any length; U：for a lock of unknown type; x：for an SCO OpenServer Xenix lock on part of the file; X：for an SCO OpenServer Xenix lock on the entire file; space：if there is no lock. 文件类型： DIR：表示目录。 CHR：表示字符类型。 BLK：块设备类型。 UNIX： UNIX 域套接字。 FIFO：先进先出 (FIFO) 队列。 IPv4：网际协议 (IP) 套接字。 DEVICE：指定磁盘的名称 SIZE：文件的大小 NODE：索引节点（文件在磁盘上的标识） NAME：打开文件的确切名称 2）查看谁正在使用某个文件，也就是说查找某个文件相关的进程 1234# lsof /bin/bashCOMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEbash 24159 root txt REG 8,2 801528 5368780 /bin/bashbash 24909 root txt REG 8,2 801528 5368780 /bin/bash 3）递归查看某个目录的文件信息 1234# lsof test/test3COMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEbash 24941 root cwd DIR 8,2 4096 2258872 test/test3vi 24976 root cwd DIR 8,2 4096 2258872 test/test3 说明： 使用了+D，对应目录下的所有子目录和文件都会被列出 4）不使用+D选项，遍历查看某个目录的所有文件信息的方法 1234# lsof |grep &apos;test/test3&apos;bash 24941 root cwd DIR 8,2 4096 2258872 /opt/soft/test/test3vi 24976 root cwd DIR 8,2 4096 2258872 /opt/soft/test/test3vi 24976 root 4u REG 8,2 12288 2258882 /opt/soft/test/test3/.log2013.log.swp 5）列出某个用户打开的文件信息 1# lsof -u username 说明: -u 选项，u其实是user的缩写 6）列出某个程序进程所打开的文件信息 1# lsof -c mysql 说明: -c 选项将会列出所有以mysql这个进程开头的程序的文件，其实你也可以写成 lsof | grep mysql, 但是第一种方法明显比第二种方法要少打几个字符了 7）列出多个进程多个打开的文件信息 1# lsof -c mysql -c apache 8）列出某个用户以及某个进程所打开的文件信息 1# lsof -u test -c mysql 9）列出除了某个用户外的被打开的文件信息 1# lsof -u ^root 说明： ^这个符号在用户名之前，将会把是root用户打开的进程不让显示 10）通过某个进程号显示该进行打开的文件 1# lsof -p 1 11）列出多个进程号对应的文件信息 1# lsof -p 1,2,3 12）列出除了某个进程号，其他进程号所打开的文件信息 1# lsof -p ^1 13）列出所有的网络连接 1# lsof -i 14）列出所有tcp 网络连接信息 1# lsof -i tcp 15）列出所有udp网络连接信息 1# lsof -i udp 16）列出谁在使用某个端口 1# lsof -i :3306 17）列出谁在使用某个特定的udp端口 1# lsof -i udp:55 18）特定的tcp端口 1# lsof -i tcp:80 19）列出某个用户的所有活跃的网络端口 1# lsof -a -u test -i 20）列出所有网络文件系统 1# lsof -N 21）域名socket文件 1# lsof -u 22）某个用户组所打开的文件信息 1# lsof -g 5555 23）根据文件描述列出对应的文件信息 1234# lsof -d description(like 2)# lsof -d txt# lsof -d 1# lsof -d 2 说明： 0表示标准输入，1表示标准输出，2表示标准错误，从而可知：所以大多数应用程序所打开的文件的 FD 都是从 3 开始 24）根据文件描述范围列出文件信息 1# lsof -d 2-3 25）列出COMMAND列中包含字符串” sshd”，且文件描符的类型为txt的文件信息 1234# lsof -c sshd -a -d txtCOMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEsshd 2756 root txt REG 8,2 409488 1027867 /usr/sbin/sshdsshd 24155 root txt REG 8,2 409488 1027867 /usr/sbin/sshd 26）列出被进程号为1234的进程所打开的所有IPV4 network files 1# lsof -i 4 -a -p 1234 27）列出目前连接主机peida.linux上端口为：20，21，22，25，53，80相关的所有文件信息，且每隔3秒不断的执行lsof指令 1# lsof -i @peida.linux:20,21,22,25,53,80 -r 3 参考链接： http://www.cnblogs.com/peida/archive/2013/02/26/2932972.html https://man.linuxde.net/lsof]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（48）-crontab]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8848%EF%BC%89-crontab%2F</url>
    <content type="text"><![CDATA[crontab命令被用来提交和管理用户的需要周期性执行的任务，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 语法crontab(选项)(参数) 选项12345-e：编辑该用户的计时器设置;-l：列出该用户的计时器设置;-r：删除该用户的计时器设置;-u&lt;用户名称&gt;：指定要设定计时器的用户名称;-i：在删除用户的crontab文件时给确认提示。 参数crontab文件：指定包含待执行任务的crontab文件。 知识扩展Linux下的任务调度分为两类：系统任务调度和用户任务调度 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 /etc/crontab文件包括下面几行： 123456789SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=&quot;&quot;HOME=/# run-parts51 * * * * root run-parts /etc/cron.hourly24 7 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly 前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中。其文件名与用户名一致，使用者权限文件如下： 123/etc/cron.deny 该文件中所列用户不允许使用crontab命令/etc/cron.allow 该文件中所列用户允许使用crontab命令/var/spool/cron/ 所有用户crontab文件存放的目录,以用户名命名 crontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： 1minute hour day month week command 顺序：分 时 日 月 周 minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符： 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 查看crontab服务状态： 1service crond status 手动启动crontab服务： 1service crond start 查看crontab服务是否已设置为开机启动，执行命令： 1ntsysv 加入开机自动启动： 1chkconfig –level 35 crond on 常用方法1). 创建一个新的crontab文件 在考虑向cron进程提交一个crontab文件之前，首先要做的一件事情就是设置环境变量EDITOR。cron进程根据它来确定使用哪个编辑器编辑crontab文件。9 9 %的UNIX和LINUX用户都使用vi，如果你也是这样，那么你就编辑$ HOME目录下的. profile文件，在其中加入这样一行： EDITOR=vi; export EDITOR 然后保存并退出。不妨创建一个名为\ cron的文件，其中\是用户名，例如， davecron。在该文件中加入如下的内容。 ​ # (put your own initials here)echo the date to the console every ​ # 15minutes between 6pm and 6am ​ 0,15,30,45 18-06 * /bin/echo ‘date’ &gt; /dev/console ​ 保存并退出。确信前面5个域用空格分隔。 在上面的例子中，系统将每隔1 5分钟向控制台输出一次当前时间。如果系统崩溃或挂起，从最后所显示的时间就可以一眼看出系统是什么时间停止工作的。在有些系统中，用tty1来表示控制台，可以根据实际情况对上面的例子进行相应的修改。为了提交你刚刚创建的crontab文件，可以把这个新创建的文件作为cron命令的参数： ​ $ crontab davecron 现在该文件已经提交给cron进程，它将每隔1 5分钟运行一次。 同时，新创建文件的一个副本已经被放在/var/spool/cron目录中，文件名就是用户名(即dave)。 2). 列出crontab文件 为了列出crontab文件，可以用： ​ $ crontab -l ​ 0,15,30,45,18-06 * /bin/echo date &gt; dev/tty1 你将会看到和上面类似的内容。可以使用这种方法在$ H O M E目录中对crontab文件做一备份： ​ \$ crontab -l &gt; $HOME/mycron ​ 这样，一旦不小心误删了crontab文件，可以用上一节所讲述的方法迅速恢复。 3). 编辑crontab文件 如果希望添加、删除或编辑crontab文件中的条目，而E D I TO R环境变量又设置为v i，那么就可以用v i来编辑crontab文件，相应的命令为： ​ $ crontab -e 可以像使用v i编辑其他任何文件那样修改crontab文件并退出。如果修改了某些条目或添加了新的条目，那么在保存该文件时， c r o n会对其进行必要的完整性检查。如果其中的某个域出现了超出允许范围的值，它会提示你。 我们在编辑crontab文件时，没准会加入新的条目。例如，加入下面的一条： # DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month ​ 30 3 1,7,14,21,26 /bin/find -name “core’ -exec rm {} \; 现在保存并退出。最好在crontab文件的每一个条目之上加入一条注释，这样就可以知道它的功能、运行时间，更为重要的是，知道这是哪位用户的作业。 现在让我们使用前面讲过的crontab -l命令列出它的全部信息： $ crontab -l # (crondave installed on Tue May 4 13:07:43 1999) # DT:ech the date to the console every 30 minites 0,15,30,45 18-06 * * * /bin/echo `date` &gt; /dev/tty1 # DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month 30 3 1,7,14,21,26 * * /bin/find -name &quot;core&apos; -exec rm {} \; 4). 删除crontab文件 要删除crontab文件，可以用： $ crontab -r 5). 恢复丢失的crontab文件 如果不小心误删了crontab文件，假设你在自己的$ H O M E目录下还有一个备份，那么可以将其拷贝到/var/spool/cron/\，其中\是用户名。如果由于权限问题无法完成拷贝，可以用： ​ $ crontab \ ​ 其中，\是你在$ H O M E目录中副本的文件名。 我建议你在自己的$ H O M E目录中保存一个该文件的副本。我就有过类似的经历，有数次误删了crontab文件（因为r键紧挨在e键的右边）。这就是为什么有些系统文档建议不要直接编辑crontab文件，而是编辑该文件的一个副本，然后重新提交新的文件。 有些crontab的变体有些怪异，所以在使用crontab命令时要格外小心。如果遗漏了任何选项，crontab可能会打开一个空文件，或者看起来像是个空文件。这时敲delete键退出，不要按，否则你将丢失crontab文件。 常用实例1）每1分钟执行一次command 1* * * * * command 2）每小时的第3和第15分钟执行 13,15 * * * * command 3）在上午8点到11点的第3和第15分钟执行 13,15 8-11 * * * command 4）每隔两天的上午8点到11点的第3和第15分钟执行 13,15 8-11 */2 * * command 5）每个星期一的上午8点到11点的第3和第15分钟执行 13,15 8-11 * * 1 command 6）每晚的21:30重启smb 130 21 * * * /etc/init.d/smb restart 7）每月1、10、22日的4 : 45重启smb 145 4 1,10,22 * * /etc/init.d/smb restart 8）每周六、周日的1 : 10重启smb 110 1 * * 6,0 /etc/init.d/smb restart 9）每天18 : 00至23 : 00之间每隔30分钟重启smb 10,30 18-23 * * * /etc/init.d/smb restart 10）每星期六的晚上11 : 00 pm重启smb 10 23 * * 6 /etc/init.d/smb restart 11）每一小时重启smb 1* */1 * * * /etc/init.d/smb restart 12）晚上11点到早上7点之间，每隔一小时重启smb 1* 23-7/1 * * * /etc/init.d/smb restart 13）每月的4号与每周一到周三的11点重启smb 10 11 4 * mon-wed /etc/init.d/smb restart 14）一月一号的4点重启smb 10 4 1 jan * /etc/init.d/smb restart 15）每小时执行/etc/cron.hourly目录内的脚本 101 * * * * root run-parts /etc/cron.hourly 说明： run-parts这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是目录名了 使用注意事项有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。 在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点： 1）脚本中涉及文件路径时写全局路径； 2）脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如： cat start_cbp.sh #!/bin/sh source /etc/profile export RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf /usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 3）当手动执行脚本OK，但是crontab死活不执行时。这时必须大胆怀疑是环境变量惹的祸，并可以尝试在crontab中直接引入环境变量解决问题。如： 0 . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 注意清理系统用户的邮件日志 每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在crontab文件中设置如下形式，忽略日志输出： 0 /3 /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 “/dev/null 2&gt;&amp;1”表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。 . 系统级任务调度与用户级任务调度 系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过“crontab –uroot –e”来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。 其他注意事项 新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。 当crontab突然失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错tail -f /var/log/cron。 千万别乱运行crontab -r。它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。 在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\%，如经常用的date ‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+\%Y\%m\%d’。 参考链接： http://www.cnblogs.com/peida/archive/2013/01/08/2850483.html http://man.linuxde.net/crontab]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（47）-watch]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8847%EF%BC%89-watch%2F</url>
    <content type="text"><![CDATA[watch是一个非常实用的命令，基本所有的Linux发行版都带有这个小工具，如同名字一样，watch可以帮你监测一个命令的运行结果，省得你一遍遍的手动运行。在Linux下，watch是周期性的执行下个程序，并全屏显示执行结果。你可以拿他来监测你想要的一切命令的结果变化，比如 tail 一个 log 文件，ls 监测某个文件的大小变化，看你的想象力了！ 语法watch(选项)(参数) 选项123-n：指定指令执行的间隔时间（秒）；-d：高亮显示指令输出信息不同之处；-t：不显示标题。 参数指令：需要周期性执行的指令。 常用实例1）每隔一秒高亮显示网络链接数的变化情况 1# watch -n 1 -d netstat -ant 说明： 其它操作： 中断watch操作：Ctrl+c 2）每隔一秒高亮显示http链接数的变化情况 1watch -n 1 -d &apos;pstree|grep http&apos; 说明： 每隔一秒高亮显示http链接数的变化情况。 后面接的命令若带有管道符，需要加’’将命令区域归整。 3）实时查看模拟攻击客户机建立起来的连接数 1watch &apos;netstat -an | grep:21 | \ grep&lt;模拟攻击客户机的IP&gt;| wc -l&apos; 4）监测当前目录中 scf’ 的文件的变化 1watch -d &apos;ls -l|grep scf&apos; 5）10秒一次输出系统的平均负载 1watch -n 10 &apos;cat /proc/loadavg&apos; 6）监测磁盘inode和block数目变化情况 1#watch -n 1 &quot;df -i;df&quot; 参考链接： http://www.cnblogs.com/peida/archive/2012/12/31/2840241.html http://man.linuxde.net/watch]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（46）-iostat]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8846%EF%BC%89-iostat%2F</url>
    <content type="text"><![CDATA[Linux系统中的 iostat是I/O statistics（输入/输出统计）的缩写，iostat工具将对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况。同vmstat一样，iostat也有一个弱点，就是它不能对某个进程进行深入分析，仅对系统的整体情况进行分析。iostat属于sysstat软件包。可以用yum install sysstat 直接安装。 语法iostat(选项)(参数) 选项12345678-c：仅显示CPU使用情况；-d：仅显示设备利用率；-k：显示状态以千字节每秒为单位，而不使用块每秒；-m：显示状态以兆字节每秒为单位；-p：仅显示块设备和所有被使用的其他分区的状态；-t：显示每个报告产生时的时间；-V：显示版号并退出；-x：显示扩展状态。 参数 间隔时间：每次报告的间隔时间（秒）； 次数：显示报告的次数。 常用实例 1）显示所有设备负载情况 12345678910# iostat Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnxvda 0.38 0.10 5.71 729274 43157304xvdb 30.95 7.62 686.84 57636578 5194927160dm-0 85.98 7.62 686.84 57635962 5194927160 说明： cpu属性值说明： %user：CPU处在用户模式下的时间百分比。 %nice：CPU处在带NICE值的用户模式下的时间百分比。 %system：CPU处在系统模式下的时间百分比。 %iowait：CPU等待输入输出完成时间的百分比。 %steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。 %idle：CPU空闲时间百分比。 备注：如果%iowait的值过高，表示硬盘存在I/O瓶颈，%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。 2）定时显示所有信息 1iostat 2 3 说明： 每隔 2秒刷新显示，且显示3次 3）显示指定磁盘信息 12345# iostat -d xvdaLinux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnxvda 0.38 0.10 5.71 729290 43160264 4）显示tty和cpu信息 1234567891011# iostat -tLinux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)04/04/2018 02:25:52 PMavg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnxvda 0.38 0.10 5.71 729290 43160960xvdb 30.95 7.62 686.87 57637978 5195407104dm-0 85.99 7.62 686.87 57637362 5195407104 5）以m为单位显示所有信息 12345678910# iostat -mLinux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90Device: tps MB_read/s MB_wrtn/s MB_read MB_wrtnxvda 0.38 0.00 0.00 356 21075xvdb 30.95 0.00 0.34 28143 2536858dm-0 85.99 0.00 0.34 28143 2536858 6）查看TPS和吞吐量信息 1234567# iostat -d -k 1 1 Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnxvda 0.38 0.05 2.85 364653 21581388xvdb 30.95 3.81 343.44 28819121 2597806052dm-0 85.99 3.81 343.44 28818813 2597806052 说明： tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。“一次传输”意思是“一次I/O请求”。多个逻辑请求可能会被合并为“一次I/O请求”。“一次传输”请求的大小是未知的。 kB_read/s：每秒从设备（drive expressed）读取的数据量； kB_wrtn/s：每秒向设备（drive expressed）写入的数据量； kB_read：读取的总数据量；kB_wrtn：写入的总数量数据量； 这些单位都为Kilobytes。 7）查看设备使用率（%util）、响应时间（await） 1234567# iostat -d -x -k 1 1 Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilxvda 0.00 0.33 0.00 0.38 0.05 2.85 15.16 0.00 3.11 1.94 3.12 1.06 0.04xvdb 0.00 55.03 0.13 30.83 3.81 343.45 22.44 0.33 10.74 3.87 10.76 0.12 0.37dm-0 0.00 0.00 0.13 85.86 3.81 343.45 8.08 0.33 3.89 3.88 3.89 0.04 0.37 说明： rrqm/s： 每秒进行 merge 的读操作数目.即 delta(rmerge)/s wrqm/s： 每秒进行 merge 的写操作数目.即 delta(wmerge)/s r/s： 每秒完成的读 I/O 设备次数.即 delta(rio)/s w/s： 每秒完成的写 I/O 设备次数.即 delta(wio)/s rsec/s： 每秒读扇区数.即 delta(rsect)/s wsec/s： 每秒写扇区数.即 delta(wsect)/s rkB/s： 每秒读K字节数.是 rsect/s 的一半,因为每扇区大小为512字节.(需要计算) wkB/s： 每秒写K字节数.是 wsect/s 的一半.(需要计算) avgrq-sz：平均每次设备I/O操作的数据大小 (扇区).delta(rsect+wsect)/delta(rio+wio) avgqu-sz：平均I/O队列长度.即 delta(aveq)/s/1000 (因为aveq的单位为毫秒). await： 平均每次设备I/O操作的等待时间 (毫秒).即 delta(ruse+wuse)/delta(rio+wio) svctm： 平均每次设备I/O操作的服务时间 (毫秒).即 delta(use)/delta(rio+wio) %util： 一秒中有百分之多少的时间用于 I/O 操作,或者说一秒中有多少时间 I/O 队列是非空的，即 delta(use)/s/1000 (因为use的单位为毫秒) 如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。 idle小于70% IO压力就较大了，一般读取速度有较多的wait。 同时可以结合vmstat 查看查看b参数(等待资源的进程数)和wa参数(IO等待所占用的CPU时间的百分比，高过30%时IO压力高)。 另外 await 的参数也要多和 svctm 来参考。差的过高就一定有 IO 的问题。 avgqu-sz 也是个做 IO 调优时需要注意的地方，这个就是直接每次操作的数据的大小，如果次数多，但数据拿的小的话，其实 IO 也会很小。如果数据拿的大，才IO 的数据会高。也可以通过 avgqu-sz × ( r/s or w/s ) = rsec/s or wsec/s。也就是讲，读定速度是这个来决定的。 svctm 一般要小于 await (因为同时等待的请求的等待时间被重复计算了)，svctm 的大小一般和磁盘性能有关，CPU/内存的负荷也会对其有影响，请求过多也会间接导致 svctm 的增加。await 的大小一般取决于服务时间(svctm) 以及 I/O 队列的长度和 I/O 请求的发出模式。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明 I/O 队列太长，应用得到的响应时间变慢，如果响应时间超过了用户可以容许的范围，这时可以考虑更换更快的磁盘，调整内核 elevator 算法，优化应用，或者升级 CPU。 队列长度(avgqu-sz)也可作为衡量系统 I/O 负荷的指标，但由于 avgqu-sz 是按照单位时间的平均值，所以不能反映瞬间的 I/O 洪水。 ​ 形象的比喻：​ r/s+w/s 类似于交款人的总数​ 平均队列长度(avgqu-sz)类似于单位时间里平均排队人的个数​ 平均服务时间(svctm)类似于收银员的收款速度​ 平均等待时间(await)类似于平均每人的等待时间​ 平均I/O数据(avgrq-sz)类似于平均每人所买的东西多少​ I/O 操作率 (%util)类似于收款台前有人排队的时间比例​ 设备IO操作:总IO(io)/s = r/s(读) +w/s(写) =1.46 + 25.28=26.74​ 平均每次设备I/O操作只需要0.36毫秒完成,现在却需要10.57毫秒完成，因为发出的 请求太多(每秒26.74个)，假如请求时同时发出的，可以这样计算平均等待时间:​ 平均等待时间=单个I/O服务器时间*(1+2+…+请求总数-1)/请求总数​ 每秒发出的I/0请求很多,但是平均队列就4,表示这些请求比较均匀,大部分处理还是比较及时。 8）查看cpu状态 1234567891011# iostat -c 1 3Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90avg-cpu: %user %nice %system %iowait %steal %idle 2.29 0.13 0.38 0.00 0.13 97.07avg-cpu: %user %nice %system %iowait %steal %idle 4.32 0.00 0.51 0.13 0.00 95.04 参考链接： http://www.cnblogs.com/peida/archive/2012/12/28/2837345.html http://man.linuxde.net/iostat]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（45）-vmstat]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8845%EF%BC%89-vmstat%2F</url>
    <content type="text"><![CDATA[vmstat是Virtual Meomory Statistics（虚拟内存统计）的缩写，可对操作系统的虚拟内存、进程、CPU活动进行监控。他是对系统整体情况进行统计，不足之处是无法对某个进程今次那个深入分析。vmstat工具提供了一种低开销的系统性能观察方式。因为vmstat本身就是低开销工具，在非常高负荷的服务器上，你需要查看并监控系统的健康情况，在控制窗口还是能够使用vmstat命令前，我们先了解下Linux系统中关于物理内存和虚拟内存相关信息。 物理内存和虚拟内存区别： 我们知道，直接从物理内存读取数据要比从硬盘读写数据要快的多，因此，我们希望所有数据的读取和写入都在内存完成，而内存是有限的，这样就引出了物理内存与虚拟内存的概念。 物理内存就是系统硬件提供的内存大小，是真正的内存，在linux下还有一个虚拟内存的概念，虚拟内存就是为了满足物理内存的不足而提出的策略，它是利用磁盘空间虚拟出的一块逻辑内存，用作虚拟内存的磁盘空间被称为交换空间（Swap Space）。 作为物理内存的扩展，linux会在物理内存不足时，使用交换分区的虚拟内存，更详细的说，就是内核会将暂时不用的内存块信息写到交换空间，这样以来，物理内存得到了释放，这块内存就是用于其目的，当需要用到原始的内容时，这些信息会被重新从交换空间读入物理内存。 linux的内存管理采取的是分页存取机制，为了保证物理内存能得到充分的利用，内核会在适当的时候讲物理内存中不经常使用的数据块自动交换到虚拟内存中，而将经常使用的信息保留到物理内存。 要深入了解linux内存运行机制，需要知道下面提到的几个方面： 首先，linux系统会不时的进行页面，以保持尽可能多的空闲物理内存，即使并没有什么事情需要内存，linux也会交换出暂时不用的内存页面。这可以避免等待交互所需的时间。 其次，linux进行页面交换是有条件的，不时所有页面在不用时都交换到虚拟内存，linux内核根据“最近最经常使用”算法，仅仅将一些不经常使用的页面文件交换到虚拟内存，有时我们会看到这么一个现象：linux物理内存还有很多，但是交换空间也使用了很多。其实，这并不奇怪，例如，一个占用很大内存的进程运行时，需要耗费很多内存资源，此时就会有一些不常用页面文件被交换到虚拟内存中，但后来这个占用很多内存资源的进程结束并释放了很多内存是，刚才被交换出去的页面文件并不会自动的交换进物理内存，除非有这个必要，那么此刻系统物理内存就会空闲很多，同时交换空间也在被使用，就出现了刚才所说的现象了。关于这点，不用担心什么，只要知道是怎么一回事就可以了。 最后，交换空间的页面在使用时会首先被交换到物理内存，如果此时没有足够的物理内存来容纳这些页面，他们又会被马上交换出去，如此以来，虚拟内存中可能没有足够空间来存储这些交换页面，最终会导致linux出现假死机、服务异常等问题，linux虽然可以在一段时间内自行恢复，但是恢复后的系统已经基本不可用了。 因此，合理规划和设计linux内存的使用，是非常重要的。 虚拟内存原理： 在系统中运行的每个进程都需要使用到内存，但不是每个进程都需要每时每刻使用系统分配的内存空间。当系统运行所需内存超过实际的物理内存，内核会释放某些进程所占用但未使用的部分或所有物理内存，将这部分资料存储在磁盘上直到进程下一次调用，并将释放出的内存提供给有需要的进程使用。 在linux内存管理中，主要是通过“调页Paging”和“交换Swapping”来完成上述的内存调度。调页算法是将内存中最近不常使用的页面换到磁盘上，把活动页面保留在内存中供进程使用。交换技术是将整个进程，而不是部分页面，全部交换到磁盘上。 分页（Page）写入磁盘的过程被称作Page-Out，分页（Page）从磁盘重新回到内存的过程被称作Page-In。当内核需要一个分页时，但发现此分页不在物理内存中（因为已经被Page-Out了），此时就发生了分页错误（Page Fault）。 当系统内核发现可运行内存变少时，就会通过Page-Out来释放一部分物理内存。尽管Page-Out不是经常发生，但是如果Page-out频繁不断的发生，直到当内核管理分页的时间超过运行程式的时间时，系统效能会急剧下降。这时的系统已经运行非常慢或进入暂停状态，这种状态也被称作thrashing（颠簸）。 语法vmstat(选项)(参数) 选项12345678-a：显示活动内页；-f：显示启动后创建的进程总数；-m：显示slab信息；-n：头信息仅显示一次；-s：以表格方式显示事件计数器和内存状态；-d：报告磁盘状态；-p：显示指定的硬盘分区状态；-S：输出信息的单位。 参数 事件间隔：状态信息刷新的时间间隔； 次数：显示报告的次数。 常用实例1）显示虚拟内存使用情况 12345678# vmstat 5 6procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 3029876 199616 690980 0 0 0 2 3 2 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 41 1009 39 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 3 1004 36 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 4 1004 36 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 6 1003 33 0 0 100 0 0 说明： 字段说明： Procs（进程）： r: 运行队列中进程数量 b: 等待IO的进程数量 Memory（内存）： swpd: 使用虚拟内存大小 free: 可用内存大小 buff: 用作缓冲的内存大小 cache: 用作缓存的内存大小 Swap： si: 每秒从交换区写到内存的大小 so: 每秒写入交换区的内存大小 IO：（现在的Linux版本块的大小为1024bytes） bi: 每秒读取的块数 bo: 每秒写入的块数 系统： in: 每秒中断数，包括时钟中断。 cs: 每秒上下文切换数。 CPU（以百分比表示）： us: 用户进程执行时间(user time) sy: 系统进程执行时间(system time) id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。 wa: 等待IO时间 备注： 如果 r经常大于 4 ，且id经常少于40，表示cpu的负荷很重。如果pi，po 长期不等于0，表示内存不足。如果disk 经常不等于0， 且在 b中的队列 大于3， 表示 io性能不好。Linux在具有高稳定性、可靠性的同时，具有很好的可伸缩性和扩展性，能够针对不同的应用和硬件环境调整，优化出满足当前应用需要的最佳性能。因此企业在维护Linux系统、进行系统调优时，了解系统性能分析工具是至关重要的。 命令： vmstat 5 5 表示在5秒时间内进行5次采样。将得到一个数据汇总他能够反映真正的系统情况。 2）显示活跃和非活跃内存 12345678# vmstat -a 2 5procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free inact active si so bi bo in cs us sy id wa st 0 0 0 3029752 387728 513008 0 0 0 2 3 2 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1005 34 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 22 1004 36 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1004 33 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1003 32 0 0 100 0 0 说明： 使用-a选项显示活跃和非活跃内存时，所显示的内容除增加inact和active外，其他显示内容与例子1相同。 字段说明： Memory（内存）： inact: 非活跃内存大小（当使用-a选项时显示） active: 活跃的内存大小（当使用-a选项时显示） 3）查看系统已经fork了多少次 12# vmstat -f 12744849 forks 说明： 这个数据是从/proc/stat中的processes字段里取得的 4）查看内存使用的详细信息 123456789101112131415161718192021222324252627# vmstat -s 4043760 total memory 1013884 used memory 513012 active memory 387728 inactive memory 3029876 free memory 199616 buffer memory 690980 swap cache 6096656 total swap 0 used swap 6096656 free swap 83587 non-nice user cpu ticks 132 nice user cpu ticks 278599 system cpu ticks 913344692 idle cpu ticks 814550 IO-wait cpu ticks 10547 IRQ cpu ticks 21261 softirq cpu ticks 0 stolen cpu ticks 310215 pages paged in 14254652 pages paged out 0 pages swapped in 0 pages swapped out 288374745 interrupts 146680577 CPU context switches 1351868832 boot time 367291 forks 说明： 这些信息的分别来自于/proc/meminfo,/proc/stat和/proc/vmstat。 5）查看磁盘的读/写 1234567891011121314151617181920212223# vmstat -ddisk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur secram0 0 0 0 0 0 0 0 0 0 0ram1 0 0 0 0 0 0 0 0 0 0ram2 0 0 0 0 0 0 0 0 0 0ram3 0 0 0 0 0 0 0 0 0 0ram4 0 0 0 0 0 0 0 0 0 0ram5 0 0 0 0 0 0 0 0 0 0ram6 0 0 0 0 0 0 0 0 0 0ram7 0 0 0 0 0 0 0 0 0 0ram8 0 0 0 0 0 0 0 0 0 0ram9 0 0 0 0 0 0 0 0 0 0ram10 0 0 0 0 0 0 0 0 0 0ram11 0 0 0 0 0 0 0 0 0 0ram12 0 0 0 0 0 0 0 0 0 0ram13 0 0 0 0 0 0 0 0 0 0ram14 0 0 0 0 0 0 0 0 0 0ram15 0 0 0 0 0 0 0 0 0 0sda 33381 6455 615407 63224 2068111 1495416 28508288 15990289 0 10491hdc 0 0 0 0 0 0 0 0 0 0fd0 0 0 0 0 0 0 0 0 0 0md0 0 0 0 0 0 0 0 0 0 0 这些信息主要来自于/proc/diskstats. merged:表示一次来自于合并的写/读请求,一般系统会把多个连接/邻近的读/写请求合并到一起来操作 6）查看/dev/sda1磁盘的读/写 12345678910# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda3 1119336548 27642068 1034835500 3% /tmpfs 32978376 0 32978376 0% /dev/shm/dev/sda1 1032088 59604 920056 7% /boot# vmstat -p /dev/sda1sda1 reads read sectors writes requested writes 18607 4249978 6 48# vmstat -p /dev/sda3sda3 reads read sectors writes requested writes 429350 35176268 28998789 980301488 说明： 这些信息主要来自于/proc/diskstats。 reads:来自于这个分区的读的次数。 read sectors:来自于这个分区的读扇区的次数。 writes:来自于这个分区的写的次数。 requested writes:来自于这个分区的写请求次数。 7）查看系统的slab信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143# vmstat -mCache Num Total Size Pagesip_conntrack_expect 0 0 136 28ip_conntrack 3 13 304 13ip_fib_alias 11 59 64 59ip_fib_hash 11 59 64 59AF_VMCI 0 0 960 4bio_map_info 100 105 1064 7dm_mpath 0 0 1064 7jbd_4k 0 0 4096 1dm_uevent 0 0 2608 3dm_tio 0 0 24 144dm_io 0 0 48 77scsi_cmd_cache 10 10 384 10sgpool-128 32 32 4096 1sgpool-64 32 32 2048 2sgpool-32 32 32 1024 4sgpool-16 32 32 512 8sgpool-8 45 45 256 15scsi_io_context 0 0 112 34ext3_inode_cache 51080 51105 760 5ext3_xattr 36 88 88 44journal_handle 18 144 24 144journal_head 56 80 96 40revoke_table 4 202 16 202revoke_record 0 0 32 112uhci_urb_priv 0 0 56 67UNIX 13 33 704 11flow_cache 0 0 128 30msi_cache 33 59 64 59cfq_ioc_pool 14 90 128 30cfq_pool 12 90 216 18crq_pool 16 96 80 48deadline_drq 0 0 80 48as_arq 0 0 96 40mqueue_inode_cache 1 4 896 4isofs_inode_cache 0 0 608 6hugetlbfs_inode_cache 1 7 576 7Cache Num Total Size Pagesext2_inode_cache 0 0 720 5ext2_xattr 0 0 88 44dnotify_cache 0 0 40 92dquot 0 0 256 15eventpoll_pwq 3 53 72 53eventpoll_epi 3 20 192 20inotify_event_cache 0 0 40 92inotify_watch_cache 1 53 72 53kioctx 0 0 320 12kiocb 0 0 256 15fasync_cache 0 0 24 144shmem_inode_cache 254 290 768 5posix_timers_cache 0 0 128 30uid_cache 0 0 128 30ip_mrt_cache 0 0 128 30tcp_bind_bucket 3 112 32 112inet_peer_cache 0 0 128 30secpath_cache 0 0 64 59xfrm_dst_cache 0 0 384 10ip_dst_cache 5 10 384 10arp_cache 1 15 256 15RAW 3 5 768 5UDP 5 10 768 5tw_sock_TCP 0 0 192 20request_sock_TCP 0 0 128 30TCP 4 5 1600 5blkdev_ioc 14 118 64 59blkdev_queue 20 30 1576 5blkdev_requests 13 42 272 14biovec-256 7 7 4096 1biovec-128 7 8 2048 2biovec-64 7 8 1024 4biovec-16 7 15 256 15biovec-4 7 59 64 59biovec-1 23 202 16 202bio 270 270 128 30utrace_engine_cache 0 0 64 59Cache Num Total Size Pagesutrace_cache 0 0 64 59sock_inode_cache 33 48 640 6skbuff_fclone_cache 7 7 512 7skbuff_head_cache 319 390 256 15file_lock_cache 1 22 176 22Acpi-Operand 4136 4248 64 59Acpi-ParseExt 0 0 64 59Acpi-Parse 0 0 40 92Acpi-State 0 0 80 48Acpi-Namespace 2871 2912 32 112delayacct_cache 81 295 64 59taskstats_cache 4 53 72 53proc_inode_cache 1427 1440 592 6sigqueue 0 0 160 24radix_tree_node 13166 13188 536 7bdev_cache 23 24 832 4sysfs_dir_cache 5370 5412 88 44mnt_cache 26 30 256 15inode_cache 2009 2009 560 7dentry_cache 60952 61020 216 18filp 479 1305 256 15names_cache 3 3 4096 1avc_node 14 53 72 53selinux_inode_security 994 1200 80 48key_jar 2 20 192 20idr_layer_cache 74 77 528 7buffer_head 164045 164800 96 40mm_struct 51 56 896 4vm_area_struct 1142 1958 176 22fs_cache 35 177 64 59files_cache 36 55 768 5signal_cache 72 162 832 9sighand_cache 68 84 2112 3task_struct 76 80 1888 2anon_vma 458 864 24 144pid 83 295 64 59shared_policy_node 0 0 48 77Cache Num Total Size Pagesnuma_policy 37 144 24 144size-131072(DMA) 0 0 131072 1size-131072 0 0 131072 1size-65536(DMA) 0 0 65536 1size-65536 1 1 65536 1size-32768(DMA) 0 0 32768 1size-32768 2 2 32768 1size-16384(DMA) 0 0 16384 1size-16384 5 5 16384 1size-8192(DMA) 0 0 8192 1size-8192 7 7 8192 1size-4096(DMA) 0 0 4096 1size-4096 110 111 4096 1size-2048(DMA) 0 0 2048 2size-2048 602 602 2048 2size-1024(DMA) 0 0 1024 4size-1024 344 352 1024 4size-512(DMA) 0 0 512 8size-512 433 480 512 8size-256(DMA) 0 0 256 15size-256 1139 1155 256 15size-128(DMA) 0 0 128 30size-64(DMA) 0 0 64 59size-64 5639 5782 64 59size-32(DMA) 0 0 32 112size-128 801 930 128 30size-32 3005 3024 32 112kmem_cache 137 137 2688 1 这组信息来自于/proc/slabinfo。 slab:由于内核会有许多小对象，这些对象构造销毁十分频繁，比如i-node，dentry，这些对象如果每次构建的时候就向内存要一个页(4kb)，而其实只有几个字节，这样就会非常浪费，为了解决这个问题，就引入了一种新的机制来处理在同一个页框中如何分配小存储区，而slab可以对小对象进行分配,这样就不用为每一个对象分配页框，从而节省了空间，内核对一些小对象创建析构很频繁，slab对这些小对象进行缓冲,可以重复利用,减少内存分配次数。 参考链接： http://www.cnblogs.com/peida/archive/2012/12/25/2833108.html http://man.linuxde.net/vmstat]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（44）-free]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8844%EF%BC%89-free%2F</url>
    <content type="text"><![CDATA[free命令可以显示Linux系统中空闲的、已用的物理内存及swap内存,及被内核使用的buffer。在Linux系统监控的工具中，free命令是最经常使用的命令之一。 语法free(选项) 选项12345678-b 以Byte为单位显示内存使用情况。 -k 以KB为单位显示内存使用情况。 -m 以MB为单位显示内存使用情况。-g 以GB为单位显示内存使用情况。 -o 不显示缓冲区调节列。 -s&lt;间隔秒数&gt; 持续观察内存使用状况。 -t 显示内存总和列。 -V 显示版本信息。 功能free 命令显示系统使用和空闲的内存情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。共享内存将被忽略 常用实例1）显示内存使用情况 123456789101112131415# free total used free shared buffers cachedMem: 32940112 30841684 2098428 0 4545340 11363424-/+ buffers/cache: 14932920 18007192Swap: 32764556 1944984 30819572# free -g total used free shared buffers cachedMem: 31 29 2 0 4 10-/+ buffers/cache: 14 17Swap: 31 1 29# free -m total used free shared buffers cachedMem: 32168 30119 2048 0 4438 11097-/+ buffers/cache: 14583 17584Swap: 31996 1899 30097 说明： 下面是对这些数值的解释： total：总计物理内存的大小。 used：已使用多大。 free：可用有多少。 Shared：多个进程共享的内存总额。 Buffers/cached：磁盘缓存的大小。 第三行（-/+ buffers/cached）： used：已使用多大。 free：可用有多少。 第四行是交换分区SWAP的，也就是他们我们通常所说的虚拟内存。 区别：第二行（mem）的used/free与第三行（-/+ buffers/cache）used/free的区别。这两个的区别在于使用的角度来看，第一行是从OS的角度来看，因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是2098428KB,已用内存是30841684KB,其中包括，内核（OS）使用+Application(X, oracle,etc)使用的+buffers+cached. 第三行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是等于可用的，因为buffer/cached是为了提高文件读取的性能，当应用程序需在用到内存的时候，buffer/cached会很快地被回收。 所以从应用程序的角度来说，可用内存=系统free memory+buffers+cached。 如本机情况的可用内存为： 18007156=2098428KB+4545340KB+11363424KB 接下来解释什么时候内存会被交换，以及按什么方交换。 当可用内存少于额定值的时候，就会开会进行交换.如何看额定值： 12345678910111213141516171819202122232425262728# cat /proc/meminfoMemTotal: 32940112 kBMemFree: 2096700 kBBuffers: 4545340 kBCached: 11364056 kBSwapCached: 1896080 kBActive: 22739776 kBInactive: 7427836 kBHighTotal: 0 kBHighFree: 0 kBLowTotal: 32940112 kBLowFree: 2096700 kBSwapTotal: 32764556 kBSwapFree: 30819572 kBDirty: 164 kBWriteback: 0 kBAnonPages: 14153592 kBMapped: 20748 kBSlab: 590232 kBPageTables: 34200 kBNFS_Unstable: 0 kBBounce: 0 kBCommitLimit: 49234612 kBCommitted_AS: 23247544 kBVmallocTotal: 34359738367 kBVmallocUsed: 278840 kBVmallocChunk: 34359459371 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0Hugepagesize: 交换将通过三个途径来减少系统中使用的物理页面的个数： 1.减少缓冲与页面cache的大小， 2.将系统V类型的内存页面交换出去， 3.换出或者丢弃页面。(Application 占用的内存页，也就是物理内存不足）。 事实上，少量地使用swap是不是影响到系统性能的。 那buffers和cached都是缓存，两者有什么区别呢？ 为了提高磁盘存取效率, Linux做了一些精心的设计, 除了对dentry进行缓存(用于VFS,加速文件路径名到inode的转换), 还采取了两种主要Cache方式：Buffer Cache和Page Cache。前者针对磁盘块的读写，后者针对文件inode的读写。这些Cache有效缩短了 I/O系统调用(比如read,write,getdents)的时间。 磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。 Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。文件的逻辑层需要映射到实际的物理磁盘，这种映射关系由文件系统来完成。当page cache的数据需要刷新时，page cache中的数据交给buffer cache，因为Buffer Cache就是缓存磁盘块的。但是这种处理在2.6版本的内核之后就变的很简单了，没有真正意义上的cache操作。 Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中，例如，文件系统的元数据都会缓存到buffer cache中。 简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。 所以我们看linux,只要不用swap的交换空间,就不用担心自己的内存太少.如果常常swap用很多,可能你就要考虑加物理内存了.这也是linux看内存是否够用的标准. 如果是应用服务器的话，一般只看第二行，+buffers/cache,即对应用程序来说free的内存太少了，也是该考虑优化程序或加内存了。 2）以总和的形式显示内存的使用信息 12345# free -t total used free shared buffers cachedMem: 32940112 30845024 2095088 0 4545340 11364324-/+ buffers/cache: 14935360 18004752Swap: 32764556 1944984 30819572Total: 65704668 32790008 32914660 3）周期性的查询内存使用信息 1234567# free -s 10 total used free shared buffers cachedMem: 32940112 30844528 2095584 0 4545340 11364380-/+ buffers/cache: 14934808 18005304Swap: 32764556 1944984 30819572 total used free shared buffers cachedMem: 32940112 30843932 2096180 0 4545340 11364388-/+ buffers/cache: 14934204 18005908Swap: 32764556 1944984 30819572 说明： 每10s 执行一次命令 转载链接： http://www.cnblogs.com/peida/archive/2012/12/25/2831814.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（43）-top]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8843%EF%BC%89-top%2F</url>
    <content type="text"><![CDATA[top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。下面详细介绍它的使用方法。top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定. 语法top(选项) 选项12345678910-b：以批处理模式操作；-c：显示完整的治命令；-d：屏幕刷新间隔时间；-I：忽略失效过程；-s：保密模式；-S：累积模式；-i&lt;时间&gt;：设置间隔时间；-u&lt;用户名&gt;：指定用户名；-p&lt;进程号&gt;：指定进程；-n&lt;次数&gt;：循环显示的次数。 功能显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 常用实例1）显示进程信息 1234567891011# toptop - 14:06:23 up 70 days, 16:44, 2 users, load average: 1.25, 1.32, 1.35Tasks: 206 total, 1 running, 205 sleeping, 0 stopped, 0 zombieCpu(s): 5.9%us, 3.4%sy, 0.0%ni, 90.4%id, 0.0%wa, 0.0%hi, 0.2%si, 0.0%stMem: 32949016k total, 14411180k used, 18537836k free, 169884k buffersSwap: 32764556k total, 0k used, 32764556k free, 3612636k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28894 root 22 0 1501m 405m 10m S 52.2 1.3 2534:16 java 18249 root 18 0 3201m 1.9g 11m S 35.9 6.0 569:39.41 java 2808 root 25 0 3333m 1.0g 11m S 24.3 3.1 526:51.85 java 25668 root 23 0 3180m 704m 11m S 14.0 2.2 360:44.53 java 说明： 统计信息区： 前五行是当前系统情况整体的统计信息区。下面我们看每一行信息的具体意义。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下： 14:06:23 — 当前系统时间 up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！） 2 users — 当前有2个用户登录系统 load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下： 系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下： 5.9%us — 用户空间占用CPU的百分比。 3.4% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 90.4% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.2% si — 软中断（Software Interrupts）占用CPU的百分比 第五行，swap交换分区信息，具体信息说明如下： 32764556k total — 交换区总量（32GB） 0k used — 使用的交换区总量（0K） 32764556k free — 空闲交换区总量（32GB） 3612636k cached — 缓冲的交换区总量（3.6GB） 备注： 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 如果出于习惯去计算可用内存数，这里有个近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached，按这个公式此台服务器的可用内存：18537836k +169884k +3612636k = 22GB左右。 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 第六行，空行。 第七行以下：各进程（任务）的状态监控，项目列信息说明如下： PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） 其他使用技巧： 1.多U多核CPU监控 在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况： 观察上图，服务器有16个逻辑CPU，实际上是4个物理CPU。再按数字键1，就会返回到top基本视图界面。 2.高亮显示当前运行进程 ​ 敲击键盘“b”（打开/关闭加亮效果），top的视图变化如下： 我们发现进程id为2570的“top”进程被加亮了，top进程就是视图第二行显示的唯一的运行态（runing）的那个进程，可以通过敲击“y”键关闭或打开运行态进程的加亮效果。 3.进程字段排序 默认进入top时，各进程是按照CPU的占用量来排序的，在下图中进程ID为28894的java进程排在第一（cpu占用142%），进程ID为574的java进程排在第二（cpu占用16%）。 ​ 敲击键盘“x”（打开/关闭排序列的加亮效果），top的视图变化如下： 可以看到，top默认的排序列是“%CPU”。 通过”shift + &gt;”或”shift + &lt;”可以向右或左改变排序列 下图是按一次”shift + &gt;”的效果图,视图现在已经按照%MEM来排序。 2）显示完整命令 1top -c 3）以批处理模式显示程序信息 1top -b 4）以累积模式显示程序信息 1top -S 5）设置信息更新次数 1top -n 2 6）设置信息更新时间 1top -d 3 说明： 表示更新周期为3秒 7）显示指定的进程信息 1top -p 574 top交互命令在top 命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了s 选项， 其中一些命令可能会被屏蔽。 h 显示帮助画面，给出一些简短的命令总结说明 k 终止一个进程。 i 忽略闲置和僵死进程。这是一个开关式命令。 q 退出程序 r 重新安排一个进程的优先级别 S 切换到累计模式 s 改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s f或者F 从当前显示中添加或者删除项目 o或者O 改变显示项目的顺序 l 切换显示平均负载和启动时间信息 m 切换显示内存信息 t 切换显示进程和CPU状态信息 c 切换显示命令名称和完整命令行 M 根据驻留内存大小进行排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中 参考链接： http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html http://man.linuxde.net/top]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（42）-killall]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8842%EF%BC%89-killall%2F</url>
    <content type="text"><![CDATA[Linux系统中的killall命令用于杀死指定名字的进程（kill processes by name）。我们可以使用kill命令杀死指定进程PID的进程，如果要找到我们需要杀死的进程，我们还需要在之前使用ps等命令再配合grep来查找进程，而killall把这两个过程合二为一，是一个很好用的命令。 语法killall(选项)(参数) 选项123456789101112-Z 只杀死拥有scontext 的进程-e 要求匹配进程名称-I 忽略小写-g 杀死进程组而不是进程-i 交互模式，杀死进程前先询问用户-l 列出所有的已知信号名称-q 不输出警告信息-s 发送指定的信号-v 报告信号是否成功发送-w 等待进程死亡--help 显示帮助信息--version 显示版本显示 参数进程名称：指定要杀死的进程名称 常用实例1）杀死所有同名进程 1234567# ps -ef|grep viroot 17581 17398 0 17:51 pts/0 00:00:00 vi test.txtroot 17640 17612 0 17:51 pts/2 00:00:00 vi test.logroot 17642 17582 0 17:51 pts/1 00:00:00 grep vi# killall vi# ps -ef|grep viroot 17645 17582 0 17:52 pts/1 00:00:00 grep vi 2）向进程发送指定信号 1234567891011121314151617181920# vi &amp; [1] 17646[root@localhost ~]# killall -TERM vi[1]+ Stopped vi# vi &amp; [2] 17648# ps -ef|grep viroot 17646 17582 0 17:54 pts/1 00:00:00 viroot 17648 17582 0 17:54 pts/1 00:00:00 viroot 17650 17582 0 17:55 pts/1 00:00:00 grep vi[2]+ Stopped vi# killall -TERM vi# ps -ef|grep viroot 17646 17582 0 17:54 pts/1 00:00:00 viroot 17648 17582 0 17:54 pts/1 00:00:00 viroot 17653 17582 0 17:55 pts/1 00:00:00 grep vi# killall -KILL vi[1]- 已杀死 vi[2]+ 已杀死 vi# ps -ef|grep viroot 17656 17582 0 17:56 pts/1 00:00:00 grep vi 3）把所有的登录后的shell给杀掉 123456789# w 18:01:03 up 41 days, 18:53, 3 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 10.2.0.68 14:58 9:52 0.10s 0.10s -bashroot pts/1 10.2.0.68 17:51 0.00s 0.02s 0.00s wroot pts/2 10.2.0.68 17:51 9:24 0.01s 0.01s -bash# killall -9 bash# w 18:01:48 up 41 days, 18:54, 1 user, load average: 0.07, 0.02, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 10.2.0.68 18:01 0.00s 0.01s 0.00s w 说明： 运行命令：killall -9 bash 后，所有bash都会被卡掉了，所以当前所有连接丢失了。需要重新连接并登录。 参考链接： http://www.cnblogs.com/peida/archive/2012/12/21/2827366.html http://man.linuxde.net/killall]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（41）-kill]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8841%EF%BC%89-kill%2F</url>
    <content type="text"><![CDATA[Linux中的kill命令用来终止指定的进程（terminate a process）的运行，是Linux下进程管理的常用命令。通常，终止一个前台进程可以使用Ctrl+C键，但是，对于一个后台进程就须用kill命令来终止，我们就需要先使用ps/pidof/pstree/top等工具获取进程PID，然后使用kill命令来杀掉该进程。kill命令是通过向进程发送指定的信号来结束相应进程的。在默认情况下，采用编号为15的TERM信号。TERM信号将终止所有不能捕获该信号的进程。对于那些可以捕获该信号的进程就要用编号为9的kill信号，强行“杀掉”该进程。 语法kill(选项)(参数) 选项12345-a：当处理当前进程时，不限制命令名和进程号的对应关系；-l &lt;信息编号&gt;：若不加&lt;信息编号&gt;选项，则-l参数会列出全部的信息名称；-p：指定kill 命令只打印相关进程的进程号，而不发送任何信号；-s &lt;信息名称或编号&gt;：指定要送出的信息；-u：指定用户。 参数进程或作业识别号：指定要删除的进程或作业。 功能发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果无法终止该程序可用“-KILL”参数，其发送的信号为SIGKILL（9），将强制结束进程，使用ps命令或者jobs命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。 常用实例1234567891011121314151617# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR213) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+439) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+1247) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-1451) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-1055) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-659) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 说明： 只有第9种信号(SIGKILL)才可以无条件终止进程，其他信号进程都有权利忽略。 下面是常用的信号： HUP 1 终端断线 INT 2 中断（同 Ctrl + C） QUIT 3 退出（同 Ctrl + \） TERM 15 终止 KILL 9 强制终止 CONT 18 继续（与STOP相反， fg/bg命令） STOP 19 暂停（同 Ctrl + Z） 2）得到指定信号的数值 12345678# kill -l KILL9# kill -l SIGKILL9# kill -l TERM15# kill -l SIGTERM15 3）先用ps查找进程，然后用kill杀掉 123456#ps -ef|grep vim root 3268 2884 0 16:21 pts/1 00:00:00 vim install.logroot 3370 2822 0 16:21 pts/0 00:00:00 grep vim# kill 3268 # kill 3268 -bash: kill: (3268) - 没有那个进程 4）彻底杀死进程 123456# ps -ef|grep vim root 3268 2884 0 16:21 pts/1 00:00:00 vim install.logroot 3370 2822 0 16:21 pts/0 00:00:00 grep vim# kill –9 3268 # kill 3268 -bash: kill: (3268) - 没有那个进程 5）杀死指定用户所有进程 12# kill -9 $(ps -ef | grep peidalinux) # kill -u peidalinux 6）init进程是不可杀的 123456789101112# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3] root 17563 17534 0 17:37 pts/1 00:00:00 grep init# kill -9 1# kill -HUP 1# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3] root 17565 17534 0 17:38 pts/1 00:00:00 grep init# kill -KILL 1# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3] root 17567 17534 0 17:38 pts/1 00:00:00 grep init 说明： init是Linux系统操作中不可缺少的程序之一。所谓的init进程，它是一个由内核启动的用户级进程。内核自行启动（已经被载入内存，开始运行，并已初始化所有的设备驱动程序和数据结构等）之后，就通过启动一个用户级程序init的方式，完成引导进程。所以,init始终是第一个进程（其进程编号始终为1）。 其它所有进程都是init进程的子孙。init进程是不可杀的！ 参考链接： http://www.cnblogs.com/peida/archive/2012/12/20/2825837.html http://man.linuxde.net/kill]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（40）-ps]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8840%EF%BC%89-ps%2F</url>
    <content type="text"><![CDATA[ps命令用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。ps命令是最基本同时也是非常强大的进程查看命令，使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等，总之大部分信息都是可以通过执行该命令得到的。 linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码: D 不可中断 uninterruptible sleep （usually IO） R 运行 runnable （on run queue） S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct （“zombie”） process 语法ps(选项) 选项1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162-a：显示所有终端机下执行的程序，除了阶段作业领导者之外。a：显示现行终端机下的所有程序，包括其他用户的程序。-A：显示所有程序。-c：显示CLS和PRI栏位。c：列出程序时，显示每个程序真正的指令名称，而不包含路径，选项或常驻服务的标示。-C&lt;指令名称&gt;：指定执行指令的名称，并列出该指令的程序的状况。-d：显示所有程序，但不包括阶段作业领导者的程序。-e：此选项的效果和指定&quot;A&quot;选项相同。e：列出程序时，显示每个程序所使用的环境变量。-f：显示UID,PPIP,C与STIME栏位。f：用ASCII字符显示树状结构，表达程序间的相互关系。-g&lt;群组名称&gt;：此选项的效果和指定&quot;-G&quot;选项相同，当亦能使用阶段作业领导者的名称来指定。g：显示现行终端机下的所有程序，包括群组领导者的程序。-G&lt;群组识别码&gt;：列出属于该群组的程序的状况，也可使用群组名称来指定。h：不显示标题列。-H：显示树状结构，表示程序间的相互关系。-j或j：采用工作控制的格式显示程序状况。-l或l：采用详细的格式来显示程序状况。L：列出栏位的相关信息。-m或m：显示所有的执行绪。n：以数字来表示USER和WCHAN栏位。-N：显示所有的程序，除了执行ps指令终端机下的程序之外。-p&lt;程序识别码&gt;：指定程序识别码，并列出该程序的状况。p&lt;程序识别码&gt;：此选项的效果和指定&quot;-p&quot;选项相同，只在列表格式方面稍有差异。r：只列出现行终端机正在执行中的程序。-s&lt;阶段作业&gt;：指定阶段作业的程序识别码，并列出隶属该阶段作业的程序的状况。s：采用程序信号的格式显示程序状况。S：列出程序时，包括已中断的子程序资料。-t&lt;终端机编号&gt;：指定终端机编号，并列出属于该终端机的程序的状况。t&lt;终端机编号&gt;：此选项的效果和指定&quot;-t&quot;选项相同，只在列表格式方面稍有差异。-T：显示现行终端机下的所有程序。-u&lt;用户识别码&gt;：此选项的效果和指定&quot;-U&quot;选项相同。u：以用户为主的格式来显示程序状况。-U&lt;用户识别码&gt;：列出属于该用户的程序的状况，也可使用用户名称来指定。U&lt;用户名称&gt;：列出属于该用户的程序的状况。v：采用虚拟内存的格式显示程序状况。-V或V：显示版本信息。-w或w：采用宽阔的格式来显示程序状况。 x：显示所有程序，不以终端机来区分。X：采用旧式的Linux i386登陆格式显示程序状况。-y：配合选项&quot;-l&quot;使用时，不显示F(flag)栏位，并以RSS栏位取代ADDR栏位 。-&lt;程序识别码&gt;：此选项的效果和指定&quot;p&quot;选项相同。--cols&lt;每列字符数&gt;：设置每列的最大字符数。--columns&lt;每列字符数&gt;：此选项的效果和指定&quot;--cols&quot;选项相同。--cumulative：此选项的效果和指定&quot;S&quot;选项相同。--deselect：此选项的效果和指定&quot;-N&quot;选项相同。--forest：此选项的效果和指定&quot;f&quot;选项相同。--headers：重复显示标题列。--help：在线帮助。--info：显示排错信息。--lines&lt;显示列数&gt;：设置显示画面的列数。--no-headers：此选项的效果和指定&quot;h&quot;选项相同，只在列表格式方面稍有差异。--group&lt;群组名称&gt;：此选项的效果和指定&quot;-G&quot;选项相同。--Group&lt;群组识别码&gt;：此选项的效果和指定&quot;-G&quot;选项相同。--pid&lt;程序识别码&gt;：此选项的效果和指定&quot;-p&quot;选项相同。--rows&lt;显示列数&gt;：此选项的效果和指定&quot;--lines&quot;选项相同。--sid&lt;阶段作业&gt;：此选项的效果和指定&quot;-s&quot;选项相同。--tty&lt;终端机编号&gt;：此选项的效果和指定&quot;-t&quot;选项相同。--user&lt;用户名称&gt;：此选项的效果和指定&quot;-U&quot;选项相同。--User&lt;用户识别码&gt;：此选项的效果和指定&quot;-U&quot;选项相同。--version：此选项的效果和指定&quot;-V&quot;选项相同。--widty&lt;每列字符数&gt;：此选项的效果和指定&quot;-cols&quot;选项相同。 常用范例1）显示所有进程信息 123456# ps -A PID TTY TIME CMD 1 ? 00:00:00 init 2 ? 00:00:01 migration/0 3 ? 00:00:00 ksoftirqd/0 4 ? 00:00:01 migration/1 2）显示指定用户信息 123456# ps -u root PID TTY TIME CMD 1 ? 00:00:00 init 2 ? 00:00:01 migration/0 3 ? 00:00:00 ksoftirqd/0 4 ? 00:00:01 migration/1 3）显示所有进程信息，连同命令行 1234567ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 Nov02 ? 00:00:00 init [3] root 2 1 0 Nov02 ? 00:00:01 [migration/0]root 3 1 0 Nov02 ? 00:00:00 [ksoftirqd/0]root 4 1 0 Nov02 ? 00:00:01 [migration/1]root 5 1 0 Nov02 ? 00:00:00 [ksoftirqd/1] 4） ps 与grep 常用组合用法，查找特定进程 1234# ps -ef|grep sshroot 2720 1 0 Nov02 ? 00:00:00 /usr/sbin/sshdroot 17394 2720 0 14:58 ? 00:00:00 sshd: root@pts/0 root 17465 17398 0 15:57 pts/0 00:00:00 grep ssh 5）将目前属于您自己这次登入的 PID 与相关信息列示出来 1234# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 17398 17394 0 75 0 - 16543 wait pts/0 00:00:00 bash4 R 0 17469 17398 0 77 0 - 15877 - pts/0 00:00:00 ps 说明： 各相关信息的意义： F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何 在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 6）列出目前所有的正在内存当中的程序 1234567# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 10368 676 ? Ss Nov02 0:00 init [3] root 2 0.0 0.0 0 0 ? S&lt; Nov02 0:01 [migration/0]root 3 0.0 0.0 0 0 ? SN Nov02 0:00 [ksoftirqd/0]root 4 0.0 0.0 0 0 ? S&lt; Nov02 0:01 [migration/1]root 5 0.0 0.0 0 0 ? SN Nov02 0:00 [ksoftirqd/1] 说明： USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 7）列出类似程序树的程序显示 12345678 ps -axjfWarning: bad syntax, perhaps a bogus &apos;-&apos;? See /usr/share/doc/procps-3.2.7/FAQ PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 0 1 1 1 ? -1 Ss 0 0:00 init [3] 1 2 1 1 ? -1 S&lt; 0 0:01 [migration/0] 1 3 1 1 ? -1 SN 0 0:00 [ksoftirqd/0] 1 4 1 1 ? -1 S&lt; 0 0:01 [migration/1] 1 5 1 1 ? -1 SN 0 0:00 [ksoftirqd/1] 8）找出与 cron 与 syslog 这两个服务有关的 PID 号码 1234# ps aux | egrep &apos;(cron|syslog)&apos;root 2682 0.0 0.0 83384 2000 ? Sl Nov02 0:00 /sbin/rsyslogd -i /var/run/syslogd.pid -c 5root 2735 0.0 0.0 74812 1140 ? Ss Nov02 0:00 crondroot 17475 0.0 0.0 61180 832 pts/0 S+ 16:27 0:00 egrep (cron|syslog) 说明： 其他实例： 可以用 | 管道和 more 连接起来分页查看 1ps -aux |more 把所有进程显示出来，并输出到ps001.txt文件 1ps -aux &gt; ps001.txt 输出指定的字段 1234# ps -o pid,ppid,pgrp,session,tpgid,comm PID PPID PGRP SESS TPGID COMMAND17398 17394 17398 17398 17478 bash17478 17398 17478 17398 17478 ps 参考链接： http://www.cnblogs.com/peida/archive/2012/12/19/2824418.html http://man.linuxde.net/ps]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（39）-wc]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8839%EF%BC%89-wc%2F</url>
    <content type="text"><![CDATA[Linux系统中的wc(Word Count)命令的功能为统计指定文件中的字节数、字数、行数，并将统计结果显示输出。 语法wc(选项)(参数) 选项1234567-c 统计字节数。-l 统计行数。-m 统计字符数。这个标志不能与 -c 标志一起使用。-w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L 打印最长行的长度。-help 显示帮助信息--version 显示版本信息 参数文件：需要统计的文件列表。 功能统计指定文件中的字节数、字数、行数，并将统计结果显示输出。该命令统计指定文件中的字节数、字数、行数。如果没有给出文件名，则从标准输入读取。wc同时也给出所指定文件的总统计数。 常用实例1）查看文件的字节数、字数、行数 1234567891011121314151617181920# cat test.txt hnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint# wc test.txt 7 8 70 test.txt# wc -l test.txt 7 test.txt# wc -c test.txt 70 test.txt# wc -w test.txt 8 test.txt# wc -m test.txt 70 test.txt# wc -L test.txt 17 test.txt 说明： 7 8 70 test.txt 行数 单词数 字节数 文件名 2）用wc命令怎么做到只打印统计数字不打印文件名 1234# wc -l test.txt 7 test.txt# cat test.txt |wc -l7 说明： 使用管道线，这在编写shell脚本时特别有用。 3）用来统计当前目录下的文件数 123456789101112#cd test6# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2014.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2015.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2016.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2017.log# ls -l | wc -l8 说明： 数量中包含当前目录 转载链接： http://www.cnblogs.com/peida/archive/2012/12/18/2822758.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（38）-grep]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8838%EF%BC%89-grep%2F</url>
    <content type="text"><![CDATA[grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 选项12345678910111213141516171819202122232425-a 不要忽略二进制数据。-A&lt;显示列数&gt; 除了显示符合范本样式的那一行之外，并显示该行之后的内容。-b 在显示符合范本样式的那一行之外，并显示该行之前的内容。-c 计算符合范本样式的列数。-C&lt;显示列数&gt;或-&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。-d&lt;进行动作&gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。-e&lt;范本样式&gt; 指定字符串作为查找文件内容的范本样式。-E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-f&lt;范本文件&gt; 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。-F 将范本样式视为固定字符串的列表。-G 将范本样式视为普通的表示法来使用。-h 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。-H 在显示符合范本样式的那一列之前，标示该列的文件名称。-i 忽略字符大小写的差别。-l 列出文件内容符合指定的范本样式的文件名称。-L 列出文件内容不符合指定的范本样式的文件名称。-n 在显示符合范本样式的那一列之前，标示出该列的编号。-q 不显示任何信息。-R/-r 此参数的效果和指定“-d recurse”参数相同。-s 不显示错误信息。-v 反转查找。-w 只显示全字符合的列。-x 只显示全列符合的列。-y 此参数效果跟“-i”相同。-o 只输出文件中匹配到的部分。 规则表达式grep的规则表达式: ^ #锚定行的开始 如：’^grep’匹配所有以grep开头的行。 \$ #锚定行的结束 如：’grep$’匹配所有以grep结尾的行。 . #匹配一个非换行符的字符 如：’gr.p’匹配gr后接一个任意字符，然后是p。 #匹配零个或多个先前字符 如：’*grep’匹配所有一个或多个空格后紧跟grep的行。 .* #一起用代表任意字符。 [] #匹配一个指定范围内的字符，如’[Gg]rep’匹配Grep和grep。 [^] #匹配一个不在指定范围内的字符，如：’[^A-FH-Z]rep’匹配不包含A-F和H-Z的一个字母开头，紧跟rep的行。 (..) #标记匹配字符，如’(love)‘，love被标记为1。 \&lt; #锚定单词的开始，如:’\&lt;grep’匹配包含以grep开头的单词的行。 \&gt; #锚定单词的结束，如’grep>‘匹配包含以grep结尾的单词的行。 x{m} #重复字符x，m次，如：’0{5}‘匹配包含5个o的行。 x{m,} #重复字符x,至少m次，如：’o{5,}‘匹配至少有5个o的行。 x{m,n} #重复字符x，至少m次，不多于n次，如：’o{5,10}‘匹配5–10个o的行。 \w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：’G\w*p’匹配以G后跟零个或多个文字或数字字符，然后是p。 \W #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \b #单词锁定符，如: ‘\bgrep\b’只匹配grep。 POSIX字符: 为了在不同国家的字符编码中保持一至，POSIX(The Portable Operating System Interface)增加了特殊的字符类，如[:alnum:]是[A-Za-z0-9]的另一个写法。要把它们放到[]号内才能成为正则表达式，如[A- Za-z0-9]或[[:alnum:]]。在linux下的grep除fgrep外，都支持POSIX的字符类。 [:alnum:] #文字数字字符 [:alpha:] #文字字符 [:digit:] #数字字符 [:graph:] #非空字符（非空格、控制字符） [:lower:] #小写字符 [:cntrl:] #控制字符 [:print:] #非空字符（包括空格） [:punct:] #标点符号 [:space:] #所有空白字符（新行，空格，制表符） [:upper:] #大写字符 [:xdigit:] #十六进制数字（0-9，a-f，A-F） 常用实例1）查找指定进程 123 ps -ef|grep svnroot 4943 1 0 Dec05 ? 00:00:00 svnserve -d -r /opt/svndata/grape/root 16867 16838 0 19:53 pts/0 00:00:00 grep svn 2）查找指定进程个数 12# ps -ef|grep -c svn 2 3）从文件中读取关键词进行搜索 12345678910111213141516# cat test.txt hnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint# cat test2.txt linuxRedhat# cat test.txt | grep -f test2.txthnlinuxubuntu linuxRedhatlinuxmint 说明： 输出test.txt文件中含有从test2.txt文件中读取出的关键词的内容行 4）从文件中读取关键词进行搜索且显示行号 12345678910111213141516# cat test.txt hnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint# cat test2.txt linuxRedhat# cat test.txt | grep -nf test2.txt1:hnlinux4:ubuntu linux6:Redhat7:linuxmint 说明： 输出test.txt文件中含有从test2.txt文件中读取出的关键词的内容行，并显示每一行的行号 5）从文件中查找关键词 12345678# grep &apos;linux&apos; test.txt hnlinuxubuntu linuxlinuxmint# grep -n &apos;linux&apos; test.txt 1:hnlinux4:ubuntu linux7:linuxmint 6）从多个文件中查找关键词 12345678910# grep -n &apos;linux&apos; test.txt test2.txt test.txt:1:hnlinuxtest.txt:4:ubuntu linuxtest.txt:7:linuxminttest2.txt:1:linux# grep &apos;linux&apos; test.txt test2.txt test.txt:hnlinuxtest.txt:ubuntu linuxtest.txt:linuxminttest2.txt:linux 说明： 多文件时，输出查询到的信息内容行时，会把文件的命名在行最前面输出并且加上”:”作为标示符 7）grep不显示本身进程 12345678910# ps aux|grep sshroot 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0 root 16901 0.0 0.0 61180 764 pts/0 S+ 20:31 0:00 grep ssh# ps aux|grep [s]shroot 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0 # ps aux | grep ssh | grep -v &quot;grep&quot;root 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0 8）找出已u开头的行内容 123# cat test.txt |grep ^uubuntuubuntu linux 9）输出非u开头的行内容 123456# cat test.txt |grep ^[^u]hnlinuxpeida.cnblogs.comredhatRedhatlinuxmint 10）输出以hat结尾的行内容 123# cat test.txt |grep hat$redhatRedhat 11）查服务器ip地址所在行 1234# ifconfig eth0|grep &quot;[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;&quot; inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0# ifconfig eth0|grep -E &quot;([0-9]&#123;1,3&#125;\.)&#123;3&#125;[0-9]&quot; inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 12）显示包含ed或者at字符的内容行 12345# cat test.txt |grep -E &quot;peida|com&quot;peida.cnblogs.com# cat test.txt |grep -E &quot;ed|at&quot;redhatRedhat 13）显示当前目录下面以.txt 结尾的文件中的所有包含每个字符串至少有7个连续小写字符的字符串的行 1234# grep &apos;[a-z]\&#123;7\&#125;&apos; *.txttest.txt:hnlinuxtest.txt:peida.cnblogs.comtest.txt:linuxmint 14）在多级目录中对文本进行递归搜索 1#grep &quot;text&quot; . -r -n # .表示当前目录。 15）显示过滤注释( # ; 开头) 和空行后的配置信息 1# grep -Ev &quot;^$|^[#;]&quot; server.conf 16）过滤/etc/passwd文件下，包含root的行，并过滤后两行 1grep -A 2 root /etc/passwd 17）过滤/etc/passwd文件下，包含root的行，并过滤前两行 1grep -B 2 root /etc/passwd 18）过滤/etc/passwd文件下，包含root的行，并过滤前后两行 1grep -C 2 root /etc/passwd 18）过滤/etc/passwd文件下，包含root的行数 1grep -c root /etc/passwd 19)过滤/etc/passwd文件下，包含root的行，并打印行号 1grep -n root /etc/passwd 20）过滤/etc/下所有文件，包含root的行 1grep -r root /etc/ 21)过滤/etc/下所有文件，包含root的行的文件名 1grep -rl root /etc/ 参考链接： http://man.linuxde.net/grep http://www.cnblogs.com/peida/archive/2012/12/17/2821195.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（37）-cal]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8837%EF%BC%89-cal%2F</url>
    <content type="text"><![CDATA[cal命令用于显示当前日历，或者指定日期的日历。 语法cal(选项)(参数) 选项123456-l：显示单月输出；-3：显示临近三个月的日历；-s：将星期日作为月的第一天；-m：将星期一作为月的第一天；-j：显示“julian”日期；-y：显示当前年的日历。 参数12月：指定月份；年：指定年份。 常用实例1）显示当前月份日历 12345678# cal March 2018 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 1011 12 13 14 15 16 1718 19 20 21 22 23 2425 26 27 28 29 30 31 2）显示指定月份的日历 1234567891011121314151617# cal 9 2012 九月 2012 日 一 二 三 四 五 六 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1516 17 18 19 20 21 2223 24 25 26 27 28 2930 3）显示2013年日历 12345678910111213141516171819202122232425262728293031323334# cal -y 2013 2013 January February March Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 1 2 1 2 6 7 8 9 10 11 12 3 4 5 6 7 8 9 3 4 5 6 7 8 913 14 15 16 17 18 19 10 11 12 13 14 15 16 10 11 12 13 14 15 1620 21 22 23 24 25 26 17 18 19 20 21 22 23 17 18 19 20 21 22 2327 28 29 30 31 24 25 26 27 28 24 25 26 27 28 29 30 31 April May June Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 1 2 3 4 1 7 8 9 10 11 12 13 5 6 7 8 9 10 11 2 3 4 5 6 7 814 15 16 17 18 19 20 12 13 14 15 16 17 18 9 10 11 12 13 14 1521 22 23 24 25 26 27 19 20 21 22 23 24 25 16 17 18 19 20 21 2228 29 30 26 27 28 29 30 31 23 24 25 26 27 28 29 30 July August September Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 1 2 3 1 2 3 4 5 6 7 7 8 9 10 11 12 13 4 5 6 7 8 9 10 8 9 10 11 12 13 1414 15 16 17 18 19 20 11 12 13 14 15 16 17 15 16 17 18 19 20 2121 22 23 24 25 26 27 18 19 20 21 22 23 24 22 23 24 25 26 27 2828 29 30 31 25 26 27 28 29 30 31 29 30 October November December Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 1 2 1 2 3 4 5 6 7 6 7 8 9 10 11 12 3 4 5 6 7 8 9 8 9 10 11 12 13 1413 14 15 16 17 18 19 10 11 12 13 14 15 16 15 16 17 18 19 20 2120 21 22 23 24 25 26 17 18 19 20 21 22 23 22 23 24 25 26 27 2827 28 29 30 31 24 25 26 27 28 29 30 29 30 31 4）显示自1月1日的天数 12345678# cal -j March 2018 Sun Mon Tue Wed Thu Fri Sat 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 5）星期一显示在第一列 12345678# cal -m March 2018 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 1112 13 14 15 16 17 1819 20 21 22 23 24 2526 27 28 29 30 31 参考链接： http://man.linuxde.net/cal http://www.cnblogs.com/peida/archive/2012/12/14/2817473.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（36）-date]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8836%EF%BC%89-date%2F</url>
    <content type="text"><![CDATA[在linux环境中，不管是编程还是其他维护，时间是必不可少的，也经常会用到时间的运算，熟练运用date命令来表示自己想要表示的时间，肯定可以给自己的工作带来诸多方便。 语法date(选项)(参数) 选项12345-d&lt;字符串&gt;：显示字符串所指的日期与时间。字符串前后必须加上双引号；-s&lt;字符串&gt;：根据字符串来设置日期与时间。字符串前后必须加上双引号；-u：显示GMT；--help：在线帮助；--version：显示版本信息。 参数&lt;+时间日期格式&gt;：指定显示时使用的日期时间格式。 日期格式字符串列表123456789101112131415161718192021222324%H 小时，24小时制（00~23）%I 小时，12小时制（01~12）%k 小时，24小时制（0~23）%l 小时，12小时制（1~12）%M 分钟（00~59）%p 显示出AM或PM%r 显示时间，12小时制（hh:mm:ss %p）%s 从1970年1月1日00:00:00到目前经历的秒数%S 显示秒（00~59）%T 显示时间，24小时制（hh:mm:ss）%X 显示时间的格式（%H:%M:%S）%Z 显示时区，日期域（CST）%a 星期的简称（Sun~Sat）%A 星期的全称（Sunday~Saturday）%h,%b 月的简称（Jan~Dec）%B 月的全称（January~December）%c 日期和时间（Tue Nov 20 14:12:58 2012）%d 一个月的第几天（01~31）%x,%D 日期（mm/dd/yy）%j 一年的第几天（001~366）%m 月份（01~12）%w 一个星期的第几天（0代表星期天）%W 一年的第几个星期（00~53，星期一为第一天）%y 年的最后两个数字（1999则是99） 常用实例1）格式化输出： 12#date +&quot;%Y-%m-%d&quot;2018-03-29 2）输出昨天日期： 12#date -d &quot;1 day ago&quot; +&quot;%Y-%m-%d&quot;2018-03-28 3）2秒后输出： 12#date -d &quot;2 second&quot; +&quot;%Y-%m-%d %H:%M:%S&quot; 2018-03-29 10:08:37 4）apache格式转换： 12date -d &quot;Dec 5, 2009 12:00:37 AM&quot; +&quot;%Y-%m-%d %H:%M.%S&quot;2009-12-05 00:00.37 5）格式转换后时间游走： 12date -d &quot;Dec 5, 2009 12:00:37 AM 2 year ago&quot; +&quot;%Y-%m-%d %H:%M.%S&quot;2007-12-05 00:00.37 6）加减操作： 1234567date +%Y%m%d //显示前天年月日date -d &quot;+1 day&quot; +%Y%m%d //显示前一天的日期date -d &quot;-1 day&quot; +%Y%m%d //显示后一天的日期date -d &quot;-1 month&quot; +%Y%m%d //显示上一月的日期date -d &quot;+1 month&quot; +%Y%m%d //显示下一月的日期date -d &quot;-1 year&quot; +%Y%m%d //显示前一年的日期date -d &quot;+1 year&quot; +%Y%m%d //显示下一年的日期 7）设定时间 1234567date -s //设置当前时间，只有root权限才能设置，其他只能查看date -s 20120523 //设置成20120523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s &quot;01:01:01 2012-05-23&quot; //这样可以设置全部时间date -s &quot;01:01:01 20120523&quot; //这样可以设置全部时间date -s &quot;2012-05-23 01:01:01&quot; //这样可以设置全部时间date -s &quot;20120523 01:01:01&quot; //这样可以设置全部时间 8）有时需要检查一组命令花费的时间，举例： 12345678#!/bin/bashstart=$(date +%s)nmap man.linuxde.net &amp;&gt; /dev/nullend=$(date +%s)difference=$(( end - start ))echo $difference seconds. 参考链接： http://man.linuxde.net/date http://www.cnblogs.com/peida/archive/2012/12/13/2815687.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（35）-diff]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8835%EF%BC%89-diff%2F</url>
    <content type="text"><![CDATA[diff 命令是 linux上非常重要的工具，用于比较文件的内容，特别是比较两个版本不同的文件以找到改动的地方。diff在命令行中打印每一个行的改动。最新版本的diff还支持二进制文件。diff程序的输出被称为补丁 (patch)，因为Linux系统中还有一个patch程序，可以根据diff的输出将a.c的文件内容更新为b.c。diff是svn、cvs、git等版本控制工具不可或缺的一部分。 语法diff(选项)(参数) 选项12345678910111213141516171819202122232425262728293031323334-&lt;行数&gt;：指定要显示多少行的文本。此参数必须与-c或-u参数一并使用；-a或——text：diff预设只会逐行比较文本文件；-b或--ignore-space-change：不检查空格字符的不同；-B或--ignore-blank-lines：不检查空白行；-c：显示全部内容，并标出不同之处；-C&lt;行数&gt;或--context&lt;行数&gt;：与执行“-c-&lt;行数&gt;”指令相同；-d或——minimal：使用不同的演算法，以小的单位来做比较；-D&lt;巨集名称&gt;或ifdef&lt;巨集名称&gt;：此参数的输出格式可用于前置处理器巨集；-e或——ed：此参数的输出格式可用于ed的script文件；-f或-forward-ed：输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处；-H或--speed-large-files：比较大文件时，可加快速度；-l&lt;字符或字符串&gt;或--ignore-matching-lines&lt;字符或字符串&gt;：若两个文件在某几行有所不同，而之际航同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异；-i或--ignore-case：不检查大小写的不同；-l或——paginate：将结果交由pr程序来分页；-n或——rcs：将比较结果以RCS的格式来显示；-N或--new-file：在比较目录时，若文件A仅出现在某个目录中，预设会显示：Only in目录，文件A 若使用-N参数，则diff会将文件A 与一个空白的文件比较；-p：若比较的文件为C语言的程序码文件时，显示差异所在的函数名称；-P或--unidirectional-new-file：与-N类似，但只有当第二个目录包含了第一个目录所没有的文件时，才会将这个文件与空白的文件做比较；-q或--brief：仅显示有无差异，不显示详细的信息；-r或——recursive：比较子目录中的文件；-s或--report-identical-files：若没有发现任何差异，仍然显示信息；-S&lt;文件&gt;或--starting-file&lt;文件&gt;：在比较目录时，从指定的文件开始比较；-t或--expand-tabs：在输出时，将tab字符展开；-T或--initial-tab：在每行前面加上tab字符以便对齐；-u，-U&lt;列数&gt;或--unified=&lt;列数&gt;：以合并的方式来显示文件内容的不同；-v或——version：显示版本信息；-w或--ignore-all-space：忽略全部的空格字符；-W&lt;宽度&gt;或--width&lt;宽度&gt;：在使用-y参数时，指定栏宽；-x&lt;文件名或目录&gt;或--exclude&lt;文件名或目录&gt;：不比较选项中所指定的文件或目录；-X&lt;文件&gt;或--exclude-from&lt;文件&gt;；您可以将文件或目录类型存成文本文件，然后在=&lt;文件&gt;中指定此文本文件；-y或--side-by-side：以并列的方式显示文件的异同之处；--help：显示帮助；--left-column：在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容；--suppress-common-lines：在使用-y参数时，仅显示不同之处。 参数 文件1：指定要比较的第一个文件； 文件2：指定要比较的第二个文件。 功能diff命令能比较单个文件或者目录内容。如果指定比较的是文件，则只有当输入为文本文件时才有效。以逐行的方式，比较文本文件的异同处。如果指定比较的是目录的时候，diff命令会比较两个目录下名字相同的文本文件。列出不同的二进制文件、公共子目录和只在一个目录出现的文件。 常用实例1）比较两个文件 123456789101112# diff log2014.log log2013.log 3c3&lt; 2014-03---&gt; 2013-038c8&lt; 2013-07---&gt; 2013-0811,12d10&lt; 2013-11&lt; 2013-12 说明： 上面的“3c3”和“8c8”表示log2014.log和log20143log文件在3行和第8行内容有所不同；”11,12d10”表示第一个文件比第二个文件多了第11和12行。 diff 的normal 显示格式有三种提示: a - add c - change d - delete 2）并排格式输出 12345678910111213141516171819202122232425# diff log2014.log log2013.log -y -W 502013-01 2013-012013-02 2013-022014-03 | 2013-032013-04 2013-042013-05 2013-052013-06 2013-062013-07 2013-072013-07 | 2013-082013-09 2013-092013-10 2013-102013-11 &lt;2013-12 &lt;# diff log2013.log log2014.log -y -W 502013-01 2013-012013-02 2013-022013-03 | 2014-032013-04 2013-042013-05 2013-052013-06 2013-062013-07 2013-072013-08 | 2013-072013-09 2013-09 &gt; 2013-11 &gt; 2013-12 说明： “|”表示前后2个文件内容有不同 “&lt;”表示后面文件比前面文件少了1行内容 “&gt;”表示后面文件比前面文件多了1行内容 3）上下文输出格式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# diff log2013.log log2014.log -c*** log2013.log 2012-12-07 16:36:26.000000000 +0800--- log2014.log 2012-12-07 18:01:54.000000000 +0800****************** 1,10 **** 2013-01 2013-02! 2013-03 2013-04 2013-05 2013-06 2013-07! 2013-08 2013-09 2013-10--- 1,12 ---- 2013-01 2013-02! 2014-03 2013-04 2013-05 2013-06 2013-07! 2013-07 2013-09 2013-10+ 2013-11+ 2013-12# diff log2014.log log2013.log -c*** log2014.log 2012-12-07 18:01:54.000000000 +0800--- log2013.log 2012-12-07 16:36:26.000000000 +0800****************** 1,12 **** 2013-01 2013-02! 2014-03 2013-04 2013-05 2013-06 2013-07! 2013-07 2013-09 2013-10- 2013-11- 2013-12--- 1,10 ---- 2013-01 2013-02! 2013-03 2013-04 2013-05 2013-06 2013-07! 2013-08 2013-09 2013-10 说明： 这种方式在开头两行作了比较文件的说明，这里有三中特殊字符： “＋” 比较的文件的后者比前着多一行 “－” 比较的文件的后者比前着少一行 “！” 比较的文件两者有差别的行 4）统一格式输出 123456789101112131415161718# diff log2014.log log2013.log -u--- log2014.log 2012-12-07 18:01:54.000000000 +0800+++ log2013.log 2012-12-07 16:36:26.000000000 +0800@@ -1,12 +1,10 @@ 2013-01 2013-02-2014-03+2013-03 2013-04 2013-05 2013-06 2013-07-2013-07+2013-08 2013-09 2013-10-2013-11-2013-12 说明： 它的第一部分，也是文件的基本信息： — log2014.log 2012-12-07 18:01:54.000000000 +0800 +++ log2013.log 2012-12-07 16:36:26.000000000 +0800 “—“表示变动前的文件，”+++”表示变动后的文件。 第二部分，变动的位置用两个@作为起首和结束。 @@ -1,12 +1,10 @@ 前面的”-1,12”分成三个部分：减号表示第一个文件（即log2014.log），”1”表示第1行，”12”表示连续12行。合在一起，就表示下面是第一个文件从第1行开始的连续12行。同样的，”+1,10”表示变动后，成为第二个文件从第1行开始的连续10行。 5）比较文件夹不同 123456789101112131415161718192021222324252627282930313233343536# diff test3 test6Only in test6: linklog.logOnly in test6: log2012.logdiff test3/log2013.log test6/log2013.log1,10c1,3&lt; 2013-01&lt; 2013-02&lt; 2013-03&lt; 2013-04&lt; 2013-05&lt; 2013-06&lt; 2013-07&lt; 2013-08&lt; 2013-09&lt; 2013-10---&gt; hostnamebaidu=baidu.com&gt; hostnamesina=sina.com&gt; hostnames=truediff test3/log2014.log test6/log2014.log1,12d0&lt; 2013-01&lt; 2013-02&lt; 2014-03&lt; 2013-04&lt; 2013-05&lt; 2013-06&lt; 2013-07&lt; 2013-07&lt; 2013-09&lt; 2013-10&lt; 2013-11&lt; 2013-12Only in test6: log2015.logOnly in test6: log2016.logOnly in test6: log2017.log 6）比较两个文件不同，并生产补丁 123456789101112131415161718192021222324# diff -ruN log2013.log log2014.log &gt;patch.log# ll总计 12-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log-rw-r--r-- 1 root root 96 12-07 18:01 log2014.log-rw-r--r-- 1 root root 248 12-07 21:33 patch.log# cat patch.log --- log2013.log 2012-12-07 16:36:26.000000000 +0800+++ log2014.log 2012-12-07 18:01:54.000000000 +0800@@ -1,10 +1,12 @@ 2013-01 2013-02-2013-03+2014-03 2013-04 2013-05 2013-06 2013-07-2013-08+2013-07 2013-09 2013-10+2013-11+2013-12 7）打补丁 1234567891011121314151617181920212223242526# cat log2013.log2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-10# patch log2013.log patch.log patching file log2013.log# cat log2013.log 2013-012013-022014-032013-042013-052013-062013-072013-072013-092013-102013-112013-12 参考链接： http://www.cnblogs.com/peida/archive/2012/12/12/2814048.html http://man.linuxde.net/diff]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（34）-ln]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8834%EF%BC%89-ln%2F</url>
    <content type="text"><![CDATA[ln是linux中又一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同步的链接.当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。 语法ln(选项)(参数) 选项1234567891011-b或--backup：删除，覆盖目标文件之前的备份；-d或-F或——directory：建立目录的硬连接；-f或——force：强行建立文件或目录的连接，不论文件或目录是否存在；-i或——interactive：覆盖既有文件之前先询问用户；-n或--no-dereference：把符号连接的目的目录视为一般文件；-s或——symbolic：对源文件建立符号连接，而非硬连接；-S&lt;字尾备份字符串&gt;或--suffix=&lt;字尾备份字符串&gt;：用&quot;-b&quot;参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，预设的备份字符串是符号“~”，用户可通过“-S”参数来改变它；-v或——verbose：显示指令执行过程；-V&lt;备份方式&gt;或--version-control=&lt;备份方式&gt;：用“-b”参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，这个字符串不仅可用“-S”参数变更，当使用“-V”参数&lt;备份方式&gt;指定不同备份方式时，也会产生不同字尾的备份字符串；--help：在线帮助；--version：显示版本信息。 参数 源文件：指定连接的源文件。如果使用-s选项创建符号连接，则“源文件”可以是文件或者目录。创建硬连接时，则“源文件”参数只能是文件； 目标文件：指定源文件的目标连接文件。 功能linux文件系统中，有所谓的链接（link），我们可以将其视为档案的别名，而链接又可分为两种：硬链接（hard link）与软链接（symbolic link），硬链接的意思是一个档案可以有多个名称，而软链接的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬链接是存在同一个系统中，而软链接却可以跨越不同的文件系统。 软链接： 1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 2.软链接可以 跨文件系统 ，硬链接不可以 3.软链接可以对一个不存在的文件名进行链接 4.软链接可以对目录进行链接 硬链接: 1.硬链接，以文件副本的形式存在。但不占用实际空间。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 这里有两点要注意： 第一，ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二，ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 常用范例1）给文件创建软链接 123456# ll-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log# ln -s log2013.log link2013# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log文件创建软链接link2013，如果log2013.log丢失，link2013将失效 2）给文件创建硬链接 12345678# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log# ln log2013.log ln2013# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 2 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log创建硬链接ln2013，log2013.log与ln2013的各项属性相同 3）接上面两实例，链接完毕后，删除和重建链接原文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 2 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root bin 61 11-13 06:03 log2013.log# rm -rf log2013.log # lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013# touch log2013.log# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013---xrw-r-- 1 root bin 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 0 12-07 16:19 log2013.log# vi log2013.log 2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-102013-112013-12# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 1 root root 96 12-07 16:21 log2013.log# cat link2013 2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-102013-112013-12# cat ln2013 hostnamebaidu=baidu.comhostnamesina=sina.comhostnames=true 说明： 1.源文件被删除后，并没有影响硬链接文件；软链接文件在centos系统下不断的闪烁，提示源文件已经不存在 2.重建源文件后，软链接不在闪烁提示，说明已经链接成功，找到了链接文件系统；重建后，硬链接文件并没有受到源文件影响，硬链接文件的内容还是保留了删除前源文件的内容，说明硬链接已经失效 4）将文件链接为另一个目录中的相同名字 1234567891011121314151617181920212223242526# ln log2013.log test3# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root root 96 12-07 16:21 log2013.log# cd test3# ll-rw-r--r-- 2 root root 96 12-07 16:21 log2013.log# vi log2013.log 2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-10# ll-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log# cd ..# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log 说明： 在test3目录中创建了log2013.log的硬链接，修改test3目录中的log2013.log文件，同时也会同步到源文件 5）给目录创建软链接 123456789101112131415161718192021222324252627# lldrwxr-xr-x 2 root root 4096 12-07 16:36 test3drwxr-xr-x 2 root root 4096 12-07 16:57 test5# cd test5# lllrwxrwxrwx 1 root root 5 12-07 16:57 test3 -&gt; test3# cd test3-bash: cd: test3: 符号连接的层数过多# lllrwxrwxrwx 1 root root 5 12-07 16:57 test3 -&gt; test3# rm -rf test3# ll# ln -sv /opt/soft/test/test3 /opt/soft/test/test5创建指向“/opt/soft/test/test3”的符号链接“/opt/soft/test/test5/test3”# lllrwxrwxrwx 1 root root 20 12-07 16:59 test3 -&gt; /opt/soft/test/test3# # cd test3# ll总计 4-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log# touch log2014.log# ll总计 4-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log-rw-r--r-- 1 root root 0 12-07 17:05 log2014.log 说明： 1.目录只能创建软链接 2.目录创建链接必须用绝对路径，相对路径创建会不成功，会提示：符号连接的层数过多 这样的错误 3.在链接目标目录中修改文件都会在源文件目录中同步变化 参考链接： http://www.cnblogs.com/peida/archive/2012/12/11/2812294.html http://man.linuxde.net/ln]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（33）-du]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8833%EF%BC%89-du%2F</url>
    <content type="text"><![CDATA[du命令也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的。 语法du [选项][文件] 选项123456789101112131415-a或-all 显示目录中个别文件的大小。-b或-bytes 显示目录或文件大小时，以byte为单位。-c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。-k或--kilobytes 以KB(1024bytes)为单位输出。-m或--megabytes 以MB为单位输出。-s或--summarize 仅显示总计，只列出最后加总的值。-h或--human-readable 以K，M，G为单位，提高信息的可读性。-x或--one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。-L&lt;符号链接&gt;或--dereference&lt;符号链接&gt; 显示选项中所指定符号链接的源文件大小。-S或--separate-dirs 显示个别目录的大小时，并不含其子目录的大小。-X&lt;文件&gt;或--exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件。--exclude=&lt;目录或文件&gt; 略过指定的目录或文件。-D或--dereference-args 显示指定符号链接的源文件大小。-H或--si 与-h参数相同，但是K，M，G是以1000为换算单位。-l或--count-links 重复计算硬件链接的文件。 常用实例1）显示目录或者文件所占空间 12345678910111213# du608 ./test6308 ./test44 ./scf/lib4 ./scf/service/deploy/product4 ./scf/service/deploy/info12 ./scf/service/deploy16 ./scf/service4 ./scf/doc4 ./scf/bin32 ./scf8 ./test31288 . 说明： 只显示当前目录下面的子目录的目录大小和当前目录的总的大小，最下面的1288为当前目录的总大小 2）显示指定文件所占空间 12# du log2012.log 300 log2012.log 3）查看指定目录的所占空间 123456789# du scf4 scf/lib4 scf/service/deploy/product4 scf/service/deploy/info12 scf/service/deploy16 scf/service4 scf/doc4 scf/bin32 scf 4）显示多个文件所占空间 123# du log30.tar.gz log31.tar.gz 4 log30.tar.gz4 log31.tar.gz 5）只显示总和的大小 1234567# du -s1288 .# du -s scf32 scf# cd ..# du -s test1288 test 6）方便阅读的格式显示 123456# du -h test608K test/test6308K test/test44.0K test/scf/lib4.0K test/scf/service/deploy/product4.0K test/scf/service/deploy/info 7）文件和目录都显示 123456789101112# du -ah test4.0K test/log31.tar.gz4.0K test/test13.tar.gz0 test/linklog.log0 test/test6/log2014.log300K test/test6/linklog.log0 test/test6/log2015.log4.0K test/test6/log2013.log300K test/test6/log2012.log0 test/test6/log2017.log0 test/test6/log2016.log608K test/test6 8）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和 1234# du -c log30.tar.gz log31.tar.gz 4 log30.tar.gz4 log31.tar.gz8 总计 说明： 加上-c选项后，du不仅显示两个目录各自占用磁盘空间的大小，还在最后一行统计它们的总和。 9）按照空间大小排序 1234567# du|sort -nr|more1288 .608 ./test6308 ./test432 ./scf16 ./scf/service12 ./scf/service/deploy 10）输出当前目录下各个子目录所使用的空间 123456# du -h --max-depth=1608K ./test6308K ./test432K ./scf8.0K ./test31.3M . 参考链接： http://www.cnblogs.com/peida/archive/2012/12/10/2810755.html http://man.linuxde.net/du]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（32）-df]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8832%EF%BC%89-df%2F</url>
    <content type="text"><![CDATA[df命令用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 语法df(选项)(参数) 选项12345678910111213141516-a或--all：包含全部的文件系统；--block-size=&lt;区块大小&gt;：以指定的区块大小来显示区块数目；-h或--human-readable：以可读性较高的方式来显示信息；-H或--si：与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes；-i或--inodes：显示inode的信息；-k或--kilobytes：指定区块大小为1024字节；-l或--local：仅显示本地端的文件系统；-m或--megabytes：指定区块大小为1048576字节；--no-sync：在取得磁盘使用信息前，不要执行sync指令，此为预设值；-P或--portability：使用POSIX的输出格式；--sync：在取得磁盘使用信息前，先执行sync指令；-t&lt;文件系统类型&gt;或--type=&lt;文件系统类型&gt;：仅显示指定文件系统类型的磁盘信息；-T或--print-type：显示文件系统的类型；-x&lt;文件系统类型&gt;或--exclude-type=&lt;文件系统类型&gt;：不要显示指定文件系统类型的磁盘信息；--help：显示帮助；--version：显示版本信息。 参数文件： 指定文件系统上的文件 常用实例1）查看系统磁盘设备，默认是KB为单位： 123456# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda2 146294492 28244432 110498708 21% //dev/sda1 1019208 62360 904240 7% /boottmpfs 1032204 0 1032204 0% /dev/shm/dev/sdb1 2884284108 218826068 2518944764 8% /data1 2）使用-h选项以KB以上的单位来显示，可读性高： 123456# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda2 140G 27G 106G 21% //dev/sda1 996M 61M 884M 7% /boottmpfs 1009M 0 1009M 0% /dev/shm/dev/sdb1 2.7T 209G 2.4T 8% /data1 3）查看全部文件系统： 12345678910# df -a文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda2 146294492 28244432 110498708 21% /proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/pts/dev/sda1 1019208 62360 904240 7% /boottmpfs 1032204 0 1032204 0% /dev/shm/dev/sdb1 2884284108 218826068 2518944764 8% /data1none 0 0 0 - /proc/sys/fs/binfmt_misc 转载链接： http://man.linuxde.net/df]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（31）-gzip]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8831%EF%BC%89-gzip%2F</url>
    <content type="text"><![CDATA[gzip命令用来压缩文件。gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多处“.gz”扩展名。 gzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。gzip不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和tar命令一起构成Linux操作系统中比较流行的压缩文件格式。据统计，gzip命令对文本文件有60%～70%的压缩率。减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。 语法gzip(选项)(参数) 选项1234567891011121314151617-a或——ascii：使用ASCII文字模式；-d或--decompress或----uncompress：解开压缩文件；-f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；-h或——help：在线帮助；-l或——list：列出压缩文件的相关信息；-L或——license：显示版本与版权信息；-n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记；-N或——name：压缩文件时，保存原来的文件名称及时间戳记；-q或——quiet：不显示警告信息；-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串；-t或——test：测试压缩文件是否正确无误；-v或——verbose：显示指令执行过程；-V或——version：显示版本信息；-&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；--best：此参数的效果和指定“-9”参数相同；--fast：此参数的效果和指定“-1”参数相同。 参数文件列表：指定要压缩的文件列表。 常用范例1）把test6目录下的每个文件压缩成.gz文件 1234567891011# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log# gzip *# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz 2）把例1中每个压缩的文件解压，并列出详细的信息 1234567891011121314# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz# gzip -dv *linklog.log.gz: 99.6% -- replaced with linklog.loglog2012.log.gz: 99.6% -- replaced with log2012.loglog2013.log.gz: 47.5% -- replaced with log2013.log# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log 3）详细显示例1中每个压缩的文件的信息，并不解压 12345# gzip -l * compressed uncompressed ratio uncompressed_name 1341 302108 99.6% linklog.log 1341 302108 99.6% log2012.log 70 61 47.5% log2013.log 4）压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gz 12345#ls -al log.tar-rw-r--r-- 1 root root 307200 11-29 17:54 log.tar# gzip -r log.tar# ls -al log.tar.gz -rw-r--r-- 1 root root 1421 11-29 17:54 log.tar.gz 5）递归的压缩目录 12345678910111213141516# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log# cd ..# gzip -rv test6test6/linklog.log: 99.6% -- replaced with test6/linklog.log.gztest6/log2013.log: 47.5% -- replaced with test6/log2013.log.gztest6/log2012.log: 99.6% -- replaced with test6/log2012.log.gz# cd test6# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz 说明： 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。 6）递归地解压目录 12345678910111213# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz# cd ..# gzip -dr test6# cd test6# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log 参考链接： http://www.cnblogs.com/peida/archive/2012/12/06/2804323.html http://man.linuxde.net/gzip]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（30）-/etc/group文件详解]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8830%EF%BC%89-etc-group%2F</url>
    <content type="text"><![CDATA[Linux /etc/group文件与/etc/passwd和/etc/shadow文件都是有关于系统管理员对用户和用户组管理时相关的文件。linux /etc/group文件是有关于系统管理员对用户和用户组管理的文件,linux用户组的所有信息都存放在/etc/group文件中。具有某种共同特征的用户集合起来就是用户组（Group）。用户组（Group）配置文件主要有 /etc/group和/etc/gshadow，其中/etc/gshadow是/etc/group的加密信息文件。 将用户分组是Linux系统中对用户进行管理及控制访问权限的一种手段。每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不同的组。当一个用户同时是多个组中的成员时，在/etc/passwd文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。 用户组的所有信息都存放在/etc/group文件中。此文件的格式是由冒号(:)隔开若干个字段，这些字段具体如下： 组名:口令:组标识号:组内用户列表 具体解释 组名：组名是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。 口令：口令字段存放的是用户组加密后的口令字。一般Linux系统的用户组都没有口令，即这个字段一般为空，或者是*。 组标识号：组标识号与用户标识号类似，也是一个整数，被系统内部用来标识组。别称GID. 组内用户列表：属于这个组的所有用户的列表，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。 使用实例12345# cat /etc/grouproot:x:0:root,linuxsirbin:x:1:root,bin,daemondaemon:x:2:root,bin,daemonsys:x:3:root,bin 说明： ​ 我们以root:\x:0:root,linuxsir 为例： 用户组root，x是密码段，表示没有设置密码，GID是0,root用户组下包括root、linuxsir以及GID为0的其它用户。 转载链接： http://www.cnblogs.com/peida/archive/2012/12/05/2802419.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（29）-chown]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8829%EF%BC%89-chown%2F</url>
    <content type="text"><![CDATA[chown将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户ID；组可以是组名或者组ID；文件是以空格分开的要改变权限的文件列表，支持通配符。系统管理员经常使用chown命令，在将文件拷贝到另一个用户的名录下之后，让用户拥有使用该文件的权限。 语法chown(选项)(参数) 选项123456789-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quite或——silent：不显示错误信息；-h或--no-dereference：只对符号连接的文件作修改，而不更改其他任何相关文件；-R或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-v或——version：显示指令执行过程；--dereference：效果和“-h”参数相同；--help：在线帮助；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同；--version：显示版本信息。 参数用户：组：指定所有者和所属工作组。当省略“：组”，仅改变文件所有者；文件：指定要改变所有者和工作组的文件列表。支持多个文件和目标，支持shell通配符。 功能通过chown改变文件的拥有者和群组。在更改文件的所有者或所属群组时，可以使用用户名称和用户识别码设置。普通用户不能将自己的文件改变成其他的拥有者。其操作权限一般为管理员。 常用实例1）改变拥有者和群组 1234567# ll---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root users 302108 11-30 08:39 log2012.log# chown mail:mail log2012.log # ll---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 mail mail 302108 11-30 08:39 log2012.log 2）改变文件拥有者 123456789ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 mail mail 302108 11-30 08:39 log2012.log# chown root: log2012.log # ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root mail 302108 11-30 08:39 log2012.log 3）改变文件群组 123456789# ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root root 302108 11-30 08:39 log2012.log# chown :mail log2012.log # ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root mail 302108 11-30 08:39 log2012.log 4）改变指定目录以及其子目录下的所有文件的拥有者和群组 1234567891011121314151617181920212223# lldrwxr-xr-x 2 root users 4096 11-30 08:39 test6# chown -R -v root:mail test6“test6/log2014.log” 的所有者已更改为 root:mail“test6/linklog.log” 的所有者已更改为 root:mail“test6/log2015.log” 的所有者已更改为 root:mail“test6/log2013.log” 的所有者已更改为 root:mail“test6/log2012.log” 的所有者已保留为 root:mail“test6/log2017.log” 的所有者已更改为 root:mail“test6/log2016.log” 的所有者已更改为 root:mail“test6” 的所有者已更改为 root:mail# lldrwxr-xr-x 2 root mail 4096 11-30 08:39 test6# cd test6# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 root mail 302108 11-30 08:39 log2012.log-rw-r--r-- 1 root mail 61 11-30 08:39 log2013.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2014.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2015.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2016.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2017.log 参考链接： http://www.cnblogs.com/peida/archive/2012/12/04/2800684.html http://man.linuxde.net/chown]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（28）-chgrp]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8828%EF%BC%89-chgrp%2F</url>
    <content type="text"><![CDATA[chgrp命令用来改变文件或目录所属的用户组。该命令用来改变指定文件所属的用户组。其中，组名可以是用户组的id，也可以是用户组的组名。文件名可以 是由空格分开的要改变属组的文件列表，也可以是由通配符描述的文件集合。如果用户不是该文件的文件主或超级用户(root)，则不能改变该文件的组。 在UNIX系统家族里，文件或目录权限的掌控以拥有者及所属群组来管理。您可以使用chgrp指令去变更文件与目录的所属群组，设置方式采用群组名称或群组识别码皆可。 语法chgrp(选项)(参数) 选项123456-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quiet或——silent：不显示错误信息；-h或--no-dereference：只对符号连接的文件作修改，而不是该其他任何相关文件；-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；-v或——verbose：显示指令执行过程；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同； 参数组：指定新工作名称 文件： 指定要改变所属组的文件 常用实例将/usr/meng及其子目录下的所有文件的用户组改为mengxin 1chgrp -R mengxin /usr/meng 转载地址： http://man.linuxde.net/chgrp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（27）-tar]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8827%EF%BC%89-tar%2F</url>
    <content type="text"><![CDATA[tar命令可以为linux的文件和目录创建档案。利用tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用tar命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。 首先要弄清两个概念：打包和压缩。打包时指将一大堆文件或目录变成为一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 语法tar(选项)(参数) 选项1234567891011121314151617181920212223-A或--catenate：新增文件到以存在的备份文件；-B：设置区块大小；-c或--create：建立新的备份文件；-C &lt;目录&gt;：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。-d：记录文件的差别；-x或--extract或--get：从备份文件中还原文件；-t或--list：列出备份文件的内容；-z或--gzip或--ungzip：通过gzip指令处理备份文件；-Z或--compress或--uncompress：通过compress指令处理备份文件；-f&lt;备份文件&gt;或--file=&lt;备份文件&gt;：指定备份文件；-v或--verbose：显示指令执行过程；-r：添加文件到已经压缩的文件；-u：添加改变了和现有的文件到已经存在的压缩文件；-j：支持bzip2解压文件；-v：显示操作过程；-l：文件系统边界设置；-k：保留原有文件不覆盖；-m：保留文件不被覆盖；-w：确认压缩文件的正确性；-p或--same-permissions：用原来的文件权限还原文件；-P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号；-N &lt;日期格式&gt; 或 --newer=&lt;日期时间&gt;：只将较指定日期更新的文件保存到备份文件里；--exclude=&lt;范本样式&gt;：排除符合范本样式的文件。 参数文件或目录：指定要打包的文件或目录列表。 常用实例1）将文件全部打包成tar包： 123tar -cvf log.tar log2012.log 仅打包，不压缩！ tar -zcvf log.tar.gz log2012.log 打包后，以 gzip 压缩 tar -jcvf log.tar.bz2 log2012.log 打包后，以 bzip2 压缩 在选项f之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加z选项，则以.tar.gz或.tgz来代表gzip压缩过的tar包；如果加j选项，则以.tar.bz2来作为tar包名。 2）查阅上述tar包内有哪些文件： 1tar -ztvf log.tar.gz 由于我们使用 gzip 压缩的log.tar.gz，所以要查阅log.tar.gz包内的文件时，就得要加上z这个选项了。 3）将tar包解压缩： 1tar -zxvf /opt/soft/test/log.tar.gz 在预设的情况下，我们可以将压缩档在任何地方解开的 4）只将tar内的部分文件解压出来： 1tar -zxvf /opt/soft/test/log30.tar.gz log2013.log 5）文件备份下来，并且保存其权限： 1tar -zcvpf log31.tar.gz log2014.log log2015.log log2016.log 这个-p的属性是很重要的，尤其是当您要保留原本文件的属性时。 6）在文件夹当中，比某个日期新的文件才备份： 1tar -N &quot;2012/11/13&quot; -zcvf log17.tar.gz test 7）备份文件夹内容是排除部分文件： 1tar --exclude scf/service -zcvf scf.tar.gz scf/* 8）其实最简单的使用 tar 就只要记忆底下的方式即可： 123压 缩：tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称查 询：tar -jtv -f filename.tar.bz2解压缩：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录 转载链接： http://man.linuxde.net/tar]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（26）-chmod]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8826%EF%BC%89-chmod%2F</url>
    <content type="text"><![CDATA[chmod命令用于改变linux系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 权限范围的表示法如下： u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户；r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”；- 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。 语法chmod(选项)(参数) 选项1234-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quiet或——silent：不显示错误信息；-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；-v或——verbose：显示指令执行过程； 参数权限模式：指定文件的权限模式；文件：要改变权限的文件。 知识扩展和实例Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內！ linux文件的用户权限的分析图 例：rwx rw- r– r=读取属性 //值＝4w=写入属性 //值＝2x=执行属性 //值＝1 1234chmod u+x,g+w f01 //为文件f01设置自己可以执行，组员可以写入的权限chmod u=rwx,g=rw,o=r f01chmod 764 f01chmod a+x f01 //对文件f01的u,g,o都设置可执行属性 文件的属主和属组属性设置 12chown user:market f01 //把文件f01给uesr，添加到market组ll -d f1 查看目录f1的属性 参考链接： http://man.linuxde.net/chmod http://www.cnblogs.com/peida/archive/2012/11/29/2794010.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（25）-linux文件属性详解]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8825%EF%BC%89-linux%E6%96%87%E4%BB%B6%E5%B1%9E%E6%80%A7%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Linux 文件或目录的属性主要包括：文件或目录的节点、种类、权限模式、链接数量、所归属的用户和用户组、最近访问或修改的时间等内容。具体情况如下： 123456789101112# ls -lih总计 316K2095120 lrwxrwxrwx 1 root root 11 11-22 06:58 linklog.log -&gt; log2012.log2095112 -rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log2095110 -rw-r--r-- 1 root root 61 11-13 06:03 log2013.log2095107 -rw-r--r-- 1 root root 0 11-13 06:03 log2014.log2095117 -rw-r--r-- 1 root root 0 11-13 06:06 log2015.log2095118 -rw-r--r-- 1 root root 0 11-16 14:41 log2016.log2095119 -rw-r--r-- 1 root root 0 11-16 14:43 log2017.log2095113 drwxr-xr-x 6 root root 4.0K 10-27 01:58 scf2095109 drwxrwxr-x 2 root root 4.0K 11-13 06:08 test32095131 drwxrwxr-x 2 root root 4.0K 11-13 05:50 test4 说明： 第一列：inode 第二列：文件种类和权限； 第三列： 硬链接个数； 第四列： 属主； 第五列：所归属的组； 第六列：文件或目录的大小； 第七列和第八列：最后访问或修改时间； 第九列：文件名或目录名 我们以log2012.log为例： 2095112 -rw-r–r– 1 root root 296K 11-13 06:03 log2012.log inode 的值是：2095112 文件类型：文件类型是-，表示这是一个普通文件； 关于文件的类型，请参考：每天一个linux命令(24)：Linux文件类型与扩展名 文件权限：文件权限是rw-r–r– ，表示文件属主可读、可写、不可执行，文件所归属的用户组不可写，可读，不可执行，其它用户不可写，可读，不可执行； 硬链接个数： log2012.log这个文件没有硬链接；因为数值是1，就是他本身； 文件属主：也就是这个文件归哪于哪个用户 ，它归于root，也就是第一个root； 文件属组：也就是说，对于这个文件，它归属于哪个用户组，在这里是root用户组； 文件大小：文件大小是296k个字节； 访问可修改时间 ：这里的时间是最后访问的时间，最后访问和文件被修改或创建的时间，有时并不是一致的； 当然文档的属性不仅仅包括这些，这些是我们最常用的一些属性。 关于inode： inode 译成中文就是索引节点。每个存储设备或存储设备的分区（存储设备是硬盘、软盘、U盘等等）被格式化为文件系统后，应该有两部份，一部份是inode，另一部份是Block，Block是用来存储数据用的。而inode呢，就是用来存储这些数 据的信息，这些信息包括文件大小、属主、归属的用户组、读写权限等。inode为每个文件进行信息索引，所以就有了inode的数值。操作系统根据指令， 能通过inode值最快的找到相对应的文件。 做个比喻，比如一本书，存储设备或分区就相当于这本书，Block相当于书中的每一页，inode 就相当于这本书前面的目录，一本书有很多的内容，如果想查找某部份的内容，我们可以先查目录，通过目录能最快的找到我们想要看的内容。虽然不太恰当，但还是比较形象。 当我们用ls 查看某个目录或文件时，如果加上-i 参数，就可以看到inode节点了；比如我们前面所说的例子： ls -li log2012.log 12# ls -li log2012.log 2095112 -rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log log2012.log 的inode值是 2095112 ； 查看一个文件或目录的inode，要通过ls 命令的的 -i参数。 转载链接： http://www.cnblogs.com/peida/archive/2012/11/23/2783762.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（24）-Linux文件类型与扩展名]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8824%EF%BC%89-linux%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%89%A9%E5%B1%95%E5%90%8D%2F</url>
    <content type="text"><![CDATA[Linux文件类型和Linux文件的文件名所代表的意义是两个不同的概念。我们通过一般应用程序而创建的比如file.txt、file.tar.gz ，这些文件虽然要用不同的程序来打开，但放在Linux文件类型中衡量的话，大多是常规文件（也被称为普通文件）。 文件类型Linux文件类型常见的有：普通文件、目录文件、字符设备文件和块设备文件、符号链接文件等，现在我们进行一个简要的说明。 普通文件我们用 ls -lh 来查看某个文件的属性，可以看到有类似-rwxrwxrwx，值得注意的是第一个符号是 - ，这样的文件在Linux中就是普通文件。这些文件一般是用一些相关的应用程序创建，比如图像工具、文档工具、归档工具… …. 或 cp工具等。这类文件的删除方式是用rm 命令。 另外，依照文件的内容，又大略可以分为： 1）纯文本档（ASCII）： 这是linux系统中最多的一种文件类型，称为纯文本档是因为人类可以直接读到的数据，例如数字、字母等等。几乎只要我们可以用来做为设定的文件都属于这一种文件类型。举例来说，你可以用命令： cat ~/.bashrc 来看到该文件内容。（cat是将一个文件内容读出来的指令）。 2） 二进制文件(binary)： Linux系统其实仅认识且可以执行二进制文件(binary file)。Linux当中的可执行文件(scripts, 文字型批处理文件不算)就是这种格式的文件。 刚刚使用的命令cat就是一个binary file。 3）数据格式文件(data)： 有些程序在运作的过程当中会读取某些特定格式的文件，那些特定格式的文件可以被称为数据文件 (data file)。举例来说，我们的Linux在使用者登录时，都会将登录的数据记录在 /var/log/wtmp那个文件内，该文件是一个data file，他能够透过last这个指令读出来！ 但是使用cat时，会读出乱码～因为他是属于一种特殊格式的文件？ 目录文件当我们在某个目录下执行，看到有类似 drwxr-xr-x ，这样的文件就是目录，目录在Linux是一个比较特殊的文件。注意它的第一个字符是d。创建目录的命令可以用 mkdir 命令，或cp命令，cp可以把一个目录复制为另一个目录。删除用rm 或rmdir命令。 字符设备或块设备文件当您进入/dev目录，列一下文件，会看到类似如下的: 1234# ls -al /dev/ttycrw-rw-rw- 1 root tty 5, 0 11-03 15:11 /dev/tty# ls -la /dev/sda1brw-r----- 1 root disk 8, 1 11-03 07:11 /dev/sda1 我们看到/dev/tty的属性是 crw-rw-rw- ，注意前面第一个字符是 c ，这表示字符设备文件。比如猫等串口设备。我们看到 /dev/sda1 的属性是 brw-r—– ，注意前面的第一个字符是b，这表示块设备，比如硬盘，光驱等设备。 这个种类的文件，是用mknode来创建，用rm来删除。目前在最新的Linux发行版本中，我们一般不用自己来创建设备文件。因为这些文件是和内核相关联的。 与系统周边及储存等相关的一些文件， 通常都集中在/dev这个目录之下！通常又分为两种： 区块(block)设备档 ： 就是一些储存数据， 以提供系统随机存取的接口设备，举例来说，硬盘与软盘等就是啦！ 你可以随机的在硬盘的不同区块读写，这种装置就是成组设备！你可以自行查一下/dev/sda看看， 会发现第一个属性为[ b ]！ 字符(character)设备文件： 亦即是一些串行端口的接口设备， 例如键盘、鼠标等等！这些设备的特色就是一次性读取的，不能够截断输出。 举例来说，你不可能让鼠标跳到另一个画面，而是滑动到另一个地方！第一个属性为 [ c ]。 数据接口文件(sockets)：数据接口文件（或者：套接口文件），这种类型的文件通常被用在网络上的数据承接了。我们可以启动一个程序来监听客户端的要求， 而客户端就可以透过这个socket来进行数据的沟通了。第一个属性为 [ s ]， 最常在/var/run这个目录中看到这种文件类型了。 例如：当我们启动MySQL服务器时，会产生一个mysql.sock的文件。 12# ls -lh /var/lib/mysql/mysql.sock srwxrwxrwx 1 mysql mysql 0 04-19 11:12 /var/lib/mysql/mysql.sock 注意这个文件的属性的第一个字符是 s。 符号链接文件：当我们查看文件属性时，会看到有类似 lrwxrwxrwx,注意第一个字符是l，这类文件是链接文件。是通过ln -s 源文件名 新文件名 。上面是一个例子，表示setup.log是install.log的软链接文件。怎么理解呢？这和Windows操作系统中的快捷方式有点相似。 符号链接文件的创建方法举例: 123456# ls -lh log2012.log-rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log# ln -s log2012.log linklog.log# ls -lh *.loglrwxrwxrwx 1 root root 11 11-22 06:58 linklog.log -&gt; log2012.log-rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log 数据输送文件（FIFO,pipe）:FIFO也是一种特殊的文件类型，他主要的目的在解决多个程序同时存取一个文件所造成的错误问题。 FIFO是first-in-first-out的缩写。第一个属性为[p] 。 Linux文件扩展名扩展名类型基本上，Linux的文件是没有所谓的扩展名的，一个Linux文件能不能被执行，与他的第一栏的十个属性有关， 与档名根本一点关系也没有。这个观念跟Windows的情况不相同喔！在Windows底下， 能被执行的文件扩展名通常是 .com .exe .bat等等，而在Linux底下，只要你的权限当中具有x的话，例如[ -rwx-r-xr-x ] 即代表这个文件可以被执行。 不过，可以被执行跟可以执行成功是不一样的～举例来说，在root家目录下的install.log 是一个纯文本档，如果经由修改权限成为 -rwxrwxrwx 后，这个文件能够真的执行成功吗？ 当然不行～因为他的内容根本就没有可以执行的数据。所以说，这个x代表这个文件具有可执行的能力， 但是能不能执行成功，当然就得要看该文件的内容. 虽然如此，不过我们仍然希望可以藉由扩展名来了解该文件是什么东西，所以，通常我们还是会以适当的扩展名来表示该文件是什么种类的。底下有数种常用的扩展名： *.sh ： 脚本或批处理文件 (scripts)，因为批处理文件为使用shell写成的，所以扩展名就编成 .sh Z, .tar, .tar.gz, .zip, *.tgz： 经过打包的压缩文件。这是因为压缩软件分别为 gunzip, tar 等等的，由于不同的压缩软件，而取其相关的扩展名！ .html, .php：网页相关文件，分别代表 HTML 语法与 PHP 语法的网页文件。 .html 的文件可使用网页浏览器来直接开启，至于 .php 的文件， 则可以透过 client 端的浏览器来 server 端浏览，以得到运算后的网页结果。 基本上，Linux系统上的文件名真的只是让你了解该文件可能的用途而已，真正的执行与否仍然需要权限的规范才行。例如虽然有一个文件为可执行文件，如常见的/bin/ls这个显示文件属性的指令，不过，如果这个文件的权限被修改成无法执行时，那么ls就变成不能执行。 上述的这种问题最常发生在文件传送的过程中。例如你在网络上下载一个可执行文件，但是偏偏在你的 Linux系统中就是无法执行！呵呵！那么就是可能文件的属性被改变了。不要怀疑，从网络上传送到你的 Linux系统中，文件的属性与权限确实是会被改变的。 Linux文件名长度限制：在Linux底下，使用预设的Ext2/Ext3文件系统时，针对文件名长度限制为： 单一文件或目录的最大容许文件名为 255 个字符 包含完整路径名称及目录 (/) 之完整档名为 4096 个字符 是相当长的档名！我们希望Linux的文件名可以一看就知道该文件在干嘛的， 所以档名通常是很长很长。 Linux文件名的字符的限制：由于Linux在文字接口下的一些指令操作关系，一般来说，你在设定Linux底下的文件名时， 最好可以避免一些特殊字符比较好！例如底下这些： ? &gt; &lt; ; &amp; ! [ ] | \ ‘ “ ` ( ) { } 因为这些符号在文字接口下，是有特殊意义的。另外，文件名的开头为小数点“.”时， 代表这个文件为隐藏文件！同时，由于指令下达当中，常常会使用到 -option 之类的选项， 所以你最好也避免将文件档名的开头以 - 或 + 来命名。 转载链接： http://www.cnblogs.com/peida/archive/2012/11/22/2781912.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（23）-Linux目录结构]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8823%EF%BC%89-%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[对于每一个Linux学习者来说，了解Linux文件系统的目录结构，是学好Linux的至关重要的一步.，深入了解linux文件目录结构的标准和每个目录的详细功能，对于我们用好linux系统只管重要，下面我们就开始了解一下linux目录结构的相关知识。 当在使用Linux的时候，如果您通过ls –l / 就会发现，在/下包涵很多的目录，比如etc、usr、var、bin … … 等目录，而在这些目录中，我们进去看看，发现也有很多的目录或文件。文件系统在Linux下看上去就象树形结构，所以我们可以把文件系统的结构形象的称为 树形结构。 文件系统的是用来组织和排列文件存取的，所以它是可见的，在Linux中，我们可以通过ls等工具来查看其结构，在Linux系统中，我们见到的都是树形结构；比如操作系统安装在一个文件系统中，他表现为由/ 起始的树形结构。linux文件系统的最顶端是/，我们称/为Linux的root，也就是 Linux操作系统的文件系统。Linux的文件系统的入口就是/，所有的目录、文件、设备都在/之下，/就是Linux文件系统的组织者，也是最上级的领导者。 由于linux是开放源代码，各大公司和团体根据linux的核心代码做各自的操作，编程。这样就造成在根下的目录的不同。这样就造成个人不能使用他人的linux系统的PC。因为你根本不知道一些基本的配置，文件在哪里。。。这就造成了混乱。这就是FHS（Filesystem Hierarchy Standard ）机构诞生的原因。该机构是linux爱好者自发的组成的一个团体，主要是是对linux做一些基本的要求，不至于是操作者换一台主机就成了linux的‘文盲’。 根据FHS(http://www.pathname.com/fhs/)的官方文件指出， 他们的主要目的是希望让使用者可以了解到已安装软件通常放置于那个目录下， 所以他们希望独立的软件开发商、操作系统制作者、以及想要维护系统的用户，都能够遵循FHS的标准。 也就是说，FHS的重点在于规范每个特定的目录下应该要放置什么样子的数据而已。 这样做好处非常多，因为Linux操作系统就能够在既有的面貌下(目录架构不变)发展出开发者想要的独特风格。 事实上，FHS是根据过去的经验一直再持续的改版的，FHS依据文件系统使用的频繁与否与是否允许使用者随意更动， 而将目录定义成为四种交互作用的形态，用表格来说有点像底下这样： 可分享的(shareable) 不可分享的(unshareable) 不变的(static) /usr (软件放置处) /etc (配置文件) /opt (第三方协力软件) /boot (开机与核心档) 可变动的(variable) /var/mail (使用者邮件信箱) /var/run (程序相关) /var/spool/news (新闻组) /var/lock (程序相关) 四中类型: 可分享的： 可以分享给其他系统挂载使用的目录，所以包括执行文件与用户的邮件等数据， 是能够分享给网络上其他主机挂载用的目录； 不可分享的： 自己机器上面运作的装置文件或者是与程序有关的socket文件等， 由于仅与自身机器有关，所以当然就不适合分享给其他主机了。 不变的： 有些数据是不会经常变动的，跟随着distribution而不变动。 例如函式库、文件说明文件、系统管理员所管理的主机服务配置文件等等； 可变动的： 经常改变的数据，例如登录文件、一般用户可自行收受的新闻组等。 事实上，FHS针对目录树架构仅定义出三层目录底下应该放置什么数据而已，分别是底下这三个目录的定义： / (root, 根目录)：与开机系统有关； /usr (unix software resource)：与软件安装/执行有关； /var (variable)：与系统运作过程有关。 根目录 (/) 的意义与内容：根目录是整个系统最重要的一个目录，因为不但所有的目录都是由根目录衍生出来的， 同时根目录也与开机/还原/系统修复等动作有关。 由于系统开机时需要特定的开机软件、核心文件、开机所需程序、 函式库等等文件数据，若系统出现错误时，根目录也必须要包含有能够修复文件系统的程序才行。 因为根目录是这么的重要，所以在FHS的要求方面，他希望根目录不要放在非常大的分区， 因为越大的分区内你会放入越多的数据，如此一来根目录所在分区就可能会有较多发生错误的机会。 因此FHS标准建议：根目录(/)所在分区应该越小越好， 且应用程序所安装的软件最好不要与根目录放在同一个分区内，保持根目录越小越好。 如此不但效能较佳，根目录所在的文件系统也较不容易发生问题。说白了，就是根目录和Windows的C盘一个样。 根据以上原因，FHS认为根目录(/)下应该包含如下子目录： 目录 应放置档案内容 /bin 系统有很多放置执行档的目录，但/bin比较特殊。因为/bin放置的是在单人维护模式下还能够被操作的指令。在/bin底下的指令可以被root与一般帐号所使用，主要有：cat,chmod(修改权限), chown, date, mv, mkdir, cp, bash等等常用的指令。 /boot 主要放置开机会使用到的档案，包括Linux核心档案以及开机选单与开机所需设定档等等。Linux kernel常用的档名为：vmlinuz ，如果使用的是grub这个开机管理程式，则还会存在/boot/grub/这个目录。 /dev 在Linux系统上，任何装置与周边设备都是以档案的型态存在于这个目录当中。 只要通过存取这个目录下的某个档案，就等于存取某个装置。比要重要的档案有/dev/null, /dev/zero, /dev/tty , /dev/lp, / dev/hd, /dev/sd*等等 /etc 系统主要的设定档几乎都放置在这个目录内，例如人员的帐号密码档、各种服务的启始档等等。 一般来说，这个目录下的各档案属性是可以让一般使用者查阅的，但是只有root有权力修改。 FHS建议不要放置可执行档(binary)在这个目录中。 比较重要的档案有：/etc/inittab, /etc/init.d/, /etc/modprobe.conf, /etc/X11/, /etc/fstab, /etc/sysconfig/等等。 另外，其下重要的目录有：/etc/init.d/ ：所有服务的预设启动script都是放在这里的，例如要启动或者关闭iptables的话： /etc/init.d/iptables start、/etc/init.d/ iptables stop/etc/xinetd.d/ ：这就是所谓的super daemon管理的各项服务的设定档目录。/etc/X11/ ：与X Window有关的各种设定档都在这里，尤其是xorg.conf或XF86Config这两个X Server的设定档。 /home 这是系统预设的使用者家目录(home directory)。 在你新增一个一般使用者帐号时，预设的使用者家目录都会规范到这里来。比较重要的是，家目录有两种代号： ~ ：代表当前使用者的家目录，而 ~guest：则代表用户名为guest的家目录。 /lib 系统的函式库非常的多，而/lib放置的则是在开机时会用到的函式库，以及在/bin或/sbin底下的指令会呼叫的函式库而已 。 什么是函式库呢？妳可以将他想成是外挂，某些指令必须要有这些外挂才能够顺利完成程式的执行之意。 尤其重要的是/lib/modules/这个目录，因为该目录会放置核心相关的模组(驱动程式)。 /media media是媒体的英文，顾名思义，这个/media底下放置的就是可移除的装置。 包括软碟、光碟、DVD等等装置都暂时挂载于此。 常见的档名有：/media/floppy, /media/cdrom等等。 /mnt 如果妳想要暂时挂载某些额外的装置，一般建议妳可以放置到这个目录中。在古早时候，这个目录的用途与/media相同啦。 只是有了/media之后，这个目录就用来暂时挂载用了。 /opt 这个是给第三方协力软体放置的目录 。 什么是第三方协力软体啊？举例来说，KDE这个桌面管理系统是一个独立的计画，不过他可以安装到Linux系统中，因此KDE的软体就建议放置到此目录下了。 另外，如果妳想要自行安装额外的软体(非原本的distribution提供的)，那么也能够将你的软体安装到这里来。 不过，以前的Linux系统中，我们还是习惯放置在/usr/local目录下。 /root 系统管理员(root)的家目录。 之所以放在这里，是因为如果进入单人维护模式而仅挂载根目录时，该目录就能够拥有root的家目录，所以我们会希望root的家目录与根目录放置在同一个分区中。 /sbin Linux有非常多指令是用来设定系统环境的，这些指令只有root才能够利用来设定系统，其他使用者最多只能用来查询而已。放在/sbin底下的为开机过程中所需要的，里面包括了开机、修复、还原系统所需要的指令。至于某些伺服器软体程式，一般则放置到/usr/sbin/当中。至于本机自行安装的软体所产生的系统执行档(system binary)，则放置到/usr/local/sbin/当中了。常见的指令包括：fdisk, fsck, ifconfig, init, mkfs等等。 /srv srv可以视为service的缩写，是一些网路服务启动之后，这些服务所需要取用的资料目录。 常见的服务例如WWW, FTP等等。 举例来说，WWW伺服器需要的网页资料就可以放置在/srv/www/里面。呵呵，看来平时我们编写的代码应该放到这里了。 /tmp 这是让一般使用者或者是正在执行的程序暂时放置档案的地方。这个目录是任何人都能够存取的，所以你需要定期的清理一下。当然，重要资料不可放置在此目录啊。 因为FHS甚至建议在开机时，应该要将/tmp下的资料都删除。 事实上FHS针对根目录所定义的标准就仅限于上表，不过仍旧有些目录也需要我们了解一下，具体如下： 目录 应放置文件内容 /lost+found 这个目录是使用标准的ext2/ext3档案系统格式才会产生的一个目录，目的在于当档案系统发生错误时，将一些遗失的片段放置到这个目录下。 这个目录通常会在分割槽的最顶层存在，例如你加装一个硬盘于/disk中，那在这个系统下就会自动产生一个这样的目录/disk/lost+found /proc 这个目录本身是一个虚拟文件系统(virtual filesystem)喔。 他放置的资料都是在内存当中，例如系统核心、行程资讯(process)（是进程吗?）、周边装置的状态及网络状态等等。因为这个目录下的资料都是在记忆体（内存）当中，所以本身不占任何硬盘空间。比较重要的档案（目录）例如： /proc/cpuinfo, /proc/dma, /proc/interrupts, /proc/ioports, /proc/net/*等等。呵呵，是虚拟内存吗[guest]？ /sys 这个目录其实跟/proc非常类似，也是一个虚拟的档案系统，主要也是记录与核心相关的资讯。 包括目前已载入的核心模组与核心侦测到的硬体装置资讯等等。 这个目录同样不占硬盘容量。 除了这些目录的内容之外，另外要注意的是，因为根目录与开机有关，开机过程中仅有根目录会被挂载， 其他分区则是在开机完成之后才会持续的进行挂载的行为。就是因为如此，因此根目录下与开机过程有关的目录， 就不能够与根目录放到不同的分区去。那哪些目录不可与根目录分开呢？有底下这些： /etc：配置文件 /bin：重要执行档 /dev：所需要的装置文件 /lib：执行档所需的函式库与核心所需的模块 /sbin：重要的系统执行文件 这五个目录千万不可与根目录分开在不同的分区。请背下来啊。 /usr 的意义与内容：依据FHS的基本定义，/usr里面放置的数据属于可分享的与不可变动的(shareable, static)， 如果你知道如何透过网络进行分区的挂载(例如在服务器篇会谈到的NFS服务器)，那么/usr确实可以分享给局域网络内的其他主机来使用喔。 /usr不是user的缩写，其实usr是Unix Software Resource的缩写， 也就是Unix操作系统软件资源所放置的目录，而不是用户的数据啦。这点要注意。 FHS建议所有软件开发者，应该将他们的数据合理的分别放置到这个目录下的次目录，而不要自行建立该软件自己独立的目录。 因为是所有系统默认的软件(distribution发布者提供的软件)都会放置到/usr底下，因此这个目录有点类似Windows 系统的C:\Windows\ + C:\Program files\这两个目录的综合体，系统刚安装完毕时，这个目录会占用最多的硬盘容量。 一般来说，/usr的次目录建议有底下这些： 目录 应放置文件内容 /usr/X11R6/ 为X Window System重要数据所放置的目录，之所以取名为X11R6是因为最后的X版本为第11版，且该版的第6次释出之意。 /usr/bin/ 绝大部分的用户可使用指令都放在这里。请注意到他与/bin的不同之处。(是否与开机过程有关) /usr/include/ c/c++等程序语言的档头(header)与包含档(include)放置处，当我们以tarball方式 (*.tar.gz 的方式安装软件)安装某些数据时，会使用到里头的许多包含档。 /usr/lib/ 包含各应用软件的函式库、目标文件(object file)，以及不被一般使用者惯用的执行档或脚本(script)。 某些软件会提供一些特殊的指令来进行服务器的设定，这些指令也不会经常被系统管理员操作， 那就会被摆放到这个目录下啦。要注意的是，如果你使用的是X86_64的Linux系统， 那可能会有/usr/lib64/目录产生 /usr/local/ 统管理员在本机自行安装自己下载的软件(非distribution默认提供者)，建议安装到此目录， 这样会比较便于管理。举例来说，你的distribution提供的软件较旧，你想安装较新的软件但又不想移除旧版， 此时你可以将新版软件安装于/usr/local/目录下，可与原先的旧版软件有分别啦。 你可以自行到/usr/local去看看，该目录下也是具有bin, etc, include, lib…的次目录 /usr/sbin/ 非系统正常运作所需要的系统指令。最常见的就是某些网络服务器软件的服务指令(daemon) /usr/share/ 放置共享文件的地方，在这个目录下放置的数据几乎是不分硬件架构均可读取的数据， 因为几乎都是文本文件嘛。在此目录下常见的还有这些次目录：/usr/share/man：联机帮助文件/usr/share/doc：软件杂项的文件说明/usr/share/zoneinfo：与时区有关的时区文件 /usr/src/ 一般原始码建议放置到这里，src有source的意思。至于核心原始码则建议放置到/usr/src/linux/目录下。 /var 的意义与内容：如果/usr是安装时会占用较大硬盘容量的目录，那么/var就是在系统运作后才会渐渐占用硬盘容量的目录。 因为/var目录主要针对常态性变动的文件，包括缓存(cache)、登录档(log file)以及某些软件运作所产生的文件， 包括程序文件(lock file, run file)，或者例如MySQL数据库的文件等等。常见的次目录有： 目录 应放置文件内容 /var/cache/ 应用程序本身运作过程中会产生的一些暂存档 /var/lib/ 程序本身执行的过程中，需要使用到的数据文件放置的目录。在此目录下各自的软件应该要有各自的目录。 举例来说，MySQL的数据库放置到/var/lib/mysql/而rpm的数据库则放到/var/lib/rpm去 /var/lock/ 某些装置或者是文件资源一次只能被一个应用程序所使用，如果同时有两个程序使用该装置时， 就可能产生一些错误的状况，因此就得要将该装置上锁(lock)，以确保该装置只会给单一软件所使用。 举例来说，刻录机正在刻录一块光盘，你想一下，会不会有两个人同时在使用一个刻录机烧片？ 如果两个人同时刻录，那片子写入的是谁的数据？所以当第一个人在刻录时该刻录机就会被上锁， 第二个人就得要该装置被解除锁定(就是前一个人用完了)才能够继续使用 /var/log/ 非常重要。这是登录文件放置的目录。里面比较重要的文件如/var/log/messages, /var/log/wtmp(记录登入者的信息)等。 /var/mail/ 放置个人电子邮件信箱的目录，不过这个目录也被放置到/var/spool/mail/目录中，通常这两个目录是互为链接文件。 /var/run/ 某些程序或者是服务启动后，会将他们的PID放置在这个目录下 /var/spool/ 这个目录通常放置一些队列数据，所谓的“队列”就是排队等待其他程序使用的数据。 这些数据被使用后通常都会被删除。举例来说，系统收到新信会放置到/var/spool/mail/中， 但使用者收下该信件后该封信原则上就会被删除。信件如果暂时寄不出去会被放到/var/spool/mqueue/中， 等到被送出后就被删除。如果是工作排程数据(crontab)，就会被放置到/var/spool/cron/目录中。 由于FHS仅是定义出最上层(/)及次层(/usr, /var)的目录内容应该要放置的文件或目录数据， 因此，在其他次目录层级内，就可以随开发者自行来配置了。 目录树(directory tree) :在Linux底下，所有的文件与目录都是由根目录开始的。那是所有目录与文件的源头, 然后再一个一个的分支下来，因此，我们也称这种目录配置方式为：目录树(directory tree), 这个目录树的主要特性有： 目录树的启始点为根目录 (/, root)； 每一个目录不止能使用本地端的 partition 的文件系统，也可以使用网络上的 filesystem 。举例来说， 可以利用 Network File System (NFS) 服务器挂载某特定目录等。 每一个文件在此目录树中的文件名(包含完整路径)都是独一无二的。 如果我们将整个目录树以图的方法来显示，并且将较为重要的文件数据列出来的话，那么目录树架构就如下图所示： ​ 绝对路径与相对路径除了需要特别注意的FHS目录配置外，在文件名部分我们也要特别注意。因为根据档名写法的不同，也可将所谓的路径(path)定义为绝对路径(absolute)与相对路径(relative)。 这两种文件名/路径的写法依据是这样的： 绝对路径： 由根目录(/)开始写起的文件名或目录名称， 例如 /home/dmtsai/.bashrc； 相对路径： 相对于目前路径的文件名写法。 例如 ./home/dmtsai 或 http://www.cnblogs.com/home/dmtsai/ 等等。反正开头不是 / 就属于相对路径的写法 而你必须要了解，相对路径是以你当前所在路径的相对位置来表示的。举例来说，你目前在 /home 这个目录下， 如果想要进入 /var/log 这个目录时，可以怎么写呢？ cd /var/log (absolute) cd ../var/log (relative) 因为你在 /home 底下，所以要回到上一层 (../) 之后，才能继续往 /var 来移动的，特别注意这两个特殊的目录： . ：代表当前的目录，也可以使用 ./ 来表示； .. ：代表上一层目录，也可以 ../ 来代表。 这个 . 与 .. 目录概念是很重要的，你常常会看到 cd .. 或 ./command 之类的指令下达方式， 就是代表上一层与目前所在目录的工作状态。 实例1：如何先进入/var/spool/mail/目录，再进入到/var/spool/cron/目录内？ 命令： cd /var/spool/mail cd ../cron 说明： 由于/var/spool/mail与/var/spool/cron是同样在/var/spool/目录中。如此就不需要在由根目录开始写起了。这个相对路径是非常有帮助的，尤其对于某些软件开发商来说。 一般来说，软件开发商会将数据放置到/usr/local/里面的各相对目录。 但如果用户想要安装到不同目录呢？就得要使用相对路径。 实例2：网络文件常常提到类似./run.sh之类的数据，这个指令的意义为何？ 说明： 由于指令的执行需要变量的支持，若你的执行文件放置在本目录，并且本目录并非正规的执行文件目录(/bin, /usr/bin等为正规)，此时要执行指令就得要严格指定该执行档。./代表本目录的意思，所以./run.sh代表执行本目录下， 名为run.sh的文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（22）-find参数详解]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8822%EF%BC%89-find%2F</url>
    <content type="text"><![CDATA[find一些常用参数的一些常用实例和一些具体用法和注意事项。 使用name选项：文件名选项是find命令最常用的选项，要么单独使用该选项，要么和其他选项一起使用。 可以使用某种文件名模式来匹配文件，记住要用引号将文件名模式引起来。 不管当前路径是什么，如果想要在自己的根目录$HOME中查找文件名符合*.log的文件，使用~作为 ‘pathname’参数，波浪号~代表了你的$HOME目录。 find ~ -name &quot;*.log&quot; -print 想要在当前目录及子目录中查找所有的‘ *.log‘文件，可以用： find . -name &quot;*.log&quot; -print 想要的当前目录及子目录中查找文件名以一个大写字母开头的文件，可以用： find . -name &quot;[A-Z]*&quot; -print 想要在/etc目录中查找文件名以host开头的文件，可以用： find /etc -name &quot;host*&quot; -print 想要查找$HOME目录中的文件，可以用： find ~ -name &quot;*&quot; -print 或find . -print 要想让系统高负荷运行，就从根目录开始查找所有的文件。 find / -name &quot;*&quot; -print 如果想在当前目录查找文件名以一个个小写字母开头，最后是4到9加上.log结束的文件： find . -name &quot;[a-z]*[4-9].log&quot; -print 用perm选项：按照文件权限模式用-perm选项,按文件权限模式来查找文件的话。最好使用八进制的权限表示法。 如在当前目录下查找文件权限位为755的文件，即文件属主可以读、写、执行，其他用户可以读、执行的文件，可以用： 12345# find . -perm 755 -print../scf./scf/lib./scf/service 还有一种表达方法：在八进制数字前面要加一个横杠-，表示都匹配，如-007就相当于777，-005相当于555, 忽略某个目录：如果在查找文件时希望忽略某个目录，因为你知道那个目录中没有你所要查找的文件，那么可以使用-prune选项来指出需要忽略的目录。在使用-prune选项时要当心，因为如果你同时使用了-depth选项，那么-prune选项就会被find命令忽略。如果希望在test目录下查找文件，但不希望在test/test3目录下查找，可以用： 12345# find test -path &quot;test/test3&quot; -prune -o -printtesttest/log2014.logtest/log2015.logtest/test4 说明： find \[-path ..][expression] 在路径列表的后面的是表达式 -path “test” -prune -o -print 是 -path “test” -a -prune -o -print 的简写表达式按顺序求值, -a 和 -o 都是短路求值，与 shell 的 &amp;&amp; 和 || 类似如果 -path “test” 为真，则求值 -prune , -prune 返回真，与逻辑表达式为真；否则不求值 -prune，与逻辑表达式为假。如果 -path “test” -a -prune 为假，则求值 -print ，-print返回真，或逻辑表达式为真；否则不求值 -print，或逻辑表达式为真。 这个表达式组合特例可以用伪码写为: if -path “test” then -prune else -print 避开多个文件夹:123456# find test \( -path test/test4 -o -path test/test3 \) -prune -o -printtesttest/log2014.logtest/log2015.logtest/scftest/scf/lib 说明： 圆括号表示表达式的结合。 \ 表示引用，即指示 shell 不对后面的字符作特殊解释，而留给 find 命令去解释其意义。 使用user和nouser选项：按文件属主查找文件： 实例1：在$HOME目录中查找文件属主为peida的文件 1find ~ -user peida -print 实例2：在/etc目录下查找文件属主为peida的文件: 1find /etc -user peida -print 实例3：为了查找属主帐户已经被删除的文件，可以使用-nouser选项。在/home目录下查找所有的这类文件 1find/home -nouser -print 说明： 这样就能够找到那些属主在/etc/passwd文件中没有有效帐户的文件。在使用-nouser选项时，不必给出用户名； find命令能够为你完成相应的工作。 使用group和nogroup选项：就像user和nouser选项一样，针对文件所属于的用户组， find命令也具有同样的选项，为了在/apps目录下查找属于gem用户组的文件，可以用： 1find /apps -group gem -print 要查找没有有效所属用户组的所有文件，可以使用nogroup选项。下面的find命令从文件系统的根目录处查找这样的文件: 1find / -nogroup -print 按照更改时间或访问时间等查找文件：如果希望按照更改时间来查找文件，可以使用mtime,atime或ctime选项。如果系统突然没有可用空间了，很有可能某一个文件的长度在此期间增长迅速，这时就可以用mtime选项来查找这样的文件。 用减号-来限定更改时间在距今n日以内的文件，而用加号+来限定更改时间在距今n日以前的文件。 希望在系统根目录下查找更改时间在5日以内的文件，可以用： 1find / -mtime -5 -print 为了在/var/adm目录下查找更改时间在3日以前的文件，可以用: 1find /var/adm -mtime +3 -print 查找比某个文件新或旧的文件：如果希望查找更改时间比某个文件新但比另一个文件旧的所有文件，可以使用-newer选项。 它的一般形式为： newest_file_name ! oldest_file_name 其中，！是逻辑非符号。 1）查找更改时间比文件log2012.log新但比文件log2017.log旧的文件 1234567891011121314151617# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.log-rw-r--r-- 1 root root 0 11-16 14:41 log2016.log-rw-r--r-- 1 root root 0 11-16 14:43 log2017.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# find -newer log2012.log ! -newer log2017.log../log2015.log./log2017.log./log2016.log./test3 2）查找更改时间在比log2012.log文件新的文件 123456 find -newer log2012.log../log2015.log./log2017.log./log2016.log./test3 使用type选项： 2）：在/etc目录下查找所有的目录 1find /etc -type d -print 2）：在当前目录下查找除目录以外的所有类型的文件 1find . ! -type d -print 3）：在/etc目录下查找所有的符号链接文件 1find /etc -type l -print 使用size选项：可以按照文件长度来查找文件，这里所指的文件长度既可以用块（block）来计量，也可以用字节来计量。以字节计量文件长度的表达形式为N c；以块计量文件长度只用数字表示即可。 在按照文件长度查找文件时，一般使用这种以字节表示的文件长度，在查看文件系统的大小，因为这时使用块来计量更容易转换。 1）：在当前目录下查找文件长度大于1 M字节的文件 1find . -size +1000000c -print 2）：在/home/apache目录下查找文件长度恰好为100字节的文件: 1find /home/apache -size 100c -print 3）：在当前目录下查找长度超过10块的文件（一块等于512字节） 1find . -size +10 -print 使用depth选项：在使用find命令时，可能希望先匹配所有的文件，再在子目录中查找。使用depth选项就可以使find命令这样做。这样做的一个原因就是，当在使用find命令向磁带上备份文件系统时，希望首先备份所有的文件，其次再备份子目录中的文件。 1)：find命令从文件系统的根目录开始，查找一个名为CON.FILE的文件。 1find / -name &quot;CON.FILE&quot; -depth -print 说明： 它将首先匹配所有的文件然后再进入子目录中查找 使用mount选项：在当前的文件系统中查找文件（不进入其他文件系统），可以使用find命令的mount选项。 1）：从当前目录开始查找位于本文件系统中文件名以XC结尾的文件 1find . -name &quot;*.XC&quot; -mount -print 转载链接： http://www.cnblogs.com/peida/archive/2012/11/16/2773289.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（21）-find之xargs]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8821%EF%BC%89-find%2F</url>
    <content type="text"><![CDATA[在使用 find命令的-exec选项处理匹配到的文件时， find命令将所有匹配到的文件一起传递给exec执行。但有些系统对能够传递给exec的命令长度有限制，这样在find命令运行几分钟之后，就会出现溢出错误。错误信息通常是“参数列太长”或“参数列溢出”。这就是xargs命令的用处所在，特别是与find命令一起使用。 find命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部，不像-exec选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去。 在有些系统中，使用-exec选项会为处理每一个匹配到的文件而发起一个相应的进程，并非将匹配到的文件全部作为参数一次执行；这样在有些情况下就会出现进程过多，系统性能下降的问题，因而效率不高； 而使用xargs命令则只有一个进程。另外，在使用xargs命令时，究竟是一次获取所有的参数，还是分批取得参数，以及每一次获取参数的数目都会根据该命令的选项及系统内核中相应的可调参数来确定。 使用实例： 1） 查找系统中的每一个普通文件，然后使用xargs命令来测试它们分别属于哪类文件 123456789101112#ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4# find . -type f -print | xargs file./log2014.log: empty./log2013.log: empty./log2012.log: ASCII text 2）在整个系统中查找内存信息转储文件(core dump) ，然后把结果保存到/tmp/core.log 文件中 12345678# find / -name &quot;core&quot; -print | xargs echo &quot;&quot; &gt;/tmp/core.log# cd /tmp# ll总计 16-rw-r--r-- 1 root root 1524 11-12 22:29 core.logdrwx------ 2 root root 4096 11-12 22:24 ssh-TzcZDx1766drwx------ 2 root root 4096 11-12 22:28 ssh-ykiRPk1815drwx------ 2 root root 4096 11-03 07:11 vmware-root 3）在当前目录下查找所有用户具有读、写和执行权限的文件，并收回相应的写权限 1234567891011121314151617# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4# find . -perm -7 -print | xargs chmod o-w# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 19:32 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4 说明： 执行命令后，文件夹scf、test3和test4的权限都发生改变 4）用grep命令在所有的普通文件中搜索hostname这个词 1234# find . -type f -print | xargs grep &quot;hostname&quot;./log2013.log:hostnamebaidu=baidu.com./log2013.log:hostnamesina=sina.com./log2013.log:hostnames=true 5）用grep命令在当前目录下的所有普通文件中搜索hostnames这个词 123# find . -name \* -type f -print | xargs grep &quot;hostnames&quot;./log2013.log:hostnamesina=sina.com./log2013.log:hostnames=true 说明： 注意，在上面的例子中， \用来取消find命令中的*在shell中的特殊含义。 6）使用xargs执行mv 12345678910111213141516171819202122# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:54 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4# cd test4/# ll总计 0[root@localhost test4]# cd ..# find . -name &quot;*.log&quot; | xargs -i mv &#123;&#125; test4# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# cd test4/# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log 7）find后执行xargs提示xargs: argument line too long解决方法： 12#find . -type f -atime +0 -print0 | xargs -0 -l1 -t rm -frm -f 说明： -l1是一次处理一个；-t是处理之前打印出命令 8）使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符，如例子中的[] 1234567891011121314151617181920# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# cd test4# find . -name &quot;file&quot; | xargs -I [] cp [] ..# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4 说明： 使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符，如例子中的[] 9）xargs的-p参数的使用 123456789101112131415161718192021222324252627# ll总计 0-rw-r--r-- 1 root root 0 11-13 06:06 log2015.log# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:06 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# cd test3# find . -name &quot;*.log&quot; | xargs -p -i mv &#123;&#125; ..mv ./log2015.log .. ?...y# ll总计 0# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4 说明： -p参数会提示让你确认是否执行后面的命令,y执行，n不执行。 转载链接： http://www.cnblogs.com/peida/archive/2012/11/15/2770888.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（20）-find之exec]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8820%EF%BC%89-find%2F</url>
    <content type="text"><![CDATA[find是我们很常用的一个Linux命令，但是我们一般查找出来的并不仅仅是看看而已，还会有进一步的操作，这个时候exec的作用就显现出来了。 exec解释：-exec参数后面跟的是command命令，它的终止是以；为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会与不同的意义，所以前面加反斜杠。 {}花括号代表前面find查找出来的文件名。 使用find时，只要把想要的操作写在一个文件里，就可以用exec来配合find查找，很方便的。在有些操作系统中只允许-exec选项执行诸如l s或ls -l这样的命令。大多数用户使用这一选项是为了查找旧文件并删除它们。建议在真正执行rm命令删除文件之前，最好先用ls命令看一下，确认它们是所要删除的文件。 exec选项后面跟随着所要执行的命令或脚本，然后是一对儿{ }，一个空格和一个\，最后是一个分号。为了使用exec选项，必须要同时使用print选项。如果验证一下find命令，会发现该命令只输出从当前路径起的相对路径及文件名。 常用实例1）ls -l命令放在find命令的-exec选项中 1234567# find . -type f -exec ls -l &#123;&#125; \; -rw-r--r-- 1 root root 127 10-28 16:51 ./log2014.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-2.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-3.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-1.log-rw-r--r-- 1 root root 33 10-28 16:54 ./log2013.log-rw-r--r-- 1 root root 302108 11-03 06:19 ./log2012.log 说明： 上面的例子中，find命令匹配到了当前目录下的所有普通文件，并在-exec选项中使用ls -l命令将它们列出。 2）在目录中查找更改时间在n日以前的文件并删除它们 12345678910111213141516171819# ll总计 328-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 33 10-28 16:54 log2013.log-rw-r--r-- 1 root root 127 10-28 16:51 log2014.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 25 10-28 17:02 log.log-rw-r--r-- 1 root root 37 10-28 17:07 log.txtdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4# find . -type f -mtime +14 -exec rm &#123;&#125; \;# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4 说明： 在shell中用任何方式删除文件之前，应当先查看相应的文件，一定要小心！当使用诸如mv或rm命令时，可以使用-exec选项的安全模式。它将在对每个匹配到的文件进行操作之前提示你。 3）在目录中查找更改时间在n日以前的文件并删除它们，在删除之前先给出提示 12345678910111213141516# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4# find . -name &quot;*.log&quot; -mtime +5 -ok rm &#123;&#125; \;&lt; rm ... ./log_link.log &gt; ? y&lt; rm ... ./log2012.log &gt; ? n# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4 说明： 在上面的例子中， find命令在当前目录中查找所有文件名以.log结尾、更改时间在5日以上的文件，并删除它们，只不过在删除之前先给出提示。 按y键删除文件，按n键不删除。 4）-exec中使用grep命令 123# find /etc -name &quot;passwd*&quot; -exec grep &quot;root&quot; &#123;&#125; \;root:x:0:0:root:/root:/bin/bashroot:x:0:0:root:/root:/bin/bash 说明： 任何形式的命令都可以在-exec选项中使用。 在上面的例子中我们使用grep命令。find命令首先匹配所有文件名为“ passwd*”的文件，例如passwd、passwd.old、passwd.bak，然后执行grep命令看看在这些文件中是否存在一个root用户。 5）查找文件移动到指定目录 123456789101112131415161718192021# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:49 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4# cd test3/# ll总计 304-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.log# find . -name &quot;*.log&quot; -exec mv &#123;&#125; .. \;# ll总计 0[root@localhost test3]# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:50 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4 6）用exec选项执行cp命令 123456789101112131415161718192021# ll总计 0# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:50 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4# find . -name &quot;*.log&quot; -exec cp &#123;&#125; test3 \;cp: “./test3/log2014.log” 及 “test3/log2014.log” 为同一文件cp: “./test3/log2013.log” 及 “test3/log2013.log” 为同一文件cp: “./test3/log2012.log” 及 “test3/log2012.log” 为同一文件# cd test3# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log 转载链接： http://www.cnblogs.com/peida/archive/2012/11/14/2769248.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（19）-find命令概览]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8819%EF%BC%89-find%2F</url>
    <content type="text"><![CDATA[Linux下find命令在目录结构中搜索文件，并执行指定的操作。Linux下find命令提供了相当多的查找条件，功能很强大。由于find具有强大的功能，所以它的选项也很多，其中大部分选项都值得我们花时间来了解一下。即使系统中含有网络文件系统( NFS)，find命令在该文件系统中同样有效，只你具有相应的权限。 在运行一个非常消耗资源的find命令时，很多人都倾向于把它放在后台执行，因为遍历一个大的文件系统可能会花费很长的时间(这里是指30G字节以上的文件系统)。 命令格式find pathname -options [-print -exec -ok ...] 命令功能用于在文件树种查找文件，并作出相应的处理 命令参数1234pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print： find命令将匹配的文件输出到标准输出。 -exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为&apos;command&apos; &#123; &#125; \;，注意&#123; &#125;和\；之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 命令选项12345678910111213141516171819202122232425262728-name 按照文件名查找文件。-perm 按照文件权限来查找文件。-prune 使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。-user 按照文件属主来查找文件。-group 按照文件所属的组来查找文件。-mtime -n +n 按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+ n表示文件更改时间距现在n天以前。find命令还有-atime和-ctime 选项，但它们都和-m time选项。-nogroup 查找无有效所属组的文件，即该文件所属的组在/etc/groups中不存在。-nouser 查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。-newer file1 ! file2 查找更改时间比文件file1新但比文件file2旧的文件。-type 查找某一类型的文件，诸如：b - 块设备文件。d - 目录。c - 字符设备文件。p - 管道文件。l - 符号链接文件。f - 普通文件。-size n：[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。-depth：在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找。-fstype：查找位于某一类型文件系统中的文件，这些文件系统类型通常可以在配置文件/etc/fstab中找到，该配置文件中包含了本系统中有关文件系统的信息。-mount：在查找文件时不跨越文件系统mount点。-follow：如果find命令遇到符号链接文件，就跟踪至链接所指向的文件。-cpio：对匹配的文件使用cpio命令，将这些文件备份到磁带设备中。另外,下面三个的区别:-amin n 查找系统中最后N分钟访问的文件-atime n 查找系统中最后n*24小时访问的文件-cmin n 查找系统中最后N分钟被改变文件状态的文件-ctime n 查找系统中最后n*24小时被改变文件状态的文件-mmin n 查找系统中最后N分钟被改变文件数据的文件-mtime n 查找系统中最后n*24小时被改变文件数据的文件 使用实例1）查找指定时间内修改过的文件 123456# find -atime -2../logs/monitor./.bashrc./.bash_profile./.bash_history 说明： 超找48小时内修改过的文件 2）根据关键字查找 123456789# find . -name &quot;*.log&quot; ./log_link.log./log2014.log./test4/log3-2.log./test4/log3-3.log./test4/log3-1.log./log2013.log./log2012.log./log.log 说明： 在当前目录查找 以.log结尾的文件。 “. “代表当前目录 3）按照目录或文件的权限来查找文件 12345# find /opt/soft/test/ -perm 777/opt/soft/test/log_link.log/opt/soft/test/test4/opt/soft/test/test5/test3/opt/soft/test/test3 说明： 查找/opt/soft/test/目录下 权限为 777的文件 4）按类型查找 12345find . -type f -name &quot;*.log&quot;./log2014.log./test4/log3-2.log./test4/log3-3.log./test4/log3-1.log 说明： 查找当目录，以.log结尾的普通文件 5）查找当前所有目录并排序 12345678# find . -type d | sort../scf./scf/bin./scf/doc./scf/lib./scf/service./scf/service/deploy 6）按大小查找文件 12345678# find . -size +1000c -print../test4./scf./scf/lib./scf/service./scf/service/deploy./scf/service/deploy/product 说明： 查找当前目录大于1K的文件 转载链接： http://www.cnblogs.com/peida/archive/2012/11/13/2767374.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（18）-locate]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8818%EF%BC%89-locate%2F</url>
    <content type="text"><![CDATA[locate 让使用者可以很快速的搜寻档案系统内是否有指定的档案。其方法是先建立一个包括系统内所有档案名称及路径的数据库，之后当寻找时就只需查询这个数据库，而不必实际深入档案系统之中了。在一般的 distribution 之中，数据库的建立都被放在 crontab 中自动执行。 语法locate (选项)(参数) 选项12345678910-e 将排除在寻找的范围之外。-1 如果 是 1．则启动安全模式。在安全模式下，使用者不会看到权限无法看到 的档案。这会始速度减慢，因为 locate 必须至实际的档案系统中取得档案的 权限资料。-f 将特定的档案系统排除在外，例如我们没有到理要把 proc 档案系统中的档案 放在资料库中-q 安静模式，不会显示任何错误讯息-n 至多显示 n个输出-r 使用正规运算式 做寻找的条件-o 指定资料库存的名称-d 指定资料库的路径-h 显示辅助讯息-V 显示程式的版本讯息 参数查找字符串：要查找的文件名中含有的字符串。 功能locate命令可以在搜寻数据库是快速找到档案，数据库有updatedb程序来更新，updatedb是由cron daemon周期性建立的，locate命令在搜寻数据库时比由整个由硬盘来搜寻资料来得快，但locate所找到的档案若是最近才建立或刚更名的，可能会找不到，在内定值中，updatedb每天会跑一次，可以由修改crontab来更新设定值。（/etc/crontab） locate指令和find找寻档案的功能类似，但locate是透过update程序将硬盘中的所有档案盒目录资料先建立一个索引数据库，在执行locate时直接找该索引，查询速度会较快，索引数据库一般是由操作系统管理，但也可以直接下达update强迫系统立即修改索引数据库。 常用范例1）查找和pwd相关的所有文件 1234567# locate pwd/bin/pwd/etc/.pwd.lock/sbin/unix_chkpwd/usr/bin/pwdx/usr/include/pwd.h/usr/lib/python2.7/dist-packages/twisted/python/fakepwd.py 2） 搜索etc目录下所有以sh开头的文件 1234 # locate /etc/sh/etc/shadow/etc/shadow-/etc/shells 3）搜索etc目录下，所有以m开头的文件 12345# locate /etc/m/etc/magic/etc/magic.mime/etc/mailcap/etc/mailcap.order 参考链接： http://www.cnblogs.com/peida/archive/2012/11/12/2765750.html http://man.linuxde.net/locate_slocate]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（17）-whereis]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8817%EF%BC%89-whereis%2F</url>
    <content type="text"><![CDATA[whereis命令用来定位指令的二进制程序、源代码文件和man手册页等相关文件的路径。 whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 和find相比，whereis查找的速度非常快，这是因为linux系统会将 系统内的所有文件都记录在一个数据库文件中，当使用whereis和下面即将介绍的locate时，会从数据库中查找数据，而不是像find命令那样，通 过遍历硬盘来查找，效率自然会很高。 但是该数据库文件并不是实时更新，默认情况下时一星期更新一次，因此，我们在用whereis和locate 查找文件时，有时会找到已经被删除的数据，或者刚刚建立文件，却无法查找到，原因就是因为数据库文件没有被更新。 语法whereis(选项)(参数) 选项1234567-b 定位可执行文件。-m 定位帮助文件。-s 定位源代码文件。-u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。-B 指定搜索可执行文件的路径。-M 指定搜索帮助文件的路径。-S 指定搜索源代码文件的路径。 常用范例1）将和**文件相关的文件都查找出来 12# whereis svnsvn: /usr/bin/svn /usr/local/svn /usr/share/man/man1/svn.1.gz 说明： tomcat没安装，找不出来，svn安装找出了很多相关文件 2）只将二进制文件 查找出来 12# whereis -b svnsvn: /usr/bin/svn /usr/local/svn 说明： whereis -m svn 查出说明文档路径，whereis -s svn 找source源文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（16）-which]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8816%EF%BC%89-which%2F</url>
    <content type="text"><![CDATA[我们经常在linux要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索：​ which 查看可执行文件的位置。​ whereis 查看文件的位置。​ locate 配合数据库查看文件位置。​ find 实际搜寻硬盘查询文件名称。 which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 语法which 可执行文件名称 选项1234-n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。-p 与-n参数相同，但此处的包括了文件的路径。-w 指定输出时栏位的宽度。-V 显示版本信息 功能which指令会在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。 常用范例1）查找文件、显示命令路径 12# which pwd/bin/pwd 参考链接： http://www.cnblogs.com/peida/archive/2012/11/08/2759805.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（15）-tail]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8815%EF%BC%89-tail%2F</url>
    <content type="text"><![CDATA[tail 命令从指定点开始将文件写到标准输出.使用tail命令的-f选项可以方便的查阅正在改变的日志文件,tail -f filename会把filename里最尾部的内容显示在屏幕上,并且不断刷新,使你看到最新的文件内容. 语法tail(选项)(参数) 选项12345678-f 循环读取-q 不显示处理信息-v 显示详细的处理信息-c&lt;数目&gt; 显示的字节数-n&lt;行数&gt; 显示行数--pid=PID 与-f合用,表示在进程ID,PID死掉之后结束. -q, --quiet, --silent 从不输出给出文件名的首部 -s, --sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 功能用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。 常用范例1）显示文件末尾内容 123456# tail -n 5 log2014.log 2014-092014-102014-112014-12=========================== 说明： 显示文件最后5行内容 2）循环查看文件内容 1234567891011121314# ping 192.168.120.204 &gt; test.log &amp;# tail -f test.log PING 192.168.120.204 (192.168.120.204) 56(84) bytes of data.64 bytes from 192.168.120.204: icmp_seq=1 ttl=64 time=0.038 ms64 bytes from 192.168.120.204: icmp_seq=2 ttl=64 time=0.036 ms64 bytes from 192.168.120.204: icmp_seq=3 ttl=64 time=0.033 ms64 bytes from 192.168.120.204: icmp_seq=4 ttl=64 time=0.027 ms64 bytes from 192.168.120.204: icmp_seq=5 ttl=64 time=0.032 ms64 bytes from 192.168.120.204: icmp_seq=6 ttl=64 time=0.026 ms64 bytes from 192.168.120.204: icmp_seq=7 ttl=64 time=0.030 ms64 bytes from 192.168.120.204: icmp_seq=8 ttl=64 time=0.029 ms64 bytes from 192.168.120.204: icmp_seq=9 ttl=64 time=0.044 ms64 bytes from 192.168.120.204: icmp_seq=10 ttl=64 time=0.033 ms64 bytes from 192.168.120.204: icmp_seq=11 ttl=64 time=0.027 ms 3）从第5行开始显示文件 12345678910111213141516171819202122# cat log2014.log 2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12# tail -n +5 log2014.log2014-052014-062014-072014-082014-092014-102014-112014-12 参考链接： http://www.cnblogs.com/peida/archive/2012/11/07/2758084.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（14）-head]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8814%EF%BC%89-head%2F</url>
    <content type="text"><![CDATA[head 与 tail 就像它的名字一样的浅显易懂，它是用来显示开头或结尾某个数量的文字区块，head 用来显示档案的开头至标准输出中，而 tail 想当然尔就是看档案的结尾。 语法head(选项)(参数) 选项1234-n&lt;数字&gt;：指定显示头部内容的行数；-c&lt;字符数&gt;：指定显示头部内容的字符数；-v：总是显示文件名的头信息；-q：不显示文件名的头信息。 功能head 用来显示档案的开头至标准输出中，默认head命令打印其相应文件的开头10行。 常用范例1）显示文件的前n行 12345678910111213141516171819# cat log2014.log 2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12# head -n 5 log2014.log 2014-012014-022014-032014-042014-05 2）显示文件前n个字节 1234# head -c 20 log2014.log2014-012014-022014 3）文件的除了最后n个字节以外的内容 12345678910111213# head -c -32 log2014.log2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12 4）输出文件除了最后n行的全部内容 12345678# head -n -6 log2014.log2014-012014-022014-032014-042014-052014-062014-07 参考链接： http://www.cnblogs.com/peida/archive/2012/11/06/2756278.html http://man.linuxde.net/head]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（13）-less]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8813%EF%BC%89-less%2F</url>
    <content type="text"><![CDATA[less 工具也是对文件或其它输出进行分页显示的工具，应该说是linux正统查看文件内容的工具，功能极其强大。less 的用法比起 more 更加的有弹性。在 more 的时候，我们并没有办法向前面翻， 只能往后面看，但若使用了 less 时，就可以使用 [pageup][pagedown] 等按键的功能来往前往后翻看文件，更容易用来查看一个文件的内容！除此之外，在 less 里头可以拥有更多的搜索功能，不止可以向下搜，也可以向上搜。 语法less(选项)(参数) 选项1234567891011121314151617181920212223242526-b &lt;缓冲区大小&gt; 设置缓冲区的大小-e 当文件显示结束后，自动离开-f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件-g 只标志最后搜索的关键词-i 忽略搜索时的大小写-m 显示类似more命令的百分比-N 显示每行的行号-o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来-Q 不使用警告音-s 显示连续空行为一行-S 行过长时间将超出部分舍弃-x &lt;数字&gt; 将“tab”键显示为规定的数字空格/字符串：向下搜索“字符串”的功能?字符串：向上搜索“字符串”的功能n：重复前一个搜索（与 / 或 ? 有关）N：反向重复前一个搜索（与 / 或 ? 有关）b 向后翻一页d 向后翻半页h 显示帮助界面Q 退出less 命令u 向前滚动半页y 向前滚动一行空格键 滚动一行回车键 滚动一页[pagedown]： 向下翻动一页[pageup]： 向上翻动一页 功能less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。 常用实例1）ps查看进程信息并通过less分页显示 1ps -ef | less 2 ) 查看命令历史使用记录并通过less分页显示 1history | less 3）浏览多个文件 1less log2013.log log2014.log 说明： 输入 ：n后，切换到 log2014.log 输入 ：p 后，切换到log2013.log 附加备注1.全屏导航 ctrl + F - 向前移动一屏 ctrl + B - 向后移动一屏 ctrl + D - 向前移动半屏 ctrl + U - 向后移动半屏 2.单行导航 j - 向前移动一行 k - 向后移动一行 3.其它导航 G - 移动到最后一行 g - 移动到第一行 q / ZZ - 退出 less 命令 4.其它有用的命令 v - 使用配置的编辑器编辑当前文件 h - 显示 less 的帮助文档 &amp;pattern - 仅显示匹配模式的行，而不是整个文件 5.标记导航 当使用 less 查看大文件时，可以在任何一个位置作标记，可以通过命令导航到标有特定标记的文本位置： ma - 使用 a 标记文本的当前位置 ‘a - 导航到标记 a 处 参考链接： http://www.cnblogs.com/peida/archive/2012/11/05/2754477.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（12）-more]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8812%EF%BC%89-more%2F</url>
    <content type="text"><![CDATA[more命令，功能类似 cat ，cat命令是整个文件的内容从上到下显示在屏幕上。 more会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示，而且还有搜寻字串的功能 。more命令从前向后读取文件，因此在启动时就加载整个文件。 语法more(语法)(参数) 选项123456789+n 从笫n行开始显示-n 定义屏幕大小为n行+/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 -c 从顶部清屏，然后显示-d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能-l 忽略Ctrl+l（换页）字符-p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似-s 把连续的多个空行显示为一行-u 把文件内容中的下画线去掉 功能more命令和cat的功能一样都是查看文件里的内容，但有所不同的是more可以按页来查看文件的内容，还支持直接跳转行等功能。 常用操作命令123456789Enter 向下n行，需要定义。默认为1行Ctrl+F 向下滚动一屏空格键 向下滚动一屏Ctrl+B 返回上一屏= 输出当前行的行号：f 输出文件名和当前行的行号V 调用vi编辑器!命令 调用Shell，并执行命令 q 退出more 常用范例1）显示文件中从第3行起的内容 123456789101112# cat log2012.log 2012-012012-022012-032012-04-day12012-04-day22012-04-day3# more +3 log2012.log 2012-032012-04-day12012-04-day22012-04-day3 2）从文件中查找第一个出现”day3”字符串的行，并从该处前两行开始显示输出 1234567# more +/day3 log2012.log ...skipping2012-04-day12012-04-day22012-04-day32012-052012-05-day1 3）设定每屏显示行数 123456# more -5 log2012.log 2012-012012-022012-032012-04-day12012-04-day2 4）列一个目录下的文件，由于内容太多，我们应该学会用more来分页显示。这得和管道 | 结合起来 1234567891011# ls -l | more -5总计 36-rw-r--r-- 1 root root 308 11-01 16:49 log2012.log-rw-r--r-- 1 root root 33 10-28 16:54 log2013.log-rw-r--r-- 1 root root 127 10-28 16:51 log2014.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 25 10-28 17:02 log.log-rw-r--r-- 1 root root 37 10-28 17:07 log.txtdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4 说明： 每页显示5个文件信息，按 Ctrl+F 或者 空格键 将会显示下5条文件信息。 参考链接： http://www.cnblogs.com/peida/archive/2012/11/02/2750588.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（11）-nl]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8811%EF%BC%89-nl%2F</url>
    <content type="text"><![CDATA[nl命令读取 file 参数（缺省情况下标准输入），计算输入中的行号，将计算过的行号写入标准输出。在输出中，nl命令根据您在命令行中指定的标志来计算左边的行。输入文本必须写在逻辑页中。每个逻辑页有头、主体和页脚节（可以有空节）。除非使用-p选项，nl 命令在每个逻辑页开始的地方重新设置行号。可以单独为头、主体和页脚节设置行计算标志（例如，头和页脚行可以被计算然而文本行不能）。其默认的结果与cat -n有点不太一样， nl 可以将行号做比较多的显示设计，包括位数与是否自动补齐0等等的功能。 语法nl (选项) (参数) 选项123456789-b ：指定行号指定的方式，主要有两种： -b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)； -b t ：如果有空行，空的那一行不要列出行号(默认值)；-n ：列出行号表示的方法，主要有三种： -n ln ：行号在萤幕的最左方显示； -n rn ：行号在自己栏位的最右方显示，且不加 0 ； -n rz ：行号在自己栏位的最右方显示，且加 0 ；-w ：行号栏位的占用的位数。-p ：在逻辑定界符处不重新开始计算。 常用范例1）用 nl 列出 log2015.log 的内容： 123456# nl log2015.log1 2015-012 2015-023 ====== 说明：文件中的空白行，nl 不会加上行号 2）用 nl 列出 log2015.log 的内容，空本行也加上行号： 123456# nl -b a log2015.log1 2015-012 2015-02345 ====== 3）让行号前面自动补上0，统一输出格式： 1234567891011121314151617181920212223242526272829# nl -b a -n rz log2015.log000001 2015-01000002 2015-02000003 2015-03000004 2015-04000005 2015-05000006 2015-06000007 2015-07000008 2015-08000009 2015-09000010 2015-10000011 2015-11000012 2015-12000013 =======# nl -b a -n rz -w 3 log2015.log001 2015-01002 2015-02003 2015-03004 2015-04005 2015-05006 2015-06007 2015-07008 2015-08009 2015-09010 2015-10011 2015-11012 2015-12013 ======= 说明：nl -b a -n rz命令行号默认为六位，要调整位数可以加上参数-w 3调整为3位。 参考链接： http://man.linuxde.net/nl]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（10）-cat]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%8810%EF%BC%89-cat%2F</url>
    <content type="text"><![CDATA[cat命令的用途是连接文件或标准输入并打印。这个命令常用来显示文件内容，或者将几个文件连接起来显示，或者从标准输入读取内容并显示，它常与重定向符号配合使用。 语法cat(选项)(参数) 选项12345678910-A, --show-all 等价于 -vET-b, --number-nonblank 对非空输出行编号-e 等价于 -vE-E, --show-ends 在每行结束处显示 $-n, --number 对输出的所有行编号,由1开始对所有输出的行数编号-s, --squeeze-blank 有连续两行以上的空白行，就代换为一行的空白行 -t 与 -vT 等价-T, --show-tabs 将跳格字符显示为 ^I-u (被忽略)-v, --show-nonprinting 使用 ^ 和 M- 引用，除了 LFD 和 TAB 之外 功能1.一次显示整个文件:cat filename 2.从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件. 3.将几个文件合并为一个文件:cat file1 file2 &gt; file 常用范例1）把 log2012.log 的文件内容加上行号后输入 log.log 这个文件里 12345# cat log.log [root@localhost test]# cat -n log2012.log &gt; log.log# cat -n log.log 1 2012-01 2 2012-02 2）使用here doc来生成文件 12345678910111213# cat &gt;log.txt &lt;&lt;EOF&gt; Hello&gt; World&gt; Linux&gt; PWD=$(pwd)&gt; EOF# ls -l log.txt -rw-r--r-- 1 root root 37 10-28 17:07 log.txt# cat log.txt HelloWorldLinuxPWD=/opt/soft/test 说明： 注意粗体部分，here doc可以进行字符串替换。 备注： tac (反向列示) 12345# tac log.txt PWD=/opt/soft/testLinuxWorldHello 说明： tac 是将 cat 反写过来，所以他的功能就跟 cat 相反， cat 是由第一行到最后一行连续显示在萤幕上，而 tac 则是由最后一行到第一行反向在萤幕上显示出来！ 参考链接： http://www.cnblogs.com/peida/archive/2012/10/30/2746968.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（9）-touch]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%889%EF%BC%89-touch%2F</url>
    <content type="text"><![CDATA[linux的touch命令不常用，一般在使用make的时候可能会用到，用来修改文件时间戳，或者新建一个不存在的文件。 语法touch(选项)(参数) 选项123456789-a：或--time=atime或--time=access或--time=use 只更改存取时间；-c：或--no-create 不建立任何文件；-d：&lt;时间日期&gt; 使用指定的日期时间，而非现在的时间；-f：此参数将忽略不予处理，仅负责解决BSD版本touch指令的兼容性问题；-m：或--time=mtime或--time=modify 只更该变动时间；-r：&lt;参考文件或目录&gt; 把指定文件或目录的日期时间，统统设成和参考文件或目录的日期时间相同；-t：&lt;日期时间&gt; 使用指定的日期时间，而非现在的时间；--help：在线帮助；--version：显示版本信息。 功能touch命令参数可更改文档或目录的日期时间，包括存取时间和更改时间。 常用范例1）创建不存在的文件 1234# touch log2012.log log2013.log# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log 如果log2014.log不存在，则不创建文件 1234# touch -c log2014.log# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log 2）更新log.log的时间和log2012.log时间戳相同 123456789# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log# touch -r log.log log2012.log # ll-rw-r--r-- 1 root root 0 10-28 14:48 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log 3）设定文件的时间戳 123456789# ll-rw-r--r-- 1 root root 0 10-28 14:48 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log# touch -t 201211142234.50 log.log# ll-rw-r--r-- 1 root root 0 10-28 14:48 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 2012-11-14 log.log 说明： -t time 使用指定的时间值 time 作为指定文件相应时间戳记的新值．此处的 time规定为如下形式的十进制数: [[CC]YY]MMDDhhmm[.SS] 这里，CC为年数中的前两位，即”世纪数”；YY为年数的后两位，即某世纪中的年数．如果不给出CC的值，则touch 将把年数CCYY限定在1969–2068之内．MM为月数，DD为天将把年数CCYY限定在1969–2068之内．MM为月数，DD为天数，hh 为小时数(几点)，mm为分钟数，SS为秒数．此处秒的设定范围是0–61，这样可以处理闰秒．这些数字组成的时间是环境变量TZ指定的时区中的一个时 间．由于系统的限制，早于1970年1月1日的时间是错误的。 4）同时创建多个文件 123touch file&#123;1..20&#125;touch /home/&#123;aa,bb&#125;//&#123;&#125;里面为集合 参考链接： http://www.cnblogs.com/peida/archive/2012/10/30/2745714.html http://man.linuxde.net/touch]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（8）-cp]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%888%EF%BC%89-cp%2F</url>
    <content type="text"><![CDATA[cp命令用来复制文件或者目录，是Linux系统中最常用的命令之一。一般情况下，shell会设置一个别名，在命令行下复制文件时，如果目标文件已经存在，就会询问是否覆盖，不管你是否使用-i参数。但是如果是在shell脚本中执行cp时，没有-i参数时不会询问是否覆盖。这说明命令行和shell脚本的执行方式有些不同。 语法cp(选项)(参数) 选项123456789101112-a：此参数的效果和同时指定&quot;-dpR&quot;参数相同；-d：当复制符号连接时，把目标文件或目录也建立为符号连接，并指向与源文件或目录连接的原始文件或目录；-f：强行复制文件或目录，不论目标文件或目录是否已存在；-i：覆盖既有文件之前先询问用户；-l：对源文件建立硬连接，而非复制文件；-p：保留源文件或目录的属性；-R/r：递归处理，将指定目录下的所有文件与子目录一并处理；-s：对源文件建立符号连接，而非复制文件；-u：使用这项参数后只会在源文件的更改时间较目标文件更新时或是名称相互对应的目标文件并不存在时，才复制文件；-S：在备份文件时，用指定的后缀“SUFFIX”代替文件的默认后缀；-b：覆盖已存在的文件目标前将目标文件备份；-v：详细显示命令执行的操作。 参数 源文件：制定源文件列表。默认情况下，cp命令不能复制目录，如果要复制目录，则必须使用-R选项； 目标文件：指定目标文件。当“源文件”为多个文件时，要求“目标文件”为指定的目录。 常用范例1）复制单个文件到目标目录，文件在目标文件中不存在 123456789101112# cp log.log test5# ll-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxr-xr-x 2 root root 4096 10-28 14:53 test5# cd test5# ll-rw-r--r-- 1 root root 0 10-28 14:46 log5-1.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-2.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-3.log-rw-r--r-- 1 root root 0 10-28 14:53 log.log 说明： 在没有带-a参数时，两个文件的时间是不一样的。在带了-a参数时，两个文件的时间是一致的。 2）目标文件存在时，会询问是否覆盖 12345678910# cp log.log test5cp：是否覆盖“test5/log.log”? n# cp -a log.log test5cp：是否覆盖“test5/log.log”? y# cd test5/# ll-rw-r--r-- 1 root root 0 10-28 14:46 log5-1.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-2.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-3.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log 说明： 目标文件存在时，会询问是否覆盖。这是因为cp是cp -i的别名。目标文件存在时，即使加了-f标志，也还会询问是否覆盖。 3）复制整个目录 目标目录存在时： 12345678910111213#cp -a test3 test5 # ll-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxr-xr-x 3 root root 4096 10-28 15:11 test5# cd test5/# ll-rw-r--r-- 1 root root 0 10-28 14:46 log5-1.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-2.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-3.log-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxrwxrwx 2 root root 4096 10-28 14:47 test3 目标目录不存在时： 1234567# cp -a test3 test4# ll-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4drwxr-xr-x 3 root root 4096 10-28 15:11 test5 说明： 注意目标目录存在与否结果是不一样的。目标目录存在时，整个源目录被复制到目标目录里面。 4）复制的 log.log 建立一个链接到 log_link.log 12345678# cp -s log.log log_link.log# lllrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4drwxr-xr-x 3 root root 4096 10-28 15:11 test5 说明： 那个 log_link.log 是由 -s 的参数造成的，建立的是一个『快捷方式』，所以您会看到在文件的最右边，会显示这个文件是『连结』到哪里去的！ 5）将文件file复制到目录/usr/men/tmp下，并改名为file1 1cp file /usr/men/tmp/file1 6）将目录/usr/men下的所有文件及其子目录复制到目录/usr/zh中 1cp -i /usr/men m*.c /usr/zh 我们在Linux下使用cp命令复制文件时候，有时候会需要覆盖一些同名文件，覆盖文件的时候都会有提示：需要不停的按Y来确定执行覆盖。文件数量不多还好，但是要是几百个估计按Y都要吐血了，于是折腾来半天总结了一个方法： 1234567891011cp aaa/* /bbb复制目录aaa下所有到/bbb目录下，这时如果/bbb目录下有和aaa同名的文件，需要按Y来确认并且会略过aaa目录下的子目录。cp -r aaa/* /bbb这次依然需要按Y来确认操作，但是没有忽略子目录。cp -r -a aaa/* /bbb依然需要按Y来确认操作，并且把aaa目录以及子目录和文件属性也传递到了/bbb。\cp -r -a aaa/* /bbb成功，没有提示按Y、传递了目录属性、没有略过目录。 参考链接： http://www.cnblogs.com/peida/archive/2012/10/29/2744185.html http://man.linuxde.net/cp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（7）-mv]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%887%EF%BC%89-mv%2F</url>
    <content type="text"><![CDATA[mv命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。source表示源文件或目录，target表示目标文件或目录。如果将一个文件移到一个已经存在的目标文件中，则目标文件的内容将被覆盖。 mv命令可以用来将源文件移至一个目标文件中，或将一组文件移至一个目标目录中。源文件被移至目标文件有两种不同的结果： 如果目标文件是到某一目录文件的路径，源文件会被移到此目录下，且文件名不变。 如果目标文件不是目录文件，则源文件名（只能有一个）会变为此目标文件名，并覆盖己存在的同名文件。如果源文件和目标文件在同一个目录下，mv的作用就是改文件名。当目标文件是目录文件时，源文件或目录参数可以有多个，则所有的源文件都会被移至目标文件中。所有移到该目录下的文件都将保留以前的文件名。 注意事项：mv与cp的结果不同，mv好像文件“搬家”，文件个数并未增加。而cp对文件进行复制，文件个数增加了。 语法mv(选项)(参数) 选项12345678--backup=&lt;备份模式&gt;：若需覆盖文件，则覆盖前先行备份；-b：当文件存在时，覆盖前，为其创建一个备份；-f：若目标文件或目录与现有的文件或目录重复，则直接覆盖现有的文件或目录；-i：交互式操作，覆盖前先行询问用户，如果源文件与目标文件或目标目录中的文件同名，则询问用户是否覆盖目标文件。用户输入”y”，表示将覆盖目标文件；输入”n”，表示取消对源文件的移动。这样可以避免误将文件覆盖。--strip-trailing-slashes：删除源文件中的斜杠“/”；-S&lt;后缀&gt;：为备份文件指定后缀，而不使用默认的后缀；-t:--target-directory=&lt;目录&gt;,指定源文件要移动到目标目录；-u：当源文件比目标文件新或者目标文件不存在时，才执行移动操作。 参数 源文件：源文件列表。 目标文件：如果“目标文件”是文件名则在移动文件的同时，将其改名为“目标文件”；如果“目标文件”是目录名则将源文件移动到“目标文件”下。 常用范例1）文件改名 12345678910111213# ll总计 20drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5-rw-r--r-- 1 root root 16 10-28 06:04 test.log# mv test.log test1.txt# ll总计 20drwxr-xr-x 6 root root 4096 10-27 01:58 scf-rw-r--r-- 1 root root 16 10-28 06:04 test1.txtdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5 2）移动文件 12345678910111213141516# ll总计 20drwxr-xr-x 6 root root 4096 10-27 01:58 scf-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5# mv test1.txt test3# ll总计 16drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 06:09 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3# ll总计 4-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt 3）将文件log1.txt,log2.txt,log3.txt移动到目录test3中 12345678910111213141516171819202122232425262728293031# ll总计 28-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txtdrwxrwxrwx 2 root root 4096 10-28 06:09 test3# mv log1.txt log2.txt log3.txt test3# ll总计 16drwxrwxrwx 2 root root 4096 10-28 06:18 test3# cd test3/# ll总计 16-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt# ll总计 20-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txtdrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt# mv -t /opt/soft/test/test4/ log1.txt log2.txt log3.txt ]# cd ..# cd test4/# ll总计 12-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt 4）将文件file1改名为file2，如果file2已经存在，则询问是否覆盖 12345678910111213# ll总计 12-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt# cat log1.txt odfdfs# cat log2.txt ererwerwer# mv -i log1.txt log2.txt mv：是否覆盖“log2.txt”? y# cat log2.txt odfdfs 5）将文件file1改名为file2，即使file2存在，也是直接覆盖掉 12345678910111213141516171819202122# ll总计 8-rw-r--r-- 1 root root 8 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt# cat log2.txt odfdfs# cat log3cat: log3: 没有那个文件或目录# ll总计 8-rw-r--r-- 1 root root 8 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt# cat log2.txt odfdfs# cat log3.txt dfosdfsdfdss# mv -f log3.txt log2.txt # cat log2.txt dfosdfsdfdss# ll总计 4-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt 说明： log3.txt的内容直接覆盖了log2.txt内容，-f 这是个危险的选项，使用的时候一定要保持头脑清晰，一般情况下最好不用加上它。 6）目录的移动 12345678910111213141516171819202122232425ll-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt# ll-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt# cd ..# lldrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 3 root root 4096 10-28 06:24 test3drwxr-xr-x 2 root root 4096 10-28 06:48 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3# lldrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt# cd ..# mv test4 test3# lldrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 4 root root 4096 10-28 06:54 test3drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3/# lldrwxr-xr-x 2 root root 4096 10-28 06:21 log-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-28 06:48 test4 说明： 如果目录dir2不存在，将目录dir1改名为dir2；否则，将dir1移动到dir2中。 7）移动当前文件夹下的所有文件到上一级目录 123456789101112# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt# mv * ../# ll# cd ..# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txtdrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-28 07:02 test4 8）把当前目录的一个子目录里的文件移动到另一个子目录里 123456789101112131415161718192021222324# lldrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 4 root root 4096 10-28 07:02 test3drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txtdrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-28 07:02 test4# cd ..# mv test3/*.txt test5# cd test5# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1# cd ..# cd test3/# lldrwxr-xr-x 2 root root 4096 10-28 06:21 logsdrwxr-xr-x 2 root root 4096 10-28 07:02 test4 9）文件被覆盖前做简单备份，前面加参数-b 123456789101112# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1# mv log1.txt -b log2.txtmv：是否覆盖“log2.txt”? y# ll-rw-r--r-- 1 root root 25 10-28 07:02 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt~-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1 说明： -b 不接受参数，mv会去读取环境变量VERSION_CONTROL来作为备份策略。 –backup该选项指定如果目标文件存在时的动作，共有四种备份策略： 1.CONTROL=none或off : 不备份。 2.CONTROL=numbered或t：数字编号的备份 3.CONTROL=existing或nil：如果存在以数字编号的备份，则继续编号备份m+1…n： 执行mv操作前已存在以数字编号的文件log2.txt.~1~，那么再次执行将产生log2.txt~2~，以次类推。如果之前没有以数字编号的文件，则使用下面讲到的简单备份。 4.CONTROL=simple或never：使用简单备份：在被覆盖前进行了简单备份，简单备份只能有一份，再次被覆盖时，简单备份也会被覆盖。 参考链接： http://www.cnblogs.com/peida/archive/2012/10/27/2743022.html http://man.linuxde.net/mv]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（6）-rmdir]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%886%EF%BC%89-rmdir%2F</url>
    <content type="text"><![CDATA[rmdir命令。rmdir是常用的命令，该命令的功能是删除空目录，一个目录被删除之前必须是空的。（注意，rm - r dir命令可代替rmdir，但是有很大危险性。）删除某目录时也必须具有对父目录的写权限。 语法rmdir(选项)(参数) 选项12345-p或--parents：删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除；--ignore-fail-on-non-empty：此选项使rmdir命令忽略由于删除非空目录时导致的错误信息；-v或-verboes：显示命令的详细执行过程；--help：显示命令的帮助信息；--version：显示命令的版本信息。 参数目录列表：要删除的空目录列表。当删除多个空目录时，目录名之间使用空格隔开。 常用范例1）rmdir 不能删除非空目录 1234567891011121314151617181920212223242526272829303132# tree.|-- bin|-- doc| |-- info| `-- product|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product12 directories, 0 files# rmdir docrmdir: doc: 目录非空# rmdir doc/info# rmdir doc/product# tree.|-- bin|-- doc|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product10 directories, 0 files 2）rmdir -p 当子目录被删除后使它也成为空目录的话，则顺便一并删除 12345678910111213141516171819202122232425262728293031323334353637# tree.|-- bin|-- doc|-- lib|-- logs| `-- product`-- service `-- deploy |-- info `-- product10 directories, 0 files# rmdir -p logsrmdir: logs: 目录非空# tree.|-- bin|-- doc|-- lib|-- logs| `-- product`-- service `-- deploy |-- info `-- product9 directories, 0 files# rmdir -p logs/product# tree.|-- bin|-- doc|-- lib`-- service`-- deploy |-- info `-- product7 directories, 0 files 参考链接： http://www.cnblogs.com/peida/archive/2012/10/27/2742076.html http://man.linuxde.net/rmdir]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（5）-rm]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%885%EF%BC%89-rm%2F</url>
    <content type="text"><![CDATA[rm命令可以删除一个目录中的一个或多个文件或目录，也可以将某个目录及其下属的所有文件及其子目录均删除掉。对于链接文件，只是删除整个链接文件，而原有文件保持不变。 rm是一个危险的命令，使用的时候要特别当心，尤其对于新手，否则整个系统就会毁在这个命令（比如在/（根目录）下执行rm * -rf）。所以，我们在执行rm之前最好先确认一下在哪个目录，到底要删除什么东西，操作时保持高度清醒的头脑。 语法rm (选项)(参数) 选项123456-d：直接把欲删除的目录的硬连接数据删除成0，删除该目录；-f：强制删除文件或目录；-i：删除已有文件或目录之前先询问用户；-r或-R：递归处理，将指定目录下的所有文件与子目录一并处理；--preserve-root：不对根目录进行递归操作；-v：显示指令的详细执行过程。 参数文件：指定被删除的文件列表，如果参数中含有目录，则必须加上-r或者-R选项。 常用范例1）删除文件file，系统会先询问是否删除。 12# rm log.log rm：是否删除 一般文件 “log.log”? y 输入rm log.log命令后，系统会询问是否删除，输入y后就会删除文件，不想删除则数据n。 2）强行删除file，系统不再提示 1rm -f log1.log 3）删除任何.log文件；删除前逐一询问确认 1rm -i *.log 4）将 test1子目录及子目录中所有档案删除 1rm -r test1 参考链接： http://man.linuxde.net/rm http://www.cnblogs.com/peida/archive/2012/10/26/2740521.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（4）-mkdir]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%884%EF%BC%89-mkdir%2F</url>
    <content type="text"><![CDATA[mkdir命令用来创建目录。该命令创建由dirname命名的目录。如果在目录名的前面没有加任何路径名，则在当前目录下创建由dirname指定的目录；如果给出了一个已经存在的路径，将会在该目录下创建一个指定的目录。在创建目录时，应保证新建的目录与它所在目录下的文件没有重名。 语法cd (选项) (参数) 选项12345-Z：设置安全上下文，当使用SELinux时有效；-m&lt;目标属性&gt;或--mode&lt;目标属性&gt;建立目录的同时设置目录的权限；-p或--parents 若所要建立目录的上层目录目前尚未建立，则会一并建立上层目录；-v, --verbose 每次创建新目录都显示信息--version 显示版本信息。 常用范例1）创建一个空目录 1mkdir test1 2）递归创建多个目录 1mkdir -p test2/test22 3）创建权限为777的目录 1mkdir -m 777 test3 4）一个命令创建项目的目录结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#mkdir -vp scf/&#123;lib/,bin/,doc/&#123;info,product&#125;,logs/&#123;info,product&#125;,service/deploy/&#123;info,product&#125;&#125;mkdir: 已创建目录 “scf”mkdir: 已创建目录 “scf/lib”mkdir: 已创建目录 “scf/bin”mkdir: 已创建目录 “scf/doc”mkdir: 已创建目录 “scf/doc/info”mkdir: 已创建目录 “scf/doc/product”mkdir: 已创建目录 “scf/logs”mkdir: 已创建目录 “scf/logs/info”mkdir: 已创建目录 “scf/logs/product”mkdir: 已创建目录 “scf/service”mkdir: 已创建目录 “scf/service/deploy”mkdir: 已创建目录 “scf/service/deploy/info”mkdir: 已创建目录 “scf/service/deploy/product”# tree scf/scf/|-- bin|-- doc| |-- info| `-- product|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product12 directories, 0 files 参考链接： http://www.cnblogs.com/peida/archive/2012/10/25/2738271.html http://man.linuxde.net/mkdir]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（3）-pwd]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%883%EF%BC%89-pwd%2F</url>
    <content type="text"><![CDATA[Linux中用 pwd 命令来查看”当前工作目录“的完整路径。 简单得说，每当你在终端进行操作时，你都会有一个当前工作目录。 在不太确定当前位置时，就会使用pwd来判定当前目录在文件系统内的确切位置。 语法pwd（选项） 选项1234-L 目录连接链接时，输出连接路径-P 输出物理路径--help：显示帮助信息；--version：显示版本信息。 常用范例显示当前位置 1pwd 参考链接： http://www.cnblogs.com/peida/archive/2012/10/24/2737730.html http://man.linuxde.net/pwd]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天一个linux命令（2）-cd命令]]></title>
    <url>%2F2018%2F07%2F27%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%882%EF%BC%89-cd%2F</url>
    <content type="text"><![CDATA[Linux cd 命令可以说是Linux中最基本的命令语句，其他的命令语句要进行操作，都是建立在使用 cd 命令上的。所以，学习Linux 常用命令，首先就要学好 cd 命令的使用方法技巧。 语法cd (选项) (参数) 选项123-p 如果要切换到的目标目录是一个符号连接，直接切换到符号连接指向的目标目录-L 如果要切换的目标目录是一个符号的连接，直接切换到字符连接名代表的目录，而非符号连接所指向的目标目录。- 当仅实用&quot;-&quot;一个选项时，当前工作目录将被切换到环境变量&quot;OLDPWD&quot;所表示的目录。 常用范例1）进入系统根目录 1cd / 2）进入当前用户主目录 1cd ~ 3）跳转到指定目录 1cd /opt/soft 4）返回进入此目录之前所在的目录 1cd - 参考链接： http://www.cnblogs.com/peida/archive/2012/10/24/2736501.html http://man.linuxde.net/cd]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk（4）-Awk语言（上）]]></title>
    <url>%2F2018%2F07%2F27%2Fawk%EF%BC%884%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最简单的 awk 程序是一个由多个 模式–动作 语句构成的序列:pattern { action }pattern { action }… 在某些语句中, 模式可以不存在; 还有些语句, 动作及其包围它的花括号也可以不存在. 如果程序经过awk 检查后没有发现语法错误, 它就会每次读取一个输入行, 对读取到的每一行, 按顺序检查每一个模式. 对每一个与当前行匹配的模式, 对应的动作就会执行. 一个缺失的模式匹配每一个输入行, 因此每一个不带有模式的动作对每一个输入行都会执行. 只含有模式而没有动作的语句, 会打印每一个匹配模式的输入行. 大部分情况下, 在这一章出现的术语 “输入行” 与 “记录” 被当作一对同义词. 输入文件 countries作为本章许多 awk 程序的输入数据, 我们将使用文件 countries. 每一行都包括一个国家的名字, 面积 (以千平方英里为单位), 人口 (以百万为单位), 以及这个国家所在的大陆. 数据来源于 1984 年, 苏联被归到了亚洲. 在文件里, 四列数据用制表符分隔, 用一个空格分隔 North (South) 与 America.文件 countries 包含下面几行: 22USSR 8649 275 AsiaCanada 3852 25 North AmericaChina 3705 1032 AsiaUSA 3615 237 North AmericaBrazil 3286 134 South AmericaIndia 1267 746 Asia Mexico 762 78 North AmericaFrance 211 55 EuropeJapan 144 120 AsiaGermany 96 61 EuropeEngland 94 56 Europe在这一章的剩下部分里, 如果没有显式给出输入数据, 默认将 countries 作为输入. 程序格式模式–动作 语句, 以及动作内的语句通常用换行符分隔, 但是若干条语句也可以出现在同一行, 只要它们之间用分号分开即可. 一个分号可以放在任何语句的末尾. 动作的左花括号必须与它的模式在同一行; 而剩下的部分, 包括右花括号, 则可以出现在下面几行.空行会被忽略; 它们可以插入在语句之前或之后, 用于提高程序的可读性. 空格与制表符可以出现在运算符与操作数的周围, 同样也是为了提高可读性.注释可以出现在任意一行的末尾. 一个注释以井号 (#) 开始, 以换行符结束, 正如{ print $1, $3 } # print country name and population一条长语句可以分散成多行, 只要在断行处插入一个反斜杠即可:{ print \$1, # country name$2, # area in thousands of square miles$3 } # population in millions 正如这个例子所呈现的那样, 语句可以在逗号之后断行, 并且注释可以出现在断行的末尾.在这本书里我们用到了若干种编程风格, 之所以这样做, 一方面是为了比较不同风格之间的差异, 另一方面是为了避免程序占用过多的行. 对于比较短小的程序 — 就像本章中出现过的那些例子 — 格式并不是非常重要, 但是一致性与可读性对于大程序的管理非常有帮助. 模式模式控制着动作的执行: 当模式匹配时, 相应的动作便会执行. 这一小节描述模式的 6种类型, 以及匹配它们的条件. 模式汇总 BEGIN{ statements}在输入被读取之前, statements 执行一次. END{ statements}当所有输入被读取完毕之后, statements 执行一次. expression{ statements}每碰到一个使 expression 为真的输入行, statements 就执行. expression 为真指的是其值非零或非空. /regular expression/ { statements}当碰到这样一个输入行时, statements 就执行: 输入行含有一段字符串, 而该字符串可以被 regular expression 匹配. compound pattern { statements}一个复合模式将表达式用&amp;&amp;(AND),||(OR),!(NOT),以及括号组合起来; 当com-pound pattern 为真时, statements 执行. pattern 1 , pattern 2 { statements}一个范围模式匹配多个输入行, 这些输入行从匹配 pattern 1 的行开始, 到匹配 pat-tern 2 的行结束 (包括这两行), 对这其中的每一行执行 statements. BEGIN 与 END 不与其他模式组合. 一个范围模式不能是其他模式的一部分. BEGIN 与END 是唯一两个不能省略动作的模式.符. 将 FS 设置成一个非空格字符, 就会使该字符成为字段分割符.下面这个程序在 BEGIN 的动作里将字段字割符设置为制表符 (\t), 并在输出之前打印标题. 第二个 printf 语句 (对每一个输入行它都会执行) 将输出格式化成一张表格, 使得每一列都刚好与标题的列表头对齐. END 打印总和.​ BEGIN 与 与 ENDBEGIN 与 END 这两个模式不匹配任何输入行. 实际情况是, 当 awk 从输入读取数据之前, BEGIN 的语句开始执行; 当所有输入数据被读取完毕, END 的语句开始执行. 于是,BEGIN 与 END 分别提供了一种控制初始化与扫尾的方式. BEGIN 与 END 不能与其他模式作组合. 如果有多个 BEGIN, 与其关联的动作会按照它们在程序中出现的顺序执行, 这种行为对多个 END 同样适用. 我们通常将 BEGIN 放在程序开头, 将 END 放在程序末尾, 虽然这并不是强制的.BEGIN 的一个常见用途是更改输入行被分割为字段的默认方式. 分割字符由一个内建变量 FS 控制. 默认情况下字段由空格或 (和) 制表符分割, 此时 FS 的值被设置为一个空格符. 将 FS 设置成一个非空格字符, 就会使该字符成为字段分割符.下面这个程序在 BEGIN 的动作里将字段字割符设置为制表符 (\t), 并在输出之前打印标题. 第二个 printf 语句 (对每一个输入行它都会执行) 将输出格式化成一张表格, 使得每一列都刚好与标题的列表头对齐. END 打印总和. print countries with column headers and totalsBEGIN { FS = “\t” # make tab the field separatorprintf(“%10s %6s %5s %s\n\n”,“COUNTRY”, “AREA”, “POP”, “CONTINENT”)}{ printf(“%10s %6d %5d %s\n”, $1, $2, $3, $4)area = area + $2pop = pop + $3}END { printf(“\n%10s %6d %5d\n”, “TOTAL”, area, pop) }如果将 countries 作为输入, 将出将是COUNTRY AREA POP CONTINENTUSSR 8649 275 AsiaCanada 3852 25 North AmericaChina 3705 1032 AsiaUSA 3615 237 North AmericaBrazil 3286 134 South AmericaIndia 1267 746 AsiaMexico 762 78 North AmericaFrance 211 55 EuropeJapan 144 120 AsiaGermany 96 61 EuropeEngland 94 56 EuropeTOTAL 25681 2819 将表达式用作模式就像大多数程序设计语言一样，awk拥有非常丰富的用来描述数值计算的表达式，但是与许多语言不同的是，awk还有用于描述字符串操作的表达式.贯穿全书, string 都表示一个由 0 个或多个字符组成的序列. 这些字符串可以存储在变量中, 也可以以字符串常量的形式出现, 就像 “” 或 “Asia”. 字符串 “” 不包括任何字符, 叫做 空字符串 (null string). 术语 串 子字符串 (substring) 表示一个字符串内部的, 由 0 个或多个字符组成的连续序列. 对任意一个字符串, 空字符串都可以看作是该字符串第一个字符之前的, 长度为 0 的子字符串,或者是一对相邻字符之间的子字符串, 又或者是最后一个字符之后的子字符串.任意一个表达式都可以用作任意一个运算符的操作数. 如果一个表达式拥有一个数值形式的值, 而运算符要求一个字符串值, 那么该数值会自动转换成字符串, 类似地, 当运算符 25要求一个数值时, 字符串被自动转换成数值.任意一个表达式都可以当作模式来使用. 如果一个作为模式使用的表达式, 对当前输入行的求值结果不空或非零, 那么该模式就匹配该行. 典型的表达式模式是那些涉及到数值或字符串比较的表达式. 一个比较表达式包含 6 种关系运算符中的一种, 或者包含两种字符串匹配运算符中的一种: ~ 与 !~ 在下一小节讨论. 关系运算符如下图： 如果模式是一个比较表达式, 就像 NF &gt; 10, 当当前行使该条件满足时, 这个模式就算是匹配该输入行, 在这里条件满足指的是当前输入行的字段数大于 10. 如果模式是一个算术表达式, 就像 NF, 如果该表达式的值非零, 那么当前输入行被匹配. 如果模式是一个字符串表达式, 当表达式的字符串值非空时, 当前输入行被匹配. 在一个关系比较中, 如果两个操作数都是数值, 关系比较将会按照数值比较进行; 否则的话, 数值操作数会被转换成字符串, 再将操作数按字符串的形式进行比较. 两个字符串间的比较以字符为单位逐个相比, 字符间的先后顺序依赖于机器的字符集 (大多数情况下是 ASCII 字符集). 一个字符串 “小于” 另一个, 指的是它比另一个字符串更早出现, 例如: “Canada” &lt; “China”, “Asia” &lt; “Asian”.模式\$3/$2 &gt;= 0.5选择的行, 其第 3 个字段除以第 2 个字段所得的商大于 0.5, 而$0 &gt;= “M”26选择那些以字母 M, N, O 等开头的输入行:USSR 8649 275 AsiaUSA 3615 237 North AmericaMexico 762 78 North America有时候一个比较运算符的类型不能单单靠表达式表现出来的语法形式来判断. 程序\$1 &lt; $4可以以数值的形式, 或者字符串的形式比较输入行的第 1 个与第 4 个字段. 在这里, 比较的类型取决于字段的值, 并且有可能每一行都有不同的情况出现. 文件 countries 的第 1 个与第 4 个字段总是字符串, 所以比较总是以字符串的形式进行; 输出是Canada 3852 25 North AmericaBrazil 3286 134 South AmericaMexico 762 78 North AmericaEngland 94 56 Europe 只有当两个字段都是数值时, 比较才会以数值的形式进行; 这种情况可以是\$2 &lt; $3 字符串匹配模式Awk 提供了一种称为 正则表达式 (regular expression) 的表示法, 它可以用来指定和匹配一个字符串. 正则表达式在 Unix 环境用得非常普遍, 包括文本编辑器与 shell. 受限形式的正则表达式也出现在其他系统中, 在 MS-DOS 中可以用 “通配符” 指定一个文件名集合. 一个 式 字符串匹配模式 (string-matching pattern) 测试一个字符串是否包含一段可以被正则表达式匹配的子字符串.最简单的正则表达式是仅由数字与字母组成的字符串, 就像 Asia, 它匹配的就是它本身. 为了将一个正则表达式切换成一个模式, 只需要用一对斜杠包围起来即可: /Asia/这个模式匹配那些含有子字符串 Asia 的输入行, 例如 Asia, Asian, 或 Pan-Asiatic.注意, 正则表达式中空格是有意义的: 字符串匹配模式 字符串匹配模式 /regexpr/当当前输入行包含一段能够被 regexpr 匹配的子字符串时, 该模式被匹配. expression ~ /regexpr/如果 expression 的字符串值包含一段能够被 regexpr 匹配的子字符时, 该模式被匹配. expression !~ /regexpr/如果 expression 的字符串值不包含能够被 regexpr 匹配的子字符串, 该模式被匹配.在 ~ 与 !~ 的语境中, 任意一个表达式都可以用来替换 /regexpr/./ Asia /只有当 Asia 被一对空格包围时才会匹配成功.上面的模式是三种字符串匹配模式当中的一种. 它的形式是用一对斜杠将正则表达式包围起来:/r/如果某个输入行含有能被 r 匹配的子字符串, 则该行匹配成功.剩下的两种字符串匹配模式使用到了显式的匹配运算符:expression ~ /r/expression !~ /r/ 匹配运算符 ~ 的意思是 “被… 匹配”, !~ 的意思是 “不被… 匹配”. 当 expression 的字符串值包含一段能够被正则表达式 r 匹配的子字串时, 第一个模式被匹配; 当不存在这样的子字符串时, 第二个模式被匹配.匹配运算符的左操作数经常是一个字段, 模式$4 ~ /Asia/匹配所有第 4 个字段包含 Asia 的输入行, 而$4 !~ /Asia/ 匹配所有第 4 个字段 不包含 Asia 的输入行.注意到, 字符串匹配模式/Asia/是$0 ~ /Asia/的简写形式. 正则表达式正则表达式是一种用于指定和匹配字符串的表示法. 就像算术表达式一样, 一个正则表达式是一个基本表达式, 或者是多个子表达式通过运算符组合而成. 为了理解被一个正则表达式匹配的字符串, 我们需要先了解被子表达式匹配的字符串. 正则表达式 正则表达式的元字符包括:\ ^ $ . [ ] | ( ) * + ? 一个基本的正则表达式包括下面几种:一个不是元字符的字符, 例如 A, 这个正则表达式匹配的就是它本身.一个匹配特殊符号的转义字符: \t 匹配一个制表符 (见表 2.2).一个被引用的元字符, 例如 *, 按字面意义匹配元字符.^ 匹配一行的开始.$ 匹配一行的结束.. 匹配任意一个字符.一个字符类: [ABC] 匹配字符 A, B 或 C.字符类可能包含缩写形式: [A-Za-z] 匹配单个字母.一个互补的字符类: [^0-9] 匹配任意一个不是数字的字符. 这些运算符将正则表达式组合起来:选择: A|B 匹配 A 或 B.拼接: AB 匹配后面紧跟着 B 的 A.闭包: A* 匹配 0 个或多个 A.正闭包: A+ 匹配一个或多个 A.零或一: A? 匹配空字符串或 A.括号: 被 (r) 匹配的字符串, 与 r 所匹配的字符串相同. 基本的正则表达式在上面的表格中列出. 字符\ ^ $ . [ ] | ( ) * + ?叫作 符 元字符 (metacharacter), 之所以这样称呼是因为它们具有特殊的意义. 一个由单个非元字符构成的正则表达式匹配它自身. 于是, 一个字母或一个数字都算作是一个基本的正则表达式, 与自身相匹配. 为了在正则表达式中保留元字符的字面意义, 需要在字符的前面加上反斜杠. 于是, \$ 匹配普通字符 $. 如果某个字符前面冠有 \, 我们就说该字符是被 引用(quoted) 的.在一个正则表达式中, 一个未被引用的脱字符 ^ 表示一行的开始, 一个未被引用的美元符 $ 匹配一行的结束, 一个未被引用的句点 . 匹配任意一个字符. 于是,^C 匹配以字符 C 开始的字符串;C$ 匹配以字符 C 结束的字符串;^C$ 匹配只含有单个字符 C 的字符串;^.$ 匹配有且仅有一个字符的字符串;^…$ 匹配有且仅有 3 个字符的字符串;… 匹配任意 3 个字符;.$ 匹配以句点结束的字符串. 由一组被包围在方括号中的字符组成的正则表达式称为 类 字符类 (character class); 这个表达式匹配字符类中的任意一个字符. 例如, [AEIOU] 匹配 A, E, I, O 或 U.使用连字符的字符类可以表示一段字符范围. 紧跟在连字符左边的字符定义了范围的开始, 紧跟在连字符右边的字符定义了范围的结束. 于是, [0-9] 匹配任意一个数字,[a-zA-Z][0-9] 匹配一个后面紧跟着一个数字的字母. 如果左右两边都没有操作数, 那么字符类中的连字符就表示它本身, 所以 [+-] 与 [-+] 匹配一个 + 或 -. [A-Za-z-]+ 匹配一个可能包含连字符的单词.一个 补 互补 (complemented) 的字符类在 [ 之后以 ^ 开始. 这样一个类匹配任意一个不在类中的字符, “类中的字符” 指的是方括号内排在脱字符之后的那些字符. 于是, [^0-9]匹配任意一个不是数字的字符; [^a-zA-Z] 匹配任意一个不是字母的字符.^[ABC] 匹配以 A, B, 或 C 开始的字符串;^[^ABC] 匹配以任意一个字符 (除了 A, B, 或 C) 开始的字符串;[^ABC] 匹配任意一个字符, 除了 A, B, 或 C;^[^a-z]$ 匹配任意一个有且仅有一个字符的字符串, 且该字符不能是小写字母. 在一个字符类中, 所有的字符都具有它自身的字面意义, 除了引用字符 \, 互补字符类开头的 ^, 以及两个字符间的 -. 于是, [.] 匹配一个句点, ^[^^] 匹配不以脱字符开始的字符串.可以使用括号来指定正则表达式中的各个成分如何组合. 有两种二元正则表达式运算符: 选择与拼接. 选择运算符 | 用来指定一个选择: 如果 r 1 与 r 2 是正则表达式, 那么 r 1 |r 2所匹配的字符串, 或者与 r 1 , 或者与 r 2 匹配.Awk 不存在显式的拼接运算符. 如果 r 1 与 r 2 是正则表达式, 那么 (r 1 )(r 1 ) (在 (r 1 )与 (r 2 ) 之间没有空格) 所匹配的字符串具有形式 xy, 其中 x 被 r 1 匹配, y 被 r 2 匹配. 如果被括号包围的正则表达式不包含选择运算符, 那么 r 1 或 r 2 两边的括号就可以省略. 正则表达式(Asian|European|North American) (male|female) (black|blue)bird一共匹配 12 种字符串, 从Asian male blackbird到 注：The AWK Programming Language学习笔记]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk（3）-快速入门（下）]]></title>
    <url>%2F2018%2F07%2F27%2Fawk%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Awk 是一种使用方便且表现力很强的编程语言, 它可以应用在多种不同的计算与数据处理任务中. 流程控制语句Awk 提供了用于决策的 if-else 语句, 以及循环语句, 所有的这些都来源于 C 语言. 它们只能用在动作 (Action) 里. If-Else 语句下面这个程序计算每小时工资多于 $6.00 的雇员的总报酬与平均报酬. 在计算平均数时, 它用到了if 语句, 避免用 0 作除数.$2 &gt; 6 { n = n + 1; pay = pay + $2 * $3 }END { if (n &gt; 0)print n, “employees, total pay is”, pay,“average pay is”, pay/nelseprint “no employees are paid more than $6/hour”}emp.data 的输出是:no employees are paid more than $6/hour 在 if-else 语句里, if 后面的条件被求值, 如果条件为真, 第一个 print 语句执行, 否则是第二个. print 语句被执行. 注意到, 在逗号后面断行, 我们可以将一个长语句延续到下一行. While 语句一个 while 含有一个条件判断与一个循环体. 当条件为真时, 循环体执行. 下面这个程序展示了一笔钱在一个特定的利率下, 其价值如何随着投资时间的增长而增加, 价值计算的公式是 value =amount(1 + rate) years . #interest1 - compute compound interest #input: amount rate years #output: compounded value at the end of each year { i = 1while (i &lt;= $3) {printf(“\t%.2f\n”, $1 * (1 + $2) ^ i)i = i + 1}}while 后面被括号包围起来的表达式是条件判断; 循环体是跟在条件判断后面的, 被花括号包围起来的的两条语句. printf 格式控制字符串里的 \t 表示一个制表符; ^ 是指数运算符. 从井号 (#) 开始, 直到行末的文本是 释 注释 (comment), 注释会被 awk 忽略, 但有助于其他人读懂程序.读者可以键入三个数, 看一下不同的本金, 利率和时间会产生怎样的结果. 举个例子, 这个交易展示了 $1000 在 6% 与 12% 的利率下, 在 5 年的时间里如何升值:$ awk -f interest11000 .06 51060.001123.601191.021262.481338.231000 .12 51120.001254.401404.931573.521762.34 For 语句大多数循环都包括初始化, 测试, 增值, 而 for 语句将这三者压缩成一行. 这里是前一个计算投资回报的程序, 不过这次用 for 循环: # interest2 - compute compound interest # input: amount rate years #output: compounded value at the end of each year { for (i = 1; i &lt;= $3; i = i + 1)printf(“\t%.2f\n”, \$1 * (1 + $2) ^ i)} 数组Awk 提供了数组, 用来存储一组相关的值. 虽然数组给予了 awk 非常可观的力量, 但是我们在这里只展示一个简单的例子. 下面这个程序按行逆序显示输入数据. 第一个动作将输入行放入数组 line的下一个元素中; 也就是说, 第一行放入 line[1], 第二行放入 line[2], 依次类推. END 动作用一个while 循环, 从数组的最后一个元素开始打印, 一直打印到第一个元素为止: reverse - print input in reverse order by line{ line[NR] = $0 } # remember each input lineEND { i = NR # print lines in reverse orderwhile (i &gt; 0) {print line[i]i = i - 1}}用 emp.data 作输入, 输出是Susie 4.25 18Mary 5.50 22Mark 5.00 20Kathy 4.00 10Dan 3.75 0Beth 4.00 0 这是用 for 循环实现的等价的程序: 17 # reverse - print input in reverse order by line { line[NR] = $0 } # remember each input lineEND { for (i = NR; i &gt; 0; i = i - 1) print line[i]} 实用“一行”手册虽然 awk 可以写出非常复杂的程序, 但是许多实用的程序并不比我们目前为止看到的复杂多少. 这里有一些小程序集合, 对读者应该会有一些参考价值. 大多数是我们已经讨论过的程序的变形. 输入行的总行数END { print NR } 打印第 10 行NR == 10 打印每一个输入行的最后一个字段{ print $NF } 打印最后一行的最后一个字段{ field = $NF }END { print field } 打印字段数多于 4 个的输入行NF &gt; 4 打印最后一个字段值大于 4 的输入行$NF &gt; 4 打印所有输入行的字段数的总和{ nf = nf + NF }END { print nf } 打印包含 Beth 的行的数量 /Beth/ { nlines = nlines + 1 }END { print nlines } 打印具有最大值的第一个字段, 以及包含它的行 (假设 $1 总是 正的) $1 &gt; max { max = $1; maxline = $0 }END { print max, maxline } 打印至少包含一个字段的行NF &gt; 0 打印长度超过 80 个字符的行length($0) &gt; 80 在每一行的前面加上它的字段数{ print NF, $0 } 打印每一行的第 1 与第 2 个字段, 但顺序相反{ print \$2, $1 } 交换每一行的第 1 与第 2 个字段, 并打印该行{ temp = \$1; \$1 = \$2; $2 = temp; print } 将每一行的第一个字段用行号代替{ $1 = NR; print } 打印删除了第 2 个字段后的行{ $2 = “”; print } 将每一行的字段按逆序打印{ for (i = NF; i &gt; 0; i = i - 1) printf(“%s “, $i)printf(“\n”)} 打印每一行的所有字段值之和{ sum = 0for (i = 1; i &lt;= NF; i = i + 1) sum = sum + $iprint sum} 将所有行的所有字段值累加起来{ for (i = 1; i &lt;= NF; i = i + 1) sum = sum + $i }END { print sum } 将每一行的每一个字段用它的绝对值替换{ for (i = 1; i &lt;= NF; i = i + 1) if (\$i &lt; 0) \$i = -$iprint} 注：The AWK Programming Language学习笔记]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk（2）-快速入门（中）]]></title>
    <url>%2F2018%2F07%2F27%2Fawk%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Awk 是一种使用方便且表现力很强的编程语言, 它可以应用在多种不同的计算与数据处理任务中. 选择Awk 的模式非常擅长从输入中选择感兴趣的行, 以便进行进一步的处理. 因为一个没有动作的模式会将所有匹配的行打印出来, 所以许多 awk 程序仅含有一条单独的模式. 这一节给出的的例子, 其模式具有很高的实用价值. 通过比较进行选择这个程序使用一个比较模式来选择某些雇员的记录, 条件是他的每小时工资大于等于 $5.00, 也就是第二个字段大于等于 5:$2 &gt;= 5它从 emp.data 中选择这些行:Mark 5.00 20Mary 5.50 22 通过计算进行选择这个程序使用一个比较模式来选择某些雇员的记录, 条件是他的每小时工资大于等于 $5.00, 也就是第二个字段大于等于 5:$2 &gt;= 5它从 emp.data 中选择这些行:Mark 5.00 20Mary 5.50 22 通过文本内容选择除了数值选择, 用户也可以选择那些包含特定单词或短语的输入行. 这个程序打印所有第一个字段是 Susie 的行:$1 == “Susie” 操作符 == 测试相等性. 用户也可以搜索含有任意字母, 单词或短语的文本, 通过一个叫做 正则表达式(regular expressions) 的模式来完成. 这个程序打印所有包含 Susie 的行: /Susie/输出是Susie 4.25 18正则表达式可以用来指定非常精细的模式. 模式的组合模式可以使用括号和逻辑运算符进行组合, 逻辑运算符包括 &amp;&amp;, ||, 和 !, 分别表示 AND, OR, 和NOT. 程序$2 &gt;= 4 || $3 &gt;= 20打印那些 $2 至少为 4, 或者 $3 至少为 20 的行:Beth 4.00 0Kathy 4.00 10Mark 5.00 20Mary 5.50 22Susie 4.25 18两个条件都满足的行只输出一次. 将这个程序与下面这个程序作对比, 它包含两个模式:$2 &gt;= 4$3 &gt;= 20如果某行对这两个条件都满足, 它会被打印两次Beth 4.00 0Kathy 4.00 10Mark 5.00 20Mark 5.00 20Mary 5.50 22Mary 5.50 22Susie 4.25 18注意程序!(\$2 &lt; 4 &amp;&amp; $3 &lt; 20)打印的行不满足这样的条件: $2 小于 4, 并且 $3 也小于 20; 这个条件判断与上面的第一个等价, 虽然在可读性方面差了一点. 数据验证真实的数据总是存在错误. 检查数据是否具有合理的值, 格式是否正确, 这种任务通常称作 数据验证(data validation), 在这一方面 awk 是一款非常优秀的工具. 数据验证在本质上是否定: 不打印具有期望的属性的行, 而是打印可疑行. 接下来的程序使用比较模式, 将 5 条合理性测试应用到 emp.data 的每一行: 11NF != 3 { print $0, “number of fields is not equal to 3” }$2 &lt; 3.35 { print $0, “rate is below minimum wage” }$2 &gt; 10 { print $0, “rate exceeds $10 per hour” }$3 &lt; 0 { print $0, “negative hours worked” }$3 &gt; 60 { print $0, “too many hours worked” }如果数据没有错误, 就不会有输出. BEGIN 与 END特殊的模式 BEGIN 在第一个输入文件的第一行之前被匹配, END 在最后一个输入文件的最后一行被处理之后匹配. 这个程序使用 BEGIN 打印一个标题:BEGIN { print “NAME RATE HOURS”; print “” }{ print }输出是NAME RATE HOURSBeth 4.00 0Dan 3.75 0Kathy 4.00 10Mark 5.00 20Mary 5.50 22Susie 4.25 18可以在同一行放置多个语句, 语句之间用分号分开. 注意 print “” 打印一个空行, 它与一个单独的print 并不相同, 后者打印当前行. 用AWK计算一个动作就是一个语句序列, 语句之间用分号或换行符分开. 读者已经见过只有一条单独的 print语句的动作. 这一小节提供的例子所包含的语句可以用来进行简单的数学或字符串计算. 在这些语句里,不仅可以使用内建变量, 比如 NF, 还可以自己定义变量, 这些变量可以用来计算, 存储数据等. 在 awk中, 用户创建的变量不需要事先声明就可以使用. 计数这个程序用一个变量 emp 计算工作时长超过 15 个小时的员工人数: 12$3 &gt; 15 { emp = emp + 1 }END { print emp, “employees worked more than 15 hours” }对每一个第三个字段超过 15 的行, 变量 emp 的值就加 1. 用 emp.data 作输入数据, 这个程序输出:3 employees worked more than 15 hours当 awk 的变量作为数值使用时, 默认初始值为 0, 所以我们没必要初始化 emp. 计算总和与平均数为了计算雇员的人数, 我们可以使用内建变量 NR, 它的值是到目前为止读取到的行数; 当所有输入都处理完毕时, 它的值就是读取到的行数.END {print NR, “employees” }输出是:6 employees这里有个程序利用 NR 来计算平均报酬:{ pay = pay + $2 * $3 }END { print NR, “employees”print “total pay is”, payprint “average pay is”, pay / NR}第一个动作累加所有雇员的报酬. END 动作打印:6 employeestotal pay is 337.5average pay is 56.25很明显, printf 可以用来产生更加美观的输出. 这个程序有一个潜在的错误: 一种不常见的情况是 NR的值为 0, 程序会尝试将 0 作除数, 此时 awk 就会产生一条错误消息. 操作文本Awk 的长处之一是它可以非常方便地对字符串进行操作, 就像其他大多数语言处理数值那样方便.Awk 的变量除了可以存储数值, 还可以存储字符串. 这个程序搜索每小时工资最高的雇员:$2 &gt; maxrate { maxrate = $2; maxemp = $1 }END { print “highest hourly rate:”, maxrate, “for”, maxemp }它的输出是: highest hourly rate: 5.50 for Mary在这个程序里, 变量 maxrate 保存的是数值, 而 maxemp 保存的是字符串. (如果有多个雇员都拥有相同的最高每小时工资, 这个程序只会打印第一个人的名字.) 字符串拼接可以通过旧字符串的组合来生成一个新字符串; 这个操作叫作 拼接 (concatenation). 程序 { names = names $1 “ “ }END { print names }将所有雇员的名字都收集到一个单独的字符串中, 每一次拼接都是把名字与一个空格符添加到变量names 的值的末尾. names 在 END 动作中被打印出来:Beth Dan Kathy Mark Mary Susie 在一个 awk 程序中, 字符串的拼接操作通过陆续写出字符串来完成. 对每一个输入行, 上面程序中的第一条语句将三个字符串连接在一起: names 早先的值, 第一个字段, 以及一个空格; 然后再将结果字符串赋值给 names. 于是, 当所有的输入行都读取完毕时, names 包含有一个由所有雇员名字组成的, 每个名字之间由空格分隔的字符串. 用来存储字符串的变量的初始值默认为空字符串 (也就是说该字符串不包含任何字符), 因此在这个程序里, names 不需要显式地初始化. 打印最后一行虽然在 END 动作里, NR 的值被保留了下来, 但是 $0 却不会. 程序{ last = $0 }END { print last }可以用来打印文件的最后一行:Susie 4.25 18 内建函数我们已经看到 awk 提供有内建变量, 这些变量可以用来维护经常需要用到的量, 比如字段的个数,以及当前输入行的行号. 同样, awk 也提供用来计算其他值的内建函数. 求平方根, 取对数, 随机数, 除了这些数学函数, 还有其他用来操作文本的函数. 其中之一是 length, 它用来计算字符串中字符的个数.例如, 这个程序计算每一个人的名字的长度:{ print \$1, length($1) }程序运行结果是: 14Beth 4Dan 3Kathy 5 Mark 4Mary 4Susie 5 行，单词与字符的计数这个程序使用 length, NF 与 NR 计算行, 单词与字符的数量, 为方便起见, 我们将每个字段都当成一个单词.{ nc = nc + length($0) + 1nw = nw + NF}END { print NR, “lines,”, nw, “words,”, nc, “characters” }文件 emp.data 含有6 lines, 18 words, 77 characters我们为每一个输入行末尾的换行符加 1, 这是因为 $0 不包含换行符. 注：The AWK Programming Language学习笔记]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk（1）-快速入门（上）]]></title>
    <url>%2F2018%2F07%2F27%2Fawk%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Awk 是一种使用方便且表现力很强的编程语言, 它可以应用在多种不同的计算与数据处理任务中. 开始实用的 awk 程序通常都很短, 只有一两行. 假设有一个文件, 叫作 emp.data, 这个文件包含有名字, 每小时工资 (以美元为单位), 工作时长, 每一行代表一个雇员的记录, 就像这样Beth 4.00 0Dan 3.75 0Kathy 4.00 10Mark 5.00 20Mary 5.50 22Susie 4.25 18现在你想打印每位雇员的名字以及他们的报酬 (每小时工资乘以工作时长), 而雇员的工作时长必须大于零. 这种类型的工作是 awk 的设计目标之一, 所以会很简单. 只要键入下面一行即可: 12345# awk &apos;$3 &gt; 0 &#123; print $1, $2 * $3 &#125;&apos; emp.dataKathy 40Mark 100Mary 121Susie 76.5 该行命令告诉操作系统运行 awk 程序, 被运行的程序用单引号包围起来, 从输入文件 emp.data 获取 2数据. 被单引号包围的部分是一个完整的 awk 程序. 它由一个单独的 模式– 动作 语句 (pattern-actionstatement) 组成. 模式 $3 &gt; 0 扫描每一个输入行, 如果该行的第三列 (或者说 字段 (field)) 大于零, 则动作{ print \$1, \$2 * $3 }就会为每一个匹配行打印第一个字段, 以及第二与第三个字段的乘积.如果想知道哪些员工在偷懒, 键入 123# awk &apos;$3 == 0 &#123; print $1 &#125;&apos; emp.dataBethDan 模式 $3 == 0 匹配第三个字段为零的行, 动作 { print $1 } 打印该行的第一个字段. AWK程序的结构现在让我们回退一步, 看一下到底发生了什么. 在上面的命令行中, 被单引号包围的部分是使用 awk语言编写的程序. 本章的每一个 awk 程序都是由一个或多个 模式–动作 语句组成的序列:pattern { action }pattern { action }… awk 的基本操作是在由输入行组成的序列中, 陆续地扫描每一行, 搜索可以被模式 匹配 (match) 的行.“匹配” 的精确含义依赖于问题中的模式, 比如, 对于 $3 &gt; 0, 意味着 “条件为真”.每一个输入行轮流被每一个模式测试. 每匹配一个模式, 对应的动作 (可能包含多个步骤) 就会执行.然后下一行被读取, 匹配重新开始. 这个过程会一起持续到所有的输入被读取完毕为止.上面的程序是模式与动作的典型例子. 程序 $3 == 0 { print $1 }由一条单独的 模式–动作 语句组成: 如果某行的第 3 个字段为 0, 那么它的第 1 个字段就会被打印出来. 在一个 模式–动作 语句中, 模式或动作可以省略其一, 但不能两者同时被省略. 如果一个模式没有动作, 例如$3 == 0会将每一个匹配行 (也就是条件判断为真的行) 打印出来. 这个程序将文件 emp.data 中第 3 个字段为0 的两行打印出来:Beth 4.00 0Dan 3.75 0如果只有动作而没有模式, 例如 { print $1 }对于每一个输入行, 动作 (在这个例子里是打印第 1 个字段) 都会被执行.因为模式与动作都是可选的, 所以用花括号将动作包围起来, 以便区分两者. 运行AWK程序运行一个 awk 程序有多种方式. 可以键入下面这种形式的命令 1# awk &apos;program&apos; input files 这个命令对指定的输入文件的每一行, 执行 program. 例如你可以键入 1# awk &apos;$3 == 0 &#123; print $1 &#125;&apos; file1 file2 打印文件 file1 与 file2 的每一行的第一个字段 (条件是该行的第 3 个字段为 0).也可以在命令行上省略输入文件, 只要键入 1# awk &apos;program&apos; 在这种情况下, awk 会将 program 应用到你接下来在终端输入的内容上面, 直到键入一个文件结束标志(Unix 系统是组合键 Control-d). 下面是一个在 Unix 上运行的例子 123456789# awk &apos;$3 == 0 &#123; print $1 &#125;&apos;Beth 4.00 0BethDan 3.75 0DanKathy 3.75 10Kathy 3.75 0Kathy... Beth、Dan、Kathy为awk打印的字符 这种行为对测试 awk 程序来说非常方便: 键入程序与数据, 检查程序的输出. 我们再次建议读者运行并修改书中的程序.注意到, 命令行中的程序被单引号包围. 这个规定可以防止程序中的字符 (例如 $) 被 shell 解释, 也可以让程序的长度多于一行. 当程序的长度比较短时 (只有几行), 这种安排会比较方便. 如果程序比较长, 更好的做法是将它们放在一个单独的文件中, 如果文件名是 progfile 的话, 运行时只要键入 1# awk -f progfile optional list of files 选项 -f 告诉 awk 从文件中提取程序. 在 progfile 出现的地方可以是任意的文件名. 简单输出我们只将程序的主体显示出来, 而不是完整的命令行. 在每一种情况下, 程序或者可以被包围在一对单引号中, 作为 awk 命令的第一个参数来运行, 也可以将其放入一个文件中, 通过带有 -f 选项的 awk命令来运行.awk 的数据只有两种类型: 数值与由字符组成的字符串. 文件 emp.data 是很典型的待处理数据,它既含有单词, 也包括数值, 且字段之间通过制表符或空格分隔.awk 从它的输入中每次读取一行, 将行分解为一个个的字段 (默认将字段看作是非空白字符组成的序列). 当前输入行的第一个字段叫作 $1, 第二个是 $2, 依次类推. 一整行记为 $0. 每行的字段数有可能不一样.通常情况下, 我们需要做的是打印每一行的部分或全部字段, 也可能会做一些计算. 这一节中的所有程序都是这种形式. 打印每一行如果一个动作没有模式, 对于每一个输入行, 该动作都会被执行. 语句 print 会打印每一个当前输入行, 所以程序{ print }会将它所有的输入打印到标准输出. 因为 $0 表示一整行, 所以程序{ print $0 }完成同样的工作. 打印某些字段在一个 print 语句中可以将多个条目打印到同一个输出行中. 打印每一个输入行的第 1 与第 3 个字段的程序是{ print $1, $3 }当 emp.data 作为输入时, 它会输出Beth 0Dan 0Kathy 10Mark 20Mary 22Susie 18在 print 语句中由逗号分隔的表达式, 在输出时默认用一个空格符分隔. 由 print 打印的每一行都由一个换行符终止. 这些默认行为都可以修改,我们在后续会有介绍. NF,字段的数量有时候, 必须总是通过 $1, $2 这样的形式引用字段, 但是任何表达式都可以出现在 $ 的后面, 用来指明一个字段的编号: 表达式被求值, 求出的值被当作字段的编号. Awk 计算当前输入行的字段数量, 并将它存储在一个内建的变量中, 这个变量叫作 NF. 因此程序{ print NF, $1, $NF }将会打印每一个输入行的字段数量, 第一个字段, 以及最后一个字段. 计算和打印也可以用字段的值进行计算, 并将计算得到的结果放在输出语句中. 程序{ print $1, $2 * $3 }是一个很典型的例子, 它会打印雇员的名字与报酬 (每小时工资乘以工作时长): Beth 0Dan 0Kathy 40Mark 100Mary 121Susie 76.5我们待会儿就会展示如何将输出做得更好看. 打印行号Awk 提供了另一个内建变量 NR, 这个变量计算到目前为止, 读取到的行的数量. 我们可以使用 NR和 $0 为 emp.data 的每一行加上行号: { print NR, $0 }输出就像这样:1 Beth 4.00 02 Dan 3.75 03 Kathy 4.00 104 Mark 5.00 205 Mary 5.50 226 Susie 4.25 18 将文本放入输出中将文本放入输出中也可以把单词放在字段与算术表达式之间:{ print “total pay for”, $1, “is”, $2 * $3 }输出total pay for Beth is 0total pay for Dan is 0total pay for Kathy is 40total pay for Mark is 100total pay for Mary is 121total pay for Susie is 76.5在 print 语句中, 被双引号包围的文本会和字段, 以及运算结果一起输出. 更精美的输出print 用于简单快速的输出. 如果读者想要格式化输出, 那么就需要使用 printf 语句. 字段排序printf 语句具有形式printf(format, value 1 , value 2 , … , value n ) format 是一个字符串, 它包含按字面打印的文本, 中间散布着格式说明符, 格式说明符用于说明如何打印值. 一个格式说明符是一个 %, 后面跟着几个字符, 这些字符控制一个 value 的输出格式. 第一个格式说明符说明 value 1 的输出格式, 第二个格式说明符说明 value 2 的输出格式, 依次类推. 于是, 格式说明符的数量应该和被打印的 value 一样多.这个程序使用 printf 打印每位雇员的报酬: { printf(“total pay for %s is \$%.2f\n”, \$1, \$2 * $3) } 这个 printf 语句的格式字符串包含两个格式说明符. 第一个格式说明符 %s, 是说将第一个值 $1, 以字 8符串的形式打印; 第二个格式说明符 %.2f, 是说将第二个值 $2*$3, 按照数值格式打印, 且带有两位小数. 格式字符串的其他内容 (包括美元符) 按照字面值打印; 字符串末尾的 \n 表示换行符, 该符号使后面的输出从下一行开始. 当 emp.data 作为输入时, 这个程序输出: total pay for Beth is $0.00total pay for Dan is $0.00total pay for Kathy is $40.00total pay for Mark is $100.00total pay for Mary is $121.00total pay for Susie is $76.50 使用 printf 不会自动产生空格符或换行符; 用户必须自己创建它们, 不要忘了 \n.另外一个程序打印每位雇员的名字与报酬:{ printf(“%-8s $%6.2f\n”, $1, \$2 * $3) } 第一个格式说明符 %-8s, 将名字左对齐输出, 占用 8 个字符的宽度. 第二个格式说明符 %6.2f, 将报酬以带有两位小数的数值格式打印出来, 数字至少占用 6 个字符的宽度:Beth $ 0.00Dan $ 0.00Kathy $ 40.00Mark $100.00Mary $121.00Susie $ 76.50更多的关于 printf 的例子会慢慢加以介绍 输出排序设想一下你想要为每一位雇员打印所有的数据, 包括他的报酬, 报酬按照升序排列. 最简单的办法是使用 awk 在每一位雇员的记录前加上报酬, 然后再通过一个排序程序进行排序, 在 Unix 中, 命令行 1234567# awk &apos;&#123; printf(&quot;%6.2f %s\n&quot;, 2 * 3, $0) &#125;&apos; emp.data | sort -n0.00 Beth 4.00 00.00 Dan 3.75 040.00 Kathy 4.00 1076.50 Susie 4.25 18100.00 Mark 5.00 20121.00 Mary 5.50 22 将 awk 的输出通过管道传递给 sort 命令, 最后输出:. 注：The AWK Programming Language学习笔记]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker镜像]]></title>
    <url>%2F2018%2F07%2F27%2Fdocker-docker%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[镜像（image）是Docker三大核心概念中最为重要的，自Docker诞生之日起“镜像”就是相关社区最为热门的关键词。 Docker运行容器前需要本地存在对应的镜像，如果镜像没保存在本地，Docker会尝试从默认镜像仓库下载（默认使用Docker Hub公共注册服务器中的仓库），用户也可以通过配置，使用自定义的镜像仓库。 获取镜像镜像是运行容器的前提，官方的Docker Hub网站已经提供了数十万个镜像供大家开放下载。 可以使用docker pull命令直接从Docker Hub镜像源来下载镜像。改命令的格式为docker pull NAME[:TAG]。其中，NAME是镜像仓库的名称（用来区分镜像），TAG是镜像的标签（往往用来表示版本信息）。通常情况下，描述一个镜像需要包括“名称+标签”信息。 例如，获取一个Ubuntu14.04系统的基础镜像可以使用如下的命令： 1docker pull ubuntu:14.04 对于docker镜像来说，如果不显示指定TAG，则默认会选择latest标签，这会下载仓库中最新版本的镜像。 从稳定性上考虑，不要在生产环境中忽略镜像的标签信息或使用默认的latest标记的镜像。 使用images命令列出镜像123docker images REPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/tomcat latest 08f8166740f8 3 months ago 366.7 MB 使用tag命令添加镜像标签1docker tag docker.io/tomcat:latest mytomcat:latest 使用inspect命令查看详细信息12345678910111213docker inspect docker.io/tomcat:latest [ &#123; &quot;Id&quot;: &quot;sha256:08f8166740f822b79f1306648591c1013105ccb5dca0a15320c54e991e0f9538&quot;, &quot;RepoTags&quot;: [ &quot;docker.io/tomcat:latest&quot; ], &quot;RepoDigests&quot;: [], &quot;Parent&quot;: &quot;&quot;, &quot;Comment&quot;: &quot;&quot;, &quot;Created&quot;: &quot;2017-05-05T23:55:12.275453692Z&quot;, &quot;Container&quot;: &quot;172a4b1f8457373887172ff3f36c15708bbda7430c43963796ffd98f3dd6345e&quot;,..... 返回的是一个JSON格式的信息，如果我们只要其中一项内容时，可以使用参数-f来指定，例如，获取镜像的Os 12docker inspect -f &#123;&#123;&quot;.Architecture&quot;&#125;&#125; docker.io/tomcat:latest amd64 删除镜像使用标签删除镜像 使用docker rmi 命令可以删除镜像，命令格式为dockerrmi IMAGE [IMAGE…],其中IMAGE可以为标签或ID。 如果想强行删除镜像，可以使用-f参数。 创建镜像创建镜像的方法主要有三种：基于已有镜像的容器创建、基于本地模板导入、基于Dockerfile创建。 基于已有镜像的容器创建该方法主要是使用docker commit命令。命令格式为dockercommit [OPTIONS] CONTAINER [REPOSRITORY[:TAG]]，主要信息包括： -a，–author=””：作者信息 - -c，–change=[]：提交时执行Dockerfile指令，包括CMD|ENTERYPOINT|ENV|EXPOSE|LABEL|ONBUILD|USER|VOLUME|WORKDIR等； -m，–message=“”：提交消息； -p ，–pause=true：提交时暂停容器执行。 基于本地模板导入略 搜索镜像使用docker search命令可以搜索远端仓库中共享的镜像，默认搜索官方仓库中的镜像。用法为docker search TERM，支持的参数主要包括 –automated=true|false:仅显示自动创建的镜像，默认为否； –no-trunc=true|false:输出信息不截断显示，默认为否； -s，–stars=X:指定仅显示评价为指定星级以上的镜像，默认为0，即输出所有镜像。 存出和载入镜像用户可以使用docker save和docker load命令来存出和载入镜像。 存出镜像如果要导出镜像到本地，可以使用docker save名利，例如，导出本地的tomcat:latest镜像为tomcat.tart，如下所示： 1docker save -o /images/tomcat.tart tomcat:latest 之后，用户就可以通过复制tomcat.tar文件将改镜像分享给他人。 载入镜像可以使用docker load将导出的tar文件再导入本地镜像库，例如从tomcat.tar 导入镜像到本地镜像列表，如下所示： 12docker load --input tomcat.tar 或docker load &lt; tomcat.tar 这将导入镜像及其相关的元数据信息（包括标签等）。导入成功后，可以使用docker images命令进行查看。 上传镜像可以使用docker push 命令上传镜像到仓库，默认上传到DockerHub官方仓库（需要登录）。命令格式为： docker push NAME[:TAG] 用户在Docker Hub网站注册后可以上传自制的镜像。例如用户user上传本地的test：latest镜像，可以先添加新的标签user/test:latest，然后用docker push命令上传镜像： 12docker tag test:latest user/test:latestdocker push user/test:latest 第一次上传时，会提示输入登录信息或进行注册。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识容器与Docker]]></title>
    <url>%2F2018%2F07%2F27%2Fdocker-%E5%88%9D%E8%AF%86docker%2F</url>
    <content type="text"><![CDATA[如果说主机时代大家拼的是单个服务器物理性能（如CPU主频和内存），那么在云时代，最为看重的则是凭借虚拟化技术所构建的集群处理能力。 伴随着信息技术的飞速发展，虚拟化技术早已经广泛应用到各种关键场景中。从20世纪60年代IBM推出的大型主机虚拟化，到后来以Xen、KVM为代表的虚拟机虚拟化，再到现在以Docker为代表的的容器技术，虚拟化技术自身也在不断进行创新和突破。 什么是DockerDocker开源背景Docker是基于Go语言实现的开源容器项目，诞生于2013年年初，最初发起者是dotCloud公司。 Docker项目已加入了linux基金会，并遵循Apache2.0协议，全部开源代码均在https://github.com/docker/docker上进行维护。在linux基金会最近一次关于“最受欢迎的云计算开源项目”的调查中，Docker仅次于2010年发起的Openstack项目，并仍处于上升趋势。 Docker的构想是要实现“Build，Ship and Run Any App，Anywhere”，即通过对应用的封装（Packaging）、分发（Distribution）、部署（Deployment）、运行（Runtime）生命周期进行管理，达到应用组件“一次封装，到处运行”的目的。 linux容器技术—巨人的肩膀跟着大部分新兴技术的诞生一样，Docker也并非“从石头缝里蹦出来的”，而是站在前人的肩膀上，其中最重要的就是linux容器（linux Containers，LXC）技术。 从Linux容器到Docker简单地讲，可以将Docker容器理解为一种轻量级的沙盒（sandbox）。每个容器内运行着一个应用，不同的容器相互隔离，容器之间也可以通过网络互相通信。容器的创建和停止都十分快速，几乎跟创建和终止原生应用一致；另外，容器自身对系统资源的额外需求也十分有限，远远低于传统虚拟机。很多时候，甚至直接把容器当作应用本身也没有任何问题。 有理由相信，Docker技术会进一步成熟，将会成为更受欢迎的容器虚拟化技术实现，并在云计算和DevOps等领域得到更广泛的应用。 Docker容器虚拟化的好处Docker提供了一种聪明的方式，通过容器来打包应用，解耦应用和运行平台。意味着迁移的时候，只需要在新的服务器上启动需要的容器就可以了，无论新旧服务器是否同一类型的平台。这无疑将节约大量的宝贵时间，并降低部署过程出现问题的风险。 Docker在开发和运维中的优势更快的交付和部署 更高效的资源利用 更轻松的迁移和扩展 更简单的更新管理 Docker与虚拟机的比较 特性 容器 虚拟机 启动速度 秒级 分钟级 性能 接近原生 较弱 内存代价 很小 较多 硬盘使用 一般为MB 一般为GB 运行密度 单机支持上千个容器 一般为几十个 隔离性 安全隔离 完全隔离 迁移性 优秀 一般 Docker与虚拟化]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git详解-git实战]]></title>
    <url>%2F2018%2F07%2F27%2Fgit%E8%AF%A6%E8%A7%A3-git%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[本章介绍开始使用 Git 前的相关知识。我们会先了解一些版本控制工具的历史背景，然后试着让 Git 在你的系统上跑起来，直到最后配置好，可以正常开始开发工作。读完本章，你就会明白为什么 Git 会如此流行，为什么你应该立即开始使用它。 关于版本控制什么是版本控制？我真的需要吗？版本控制是一种记录若干文件内容变化，以便将来查阅特定版本修订情况的系统。在本书所展示的例子中，我们仅对保存着软件源代码的文本文件作版本控制管理，但实际上，你可以对任何类型的文件进行版本控制。 如果你是位图形或网页设计师，可能会需要保存某一幅图片或页面布局文件的所有修订版本（这或许是你非常渴望拥有的功能）。采用版本控制系统 （VCS）是个明智的选择。有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态。你可以比较文件的变化细节，查出最 后是谁修改了哪个地方，从而导致出现怪异问题，又是谁在何时报告了某个功能缺陷等等。使用版本控制系统通常还意味着，就算你乱来一气把整个项目中的文件改 的改删的删，你也照样可以轻松恢复到原先的样子。但额外增加的工作量却微乎其微。 本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单。不过坏处也不少：有时候会混淆所在的工作目录，一旦弄错文件丢了数据就没法撤销恢复。 为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异 其中最流行的一种叫做 rcs，现今许多计算机系统上都还看得到它的踪影。甚至在流行的 Mac OS X 系统上安装了开发者工具包之后，也可以使用 rcs 命令。它的工作原理基本上就是保存并管理文件补丁（patch）。文件补丁是一种特定格式的文本文件，记录着对应文件修订前后的内容变化。所以，根据每次 修订后的补丁，rcs 可以通过不断打补丁，计算出各个版本的文件内容。 集中化的版本控制系统 接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？于是，集中化的版本控制系统（ Centralized Version Control Systems，简称 CVCS ）应运而生。这类系统，诸如 CVS，Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这 已成为版本控制系统的标准做法。 这种做法带来了许多好处，特别是相较于老式的本地 VCS 来说。现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。 事分两面，有好有坏。这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要 是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就还是会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，而被客户端 提取出来的某些快照数据除外，但这样的话依然是个问题，你不能保证所有的数据都已经有人事先完整提取出来过。本地版本控制系统也存在类似问题，只要整个项 目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 分布式版本控制系统 于是分布式版本控制系统（ Distributed Version Control System，简称 DVCS ）面世了。在这类系统中，像 Git，Mercurial，Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把原始的代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜 像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份。 进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。 Git 基础 那么，简单地说，Git 究竟是怎样的一个系统呢？请注意，接下来的内容非常重要，若是理解了 Git 的思想和基本工作原理，用起来就会知其所以然，游刃有余。在开始学习 Git 的时候，请不要尝试把各种概念和其他版本控制系统（诸如 Subversion 和 Perforce 等）相比拟，否则容易混淆每个操作的实际意义。Git 在保存和处理各种信息的时候，虽然操作起来的命令形式非常相近，但它与其他版本控制系统的做法颇为不同。理解这些差异将有助于你准确地使用 Git 提供的各种工具。 直接记录快照，而非差异比较 Git 和其他版本控制系统的主要差别在于，Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。这类系统 （CVS，Subversion，Perforce，Bazaar 等等）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容 Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照 的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。Git 的工作方式如图所示 这是 Git 同其他系统的重要区别。它完全颠覆了传统版本控制的套路，并对各个环节的实现方式作了新的设计。Git 更像是个小型的文件系统，但它同时还提供了许多以此为基础的超强工具，而不只是一个简单的 VCS。稍后在第三章讨论 Git 分支管理的时候，我们会再看看这样的设计究竟会带来哪些好处。 近乎所有操作都是本地执行 在 Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。但如果用 CVCS 的话，差不多所有操作都需要连接网络。因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快。 举个例子，如果要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。所以任何时候你都可以马上翻阅，无需等待。如果想要看当前版本的文件和一个月 前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算，而不用请求远程服务器来做这件事，或是把老版本的文件拉到本地来作比较。 用 CVCS 的话，没有网络或者断开 VPN 你就无法做任何事情。但用 Git 的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程仓库。同样，在回家的路上，不用连接 VPN 你也可以继续工作。换作其他版本控制系统，这么做几乎不可能，抑或非常麻烦。比如 Perforce，如果不连到服务器，几乎什么都做不了（译注：默认无法发出命令p4 edit file 开始编辑文件，因为 Perforce 需要联网通知系统声明该文件正在被谁修订。但实际上手工修改文件权限可以绕过这个限制，只是完成后还是无法提交更新。）；如果是 Subversion 或 CVS，虽然可以编辑文件，但无法提交更新，因为数据库在网络上。看上去好像这些都不是什么大问题，但实际体验过之后，你就会惊喜地发现，这其实是会带来很大不同的。 时刻保持数据完整性 在保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。换句话说，不可能在你修改了文件或目录之后，Git 一无所知。这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致文件数据缺失，Git 都能立即察觉。 Git 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符（0-9 及 a-f）组成，看起来就像是： 124b9da6552252987aa493b52f8696cd6d3b00373 Git 的工作完全依赖于这类指纹字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名。 多数操作仅添加数据 常用的 Git 操作大多仅仅是把数据添加到数据库。因为任何一种不可逆的操作，比如删除数据，都会使回退或重现历史版本变得困难重重。在别的 VCS 中，若还未提交更新，就有可能丢失或者混淆一些修改的内容，但在 Git 里，一旦提交快照之后就完全不用担心丢失数据，特别是养成定期推送到其他仓库的习惯的话。 这种高可靠性令我们的开发工作安心不少，尽管去做各种试验性的尝试好了，再怎样也不会弄丢数据。至于 Git 内部究竟是如何保存和恢复数据的，我们会在第九章讨论 Git 内部原理时再作详述。 文件的三种状态 好，现在请注意，接下来要讲的概念非常重要。对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged）。已提交表示该文件已经被安全地保存在本地数据库 中了；已修改表示修改了某个文件，但还没有提交保存；已暂存表示把已修改的文件放在下次提交时要保存的清单中。 由此我们看到 Git 管理项目时，文件流转的三个工作区域：Git 的工作目录，暂存区域，以及本地仓库。 每个项目都有一个 Git 目录（译注：如果 git clone 出来的话，就是其中 .git 的目录；如果git clone --bare 的话，新建的目录本身就是 Git 目录。），它是 Git 用来保存元数据和对象数据库的地方。该目录非常重要，每次克隆镜像仓库的时候，实际拷贝的就是这个目录里面的数据。 从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录。这些文件实际上都是从 Git 目录中的压缩对象数据库中提取出来的，接下来就可以在工作目录中对这些文件进行编辑。 所谓的暂存区域只不过是个简单的文件，一般都放在 Git 目录中。有时候人们会把这个文件叫做索引文件，不过标准说法还是叫暂存区域。 Git 工作流程如下 1. 在工作目录中修改某些文件。 2. 对修改后的文件进行快照，然后保存到暂存区域。 3. 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。 所以，我们可以从文件所处的位置来判断状态：如果是 Git 目录中保存着的特定版本文件，就属于已提交状态；如果作了修改并已放入暂存区域，就属于已暂存状态；如果自上次取出后，作了修 改但还没有放到暂存区域，就 是已修改状态。到第二章的时候，我们会进一步了解其中细节，并学会如何根据文件状态实施后续操作，以及怎样跳过暂存直接提交。 在正式使用前，我们还需要弄清楚Git的三种重要模式，分别是已提交、已修改、已暂存 已提交(committed):表示数据文件已经顺利提交到Git数据库中。 已修改(modified):表示数据文件已经被修改，但未被保存到Git数据库中。 已暂存(staged):表示数据文件已经被修改，并会在下次提交时提交到Git数据库中。 实战 先创建一个工程的目录mkdir test_project cd test_project git init 初始化git工作目录（git init –bare功能相同） git init的结果（这个隐藏的git目录里面的内容和–bare创建的相同） git init –bare 路径 touch readme 创建一个文件 git status 查看状态 第一次查看，这个文件还没有添加到暂存区的 6.git add readme 将readme文件添加到暂存区 既然有添加，那就有删除（此处说的是暂存区的操作，不会删除文件） git rm –cached readme 7.git status 再次查看暂存区的状态 将readme添加到暂存区后的状态 git commit -m “first commit” 提交到自己的中央仓库（init就是创建自己的中央仓库） git log查看日志（相当与svn的提交日志） 到目前为止自己本地仓库就提交结束了 之后就是提交到远程仓库了 git remote –v 查看本地存储的远程仓库信息，如果是clone出来的工程这个结果如下 origin 表示的是远程仓库的别名（默认为origin，也可以自己起，fetch更新类似于update，push推数据相当于commit） 如果不是clone的工程，就不会有任何结果，要自己添加，命令如下： git remote add test ssh://root@10.0.0.5/usr/GitData/DingDang/.git 11.做完这步然后就是远程推数据了（必须保证本地仓库里面有提交，注意是本地仓库而不是暂存区） git push test 到此自己创建的文件就推到了远程的git仓库了 还有一个功能比较重要，本地仓库的版本回退 git reset –hard HEAD^ #还原历史提交版本上一次 git reset –hard 版本号 #就是上图黄色的部分，仅需要前7位即可 如果回退过头了，log是看不到未来的版本号的，想看可以用git reflog查看 转载链接Git详解之-Git实战]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git分支详解]]></title>
    <url>%2F2018%2F07%2F27%2Fgit%E5%88%86%E6%94%AF%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[​ 几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。在很多版本控制系统中，这是个昂贵的过程，常常需要创建一个源代码目录的完整副本，对大型项目来说会花费很长时间。 有人把 Git 的分支模型称为“必杀技特性”，而正是因为它，将 Git 从版本控制系统家族里区分出来。Git 有何特别之处呢？Git 的分支可谓是难以置信的轻量级，它的新建操作几乎可以在瞬间完成，并且在不同分支间切换起来也差不多一样快。和许多其他版本控制系统不同，Git 鼓励在工作流程中频繁使用分支与合并，哪怕一天之内进行许多次都没有关系。理解分支的概念并熟练运用后，你才会意识到为什么 Git 是一个如此强大而独特的工具，并从此真正改变你的开发方式。 何谓分支 为了理解 Git 分支的实现方式，我们需要回顾一下 Git 是如何储存数据的。或许你还记得第一章的内容，Git 保存的不是文件差异或者变化量，而只是一系列文件快照。 在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次提交的作者等相关附属信息，包含零个或多个指向该提交对 象的父对象指针：首次提交是没有直接祖先的，普通提交有一个祖先，由两个或多个分支合并产生的提交则有多个祖先。 为直观起见，我们假设在工作目录中有三个文件，准备将它们暂存后提交。暂存操作会对每一个文件计算校验和（即第一章中提到的 SHA-1 哈希字串），然后把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 类型的对象存储这些快照），并将校验和加入暂存区域： 121 $ git add README test.rb LICENSE2 $ git commit -m &apos;initial commit of my project&apos; 当使用 git commit 新建一个提交对象前，Git 会先计算每一个子目录（本例中就是项目根目录）的校验和，然后在 Git 仓库中将这些目录保存为树（tree）对象。之后 Git 创建的提交对象，除了包含相关提交信息以外，还包含着指向这个树对象（项目根目录）的指针，如此它就可以在将来需要的时候，重现此次快照的内容了。 现在，Git 仓库中有五个对象：三个表示文件快照内容的 blob 对象；一个记录着目录树内容及其中各个文件对应 blob 对象索引的 tree 对象；以及一个包含指向 tree 对象（根目录）的索引和其他提交信息元数据的 commit 对象。概念上来说，仓库中的各个对象保存的数据和相互关系看起来如图所示： 作些修改后再次提交，那么这次的提交对象会包含一个指向上次提交对象的指针（译注：即下图中的 parent 对象）。两次提交后，仓库历史会变成下图： 现在来谈分支。Git 中的分支，其实本质上仅仅是个指向 commit 对象的可变指针。Git 会使用 master 作为分支的默认名字。在若干次提交后，你其实已经有了一个指向最后一次提交对象的 master 分支，它在每次提交的时候都会自动向前移动。 那么，Git 又是如何创建一个新的分支的呢？答案很简单，创建一个新的分支指针。比如新建一个 testing 分支，可以使用 git branch 命令： 1 $ git branch testing 这会在当前 commit 对象上新建一个分支指针 那么，Git 是如何知道你当前在哪个分支上工作的呢？其实答案也很简单，它保存着一个名为 HEAD 的特别指针。请注意它和你熟知的许多其他版本控制系统（比如 Subversion 或 CVS）里的 HEAD 概念大不相同。在 Git 中，它是一个指向你正在工作中的本地分支的指针（译注：将 HEAD 想象为当前分支的别名。）。运行git branch 命令，仅仅是建立了一个新的分支，但不会自动切换到这个分支中去，所以在这个例子中，我们依然还在 master 分支里工作 要切换到其他分支，可以执行 git checkout 命令。我们现在转换到新建的 testing 分支： 1 $ git checkout testing 这样 HEAD 就指向了 testing 分支 这样的实现方式会给我们带来什么好处呢？好吧，现在不妨再提交一次： 12 $ vim test.rb $ git commit -a -m &apos;made a change&apos; 非常有趣，现在 testing 分支向前移动了一格，而 master 分支仍然指向原先 git checkout 时所在的 commit 对象。现在我们回到 master 分支看看： 1 $ git checkout master 这条命令做了两件事。它把 HEAD 指针移回到 master 分支，并把工作目录中的文件换成了 master 分支所指向的快照内容。也就是说，现在开始所做的改动，将始于本项目中一个较老的版本。它的主要作用是将 testing 分支里作出的修改暂时取消，这样你就可以向另一个方向进行开发。 我们作些修改后再次提交： 12 $ vim test.rb $ git commit -a -m &apos;made other changes&apos; 现在我们的项目提交历史产生了分叉，因为刚才我们创建了一个分支，转换到其中进行了一些工作，然后又回到原来的主分支进行了另外一些工作。这些改变分别孤立在不同的分支里：我们可以 在不同分支里反复切换，并在时机成熟时把它们合并到一起。而所有这些工作，仅仅需要branch 和 checkout 这两条命令就可以完成 由于 Git 中的分支实际上仅是一个包含所指对象校验和（40 个字符长度 SHA-1 字串）的文件，所以创建和销毁一个分支就变得非常廉价。说白了，新建一个分支就是向一个文件写入 41 个字节（外加一个换行符）那么简单，当然也就很快了。 这和大多数版本控制系统形成了鲜明对比，它们管理分支大多采取备份所有项目文件到特定目录的方式，所以根据项目文件数量和大小不同，可能花费的时间 也会有相当大的差别，快则几秒，慢则数分钟。而 Git 的实现与项目复杂度无关，它永远可以在几毫秒的时间内完成分支的创建和切换。同时，因为每次提交时都记录了祖先信息（译注：即parent 对象），将来要合并分支时，寻找恰当的合并基础（译注：即共同祖先）的工作其实已经自然而然地摆在那里了，所以实现起来非常容易。Git 鼓励开发者频繁使用分支，正是因为有着这些特性作保障。 接下来看看，我们为什么应该频繁使用分支。 分支的新建与合并 现在让我们来看一个简单的分支与合并的例子，实际工作中大体也会用到这样的工作流程： 1. 开发某个网站。 2. 为实现某个新的需求，创建一个分支。 3. 在这个分支上开展工作。 假设此时，你突然接到一个电话说有个很严重的问题需要紧急修补，那么可以按照下面的方式处理： 1. 返回到原先已经发布到生产服务器上的分支。 2. 为这次紧急修补建立一个新分支，并在其中修复问题。 3. 通过测试后，回到生产服务器所在的分支，将修补分支合并进来，然后再推送到生产服务器上。 4. 切换到之前实现新需求的分支，继续工作。 分支的新建与切换 首先，我们假设你正在项目中愉快地工作，并且已经提交了几次更新 现在，你决定要修补问题追踪系统上的 #53 问题。顺带说明下，Git 并不同任何特定的问题追踪系统打交道。这里为了说明要解决的问题，才把新建的分支取名为 iss53。要新建并切换到该分支，运行git checkout 并加上 -b 参数： 12 $ git checkout -b iss53 Switched to a new branch &quot;iss53&quot; 这相当于执行下面这两条命令： 12 $ git branch iss53 $ git checkout iss53 接着你开始尝试修复问题，在提交了若干次更新后，iss53 分支的指针也会随着向前推进，因为它就是当前分支（换句话说，当前的 HEAD 指针正指向 iss53）： 12$ vim index.html$ git commit -a -m &apos;added a new footer [issue 53]&apos; 现在你就接到了那个网站问题的紧急电话，需要马上修补。有了 Git ，我们就不需要同时发布这个补丁和 iss53 里作出的修改，也不需要在创建和发布该补丁到服务器之前花费大力气来复原这些修改。唯一需要的仅仅是切换回master 分支。 不过在此之前，留心你的暂存区或者工作目录里，那些还没有提交的修改，它会和你即将检出的分支产生冲突从而阻止 Git 为你切换分支。切换分支的时候最好保持一个清洁的工作区域。稍后会介绍几个绕过这种问题的办法（分别叫做 stashing 和 commit amending）。目前已经提交了所有的修改，所以接下来可以正常转换到master 分支： 12 $ git checkout master Switched to branch &quot;master&quot; 此时工作目录中的内容和你在解决问题 #53 之前一模一样，你可以集中精力进行紧急修补。这一点值得牢记：Git 会把工作目录的内容恢复为检出某分支时它所指向的那个提交对象的快照。它会自动添加、删除和修改文件以确保目录的内容和你当时提交时完全一样。 接下来，你得进行紧急修补。我们创建一个紧急修补分支 hotfix 来开展工作，直到搞定： 123456$ git checkout -b &apos;hotfix&apos;Switched to a new branch &quot;hotfix&quot;$ vim index.html$ git commit -a -m &apos;fixed the broken email address&apos;[hotfix]: created 3a0874c: &quot;fixed the broken email address&quot; 1 files changed, 0 insertions(+), 1 deletions(-) 有必要作些测试，确保修补是成功的，然后回到 master 分支并把它合并进来，然后发布到生产服务器。用 git merge 命令来进行合并： 123456$ git checkout master$ git merge hotfixUpdating f42c576..3a0874cFast forward README | 1 - 1 files changed, 0 insertions(+), 1 deletions(-) 请注意，合并时出现了“Fast forward”的提示。由于当前 master 分支所在的提交对象是要并入的 hotfix 分支的直接上游，Git 只需把master 分支指针直接右移。换句话说，如果顺着一个分支走下去可以到达另一个分支的话，那么 Git 在合并两者时，只会简单地把指针右移，因为这种单线的历史分支不存在任何需要解决的分歧，所以这种合并过程可以称为快进（Fast forward）。 现在最新的修改已经在当前 master 分支所指向的提交对象中了，可以部署到生产服务器上去了 在那个超级重要的修补发布以后，你想要回到被打扰之前的工作。由于当前 hotfix 分支和 master 都指向相同的提交对象，所以hotfix 已经完成了历史使命，可以删掉了。使用 git branch 的 -d 选项执行删除操作： 12 $ git branch -d hotfix Deleted branch hotfix (3a0874c). 现在回到之前未完成的 #53 问题修复分支上继续工作（图 3-15）： 123456$ git checkout iss53Switched to branch &quot;iss53&quot;$ vim index.html$ git commit -a -m &apos;finished the new footer [issue 53]&apos;[iss53]: created ad82d7a: &quot;finished the new footer [issue 53]&quot; 1 files changed, 1 insertions(+), 0 deletions(-) 不用担心之前 hotfix 分支的修改内容尚未包含到 iss53 中来。如果确实需要纳入此次修补，可以用git merge master 把 master 分支合并到 iss53；或者等 iss53 完成之后，再将iss53分支中的更新并入 master。 分支的合并 在问题 #53 相关的工作完成之后，可以合并回 master 分支。实际操作同前面合并 hotfix 分支差不多，只需回到master 分支，运行 git merge 命令指定要合并进来的分支： 12345$ git checkout master$ git merge iss53Merge made by recursive. README | 1 + 1 files changed, 1 insertions(+), 0 deletions(-) 请注意，这次合并操作的底层实现，并不同于之前 hotfix 的并入方式。因为这次你的开发历史是从更早的地方开始分叉的。由于当前 master 分支所指向的提交对象（C4）并不是 iss53 分支的直接祖先，Git 不得不进行一些额外处理。就此例而言，Git 会用两个分支的末端（C4 和 C5）以及它们的共同祖先（C2）进行一次简单的三方合并计算。图 3-16 用红框标出了 Git 用于合并的三个提交对象： 这次，Git 没有简单地把分支指针右移，而是对三方合并后的结果重新做一个新的快照，并自动创建一个指向它的提交对象（C6）。这个提交对象比较特殊，它有两个祖先（C4 和 C5）。 值得一提的是 Git 可以自己裁决哪个共同祖先才是最佳合并基础；这和 CVS 或 Subversion（1.5 以后的版本）不同，它们需要开发者手工指定合并基础。所以此特性让 Git 的合并操作比其他系统都要简单不少。 既然之前的工作成果已经合并到 master 了，那么 iss53 也就没用了。你可以就此删除它，并在问题追踪系统里关闭该问题。 1 $ git branch -d iss53 Checkout 历史版本从某个历史版本创建新的分支在 Git 中从当前分支创建并检出新分支的命令是 1git checkout -b name-of-new-branch 这个命令实际上是 1git checkout -b name-of-new-branch current-branch 的简写形式。也就是说，当我们不指定 checkout 起点时，Git 默认从当前活动分支开始创建新的分支。 Git 的每个提交都有一个 SHA1 散列值（Hash 值）作为 ID。我们可以在 checkout 命令中使用这些 ID 作为起点。比如： 1git checkout -b name-of-new-branch 169d2dc 这样，Git 的活动分支会切换到 name-of-new-branch 这个分支上，而它的内容与 169d2dc 这个分支一致。 注意：SHA1 的散列值有 40 个字母，相当长。所以 Git 允许我们在不引起歧义的情况下，使用散列值的前几位作为缩写。 提示：你也可以用 git branch name-of-new-branch 169d2dc 来创建一个历史分支，而不切换到该分支。 将某个历史版本 checkout 到工作区首先说明，这样做会产生一个分离的 HEAD 指针，所以个人不推荐这么做。 如果我们工作在 master 分支上，希望 checkout 到 dev 分支上，我们会这么做： 1git checkout dev 这里 dev 实际上是一个指针的别名，其本质也是一个 SHA1 散列值。所以，我们很自然地可以用 1git checkout &lt;sha1-of-a-commit&gt; 将某个历史版本 checkout 到工作区。 将某个文件的历史版本 checkout 到工作区大多数时候，我们可能只需要对某一个文件做细小的修补，因此只 checkout 该文件就行了，并不需要操作整个 commit 或分支。 上一节我们介绍了如何将某个历史版本完整地 checkout 到工作区。实际上，我们只需要在上一节的命令之后加上需要 checkout 的文件即可。 1git checkout &lt;sha1-of-a-commit&gt; &lt;/path/to/your/file&gt; 当然，有时候你需要将某个文件的历史版本 checkout 出来，并以一个新的名字保存。这时候可以这么做： 遇到冲突时的分支合并 有时候合并操作并不会如此顺利。如果在不同的分支中都修改了同一个文件的同一部分，Git 就无法干净地把两者合到一起（译注：逻辑上说，这种问题只能由人来裁决。）。如果你在解决问题 #53 的过程中修改了hotfix 中修改的部分，将得到类似下面的结果： 1234$ git merge iss53Auto-merging index.htmlCONFLICT (content): Merge conflict in index.htmlAutomatic merge failed; fix conflicts and then commit the result. Git 作了合并，但没有提交，它会停下来等你解决冲突。要看看哪些文件在合并时发生冲突，可以用 git status 查阅： 123456789101112[master*]$ git statusindex.html: needs merge# On branch master# Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)# (use &quot;git checkout -- ...&quot; to discard changes in working directory)## unmerged: index.html# 任何包含未解决冲突的文件都会以未合并（unmerged）的状态列出。Git 会在有冲突的文件里加入标准的冲突解决标记，可以通过它们来手工定位并解决这些冲突。可以看到此文件包含类似下面这样的部分： 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.htmlcontact : email.support@github.com=======please contact us at support@github.com&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html 可以看到 ======= 隔开的上半部分，是 HEAD（即 master 分支，在运行merge 命令时所切换到的分支）中的内容，下半部分是在 iss53 分支中的内容。解决冲突的办法无非是二者选其一或者由你亲自整合到一起。比如你可以通过把这段内容替换为下面这样来解决： please contact us at email.support@github.com 这个解决方案各采纳了两个分支中的一部分内容，而且我还删除了 &lt;&lt;&lt;&lt;&lt;&lt;&lt;，======= 和 &gt;&gt;&gt;&gt;&gt;&gt;&gt; 这些行。在解决了所有文件里的所有冲突后，运行 git add 将把它们标记为已解决状态（译注：实际上就是来一次快照保存到暂存区域。）。因为一旦暂存，就表示冲突已经解决。如果你想用一个有图形界面的工具来解决这些问题，不妨运行git mergetool，它会调用一个可视化的合并工具并引导你解决所有冲突： 12345678$ git mergetoolmerge tool candidates: kdiff3 tkdiff xxdiff meld gvimdiff opendiff emerge vimdiffMerging the files: index.htmlNormal merge conflict for &apos;index.html&apos;: &#123;local&#125;: modified &#123;remote&#125;: modifiedHit return to start merge resolution tool (opendiff): 如果不想用默认的合并工具（Git 为我默认选择了 opendiff，因为我在 Mac 上运行了该命令），你可以在上方”merge tool candidates”里找到可用的合并工具列表，输入你想用的工具名。我们将在第七章讨论怎样改变环境中的默认值。 退出合并工具以后，Git 会询问你合并是否成功。如果回答是，它会为你把相关文件暂存起来，以表明状态为已解决。 再运行一次 git status 来确认所有冲突都已解决： 1234567891011$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: index.html# 如果觉得满意了，并且确认所有冲突都已解决，也就是进入了暂存区，就可以用 git commit 来完成这次合并提交。提交的记录差不多是这样： 123456789Merge branch &apos;iss53&apos;Conflicts: index.html## It looks like you may be committing a MERGE.# If this is not correct, please remove the file# .git/MERGE_HEAD# and try again. 如果想给将来看这次合并的人一些方便，可以修改该信息，提供更多合并细节。比如你都作了哪些改动，以及这么做的原因。有时候裁决冲突的理由并不直接或明显，有必要略加注解。 分支的管理 到目前为止，你已经学会了如何创建、合并和删除分支。除此之外，我们还需要学习如何管理分支，在日后的常规工作中会经常用到下面介绍的管理命令。 git branch 命令不仅仅能创建和删除分支，如果不加任何参数，它会给出当前所有分支的清单： 1234$ git branch iss53* master testing 注意看 master 分支前的 * 字符：它表示当前所在的分支。也就是说，如果现在提交更新，master 分支将随着开发进度前移。若要查看各个分支最后一个提交对象的信息，运行git branch -v： 1234$ git branch -v iss53 93b412c fix javascript issue* master 7a98805 Merge branch &apos;iss53&apos; testing 782fd34 add scott to the author list in the readmes 要从该清单中筛选出你已经（或尚未）与当前分支合并的分支，可以用 --merge 和 --no-merged选项（Git 1.5.6 以上版本）。比如用git branch --merge 查看哪些分支已被并入当前分支（译注：也就是说哪些分支是当前分支的直接上游。）： 123$ git branch --merged iss53* master 之前我们已经合并了 iss53，所以在这里会看到它。一般来说，列表中没有 * 的分支通常都可以用 git branch -d 来删掉。原因很简单，既然已经把它们所包含的工作整合到了其他分支，删掉也不会损失什么。 另外可以用 git branch --no-merged 查看尚未合并的工作： 12$ git branch --no-merged testing 它会显示还未合并进来的分支。由于这些分支中还包含着尚未合并进来的工作成果，所以简单地用 git branch -d 删除该分支会提示错误，因为那样做会丢失数据： 123$ git branch -d testingerror: The branch &apos;testing&apos; is not an ancestor of your current HEAD.If you are sure you want to delete it, run &apos;git branch -D testing&apos;. 不过，如果你确实想要删除该分支上的改动，可以用大写的删除选项 -D 强制执行，就像上面提示信息中给出的那样。 利用分支进行开发的工作流程 现在我们已经学会了新建分支和合并分支，可以（或应该）用它来做点什么呢？在本节，我们会介绍一些利用分支进行开发的工作流程。而正是由于分支管理的便捷，才衍生出了这类典型的工作模式，你可以根据项目的实际情况选择一种用用看。 长期分支 由于 Git 使用简单的三方合并，所以就算在较长一段时间内，反复多次把某个分支合并到另一分支，也不是什么难事。也就是说，你可以同时拥有多个开放的分支，每个分支用于完成特定的任务，随着开发的推进，你可以随时把某个特性分支的成果并到其他分支中。 许多使用 Git 的开发者都喜欢用这种方式来开展工作，比如仅在 master 分支中保留完全稳定的代码，即已经发布或即将发布的代码。与此同时，他们还有一个名为develop 或 next 的平行分支，专门用于后续的开发，或仅用于稳定性测试 — 当然并不是说一定要绝对稳定，不过一旦进入某种稳定状态，便可以把它合并到master 里。这样，在确保这些已完成的特性分支（短期分支，比如之前的 iss53 分支）能够通过所有测试，并且不会引入更多错误之后，就可以并到主干分支中，等待下一次的发布。 本质上我们刚才谈论的，是随着提交对象不断右移的指针。稳定分支的指针总是在提交历史中落后一大截，而前沿分支总是比较靠前 或者把它们想象成工作流水线，或许更好理解一些，经过测试的提交对象集合被遴选到更稳定的流水线 你可以用这招维护不同层次的稳定性。某些大项目还会有个 proposed（建议）或 pu（proposed updates，建议更新）分支，它包含着那些可能还没有成熟到进入next 或 master 的内容。这么做的目的是拥有不同层次的稳定性：当这些分支进入到更稳定的水平时，再把它们合并到更高层分支中去。再次说明下，使用多个长期分支的做法并非必需，不过一般来说，对于特大型项目或特复杂的项目，这么做确实更容易管理。 特性分支 在任何规模的项目中都可以使用特性（Topic）分支。一个特性分支是指一个短期的，用来实现单一特性或与其相关工作的分支。可能你在以前的版本控 制系统里从未做过类似这样的事情，因为通常创建与合并分支消耗太大。然而在 Git 中，一天之内建立、使用、合并再删除多个分支是常见的事。 我们在上节的例子里已经见过这种用法了。我们创建了 iss53 和 hotfix 这两个特性分支，在提交了若干更新后，把它们合并到主干分支，然后删除。该技术允许你迅速且完全的进行语境切换 — 因为你的工作分散在不同的流水线里，每个分支里的改变都和它的目标特性相关，浏览代码之类的事情因而变得更简单了。你可以把作出的改变保持在特性分支中几 分钟，几天甚至几个月，等它们成熟以后再合并，而不用在乎它们建立的顺序或者进度。 现在我们来看一个实际的例子。请看下图，由下往上，起先我们在 master 工作到 C1，然后开始一个新分支 iss91 尝试修复 91 号缺陷，提交到 C6 的时候，又冒出一个解决该问题的新办法，于是从之前 C4 的地方又分出一个分支iss91v2，干到 C8 的时候，又回到主干 master 中提交了 C9 和 C10，再回到 iss91v2 继续工作，提交 C11，接着，又冒出个不太确定的想法，从 master 的最新提交 C10 处开了个新的分支dumbidea 做些试验。 现在，假定两件事情：我们最终决定使用第二个解决方案，即 iss91v2 中的办法；另外，我们把 dumbidea 分支拿给同事们看了以后，发现它竟然是个天才之作。所以接下来，我们准备抛弃原来的iss91 分支（实际上会丢弃 C5 和 C6），直接在主干中并入另外两个分支。最终的提交历史将变成下图 请务必牢记这些分支全部都是本地分支，这一点很重要。当你在使用分支及合并的时候，一切都是在你自己的 Git 仓库中进行的 — 完全不涉及与服务器的交互。 远程分支 远程分支（remote branch）是对远程仓库中的分支的索引。它们是一些无法移动的本地分支；只有在 Git 进行网络交互时才会更新。远程分支就像是书签，提醒着你上次连接远程仓库时上面各分支的位置。 我们用 (远程仓库名)/(分支名) 这样的形式表示远程分支。比如我们想看看上次同 origin 仓库通讯时master 的样子，就应该查看 origin/master 分支。如果你和同伴一起修复某个问题，但他们先推送了一个iss53 分支到远程仓库，虽然你可能也有一个本地的 iss53 分支，但指向服务器上最新更新的却应该是 origin/iss53 分支。 可能有点乱，我们不妨举例说明。假设你们团队有个地址为 git.ourcompany.com 的 Git 服务器。如果你从这里克隆，Git 会自动为你将此远程仓库命名为origin，并下载其中所有的数据，建立一个指向它的 master 分支的指针，在本地命名为 origin/master，但你无法在本地更改其数据。接着，Git 建立一个属于你自己的本地master 分支，始于 origin 上 master 分支相同的位置，你可以就此开始工作 一次 Git 克隆会建立你自己的本地分支 master 和远程分支 origin/master，它们都指向 origin/master 分支的最后一次提交。 如果你在本地 master 分支做了些改动，与此同时，其他人向 git.ourcompany.com 推送了他们的更新，那么服务器上的master 分支就会向前推进，而于此同时，你在本地的提交历史正朝向不同方向发展。不过只要你不和服务器通讯，你的 origin/master 指针仍然保持原位不会移动 在本地工作的同时有人向远程仓库推送内容会让提交历史开始分流。 可以运行 git fetch origin 来同步远程服务器上的数据到本地。该命令首先找到 origin 是哪个服务器（本例为git.ourcompany.com），从上面获取你尚未拥有的数据，更新你本地的数据库，然后把 origin/master 的指针移到它最新的位置上 为了演示拥有多个远程分支（在不同的远程服务器上）的项目是如何工作的，我们假设你还有另一个仅供你的敏捷开发小组使用的内部服务器 git.team1.ourcompany.com。可以用第二章中提到的git remote add 命令把它加为当前项目的远程分支之一。我们把它命名为 teamone，以便代替原始的 Git 地址 现在你可以用 git fetch teamone 来获取小组服务器上你还没有的数据了。由于当前该服务器上的内容是你 origin 服务器上的子集，Git 不会下载任何数据，而只是简单地创建一个名为teamone/master 的分支，指向 teamone 服务器上 master 分支所在的提交对象31b8e 推送本地分支 要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你的本地分支不会被自动同步到你引入的远程服务器上，除非你明确执行推送操作。换句话说，对于无意分享的分支，你尽管保留为私人分支好了，而只推送那些协同工作要用到的特性分支。 如果你有个叫 serverfix 的分支需要和他人一起开发，可以运行 git push (远程仓库名) (分支名)： 1234567$ git push origin serverfixCounting objects: 20, done.Compressing objects: 100% (14/14), done.Writing objects: 100% (15/15), 1.74 KiB, done.Total 15 (delta 5), reused 0 (delta 0)To git@github.com:schacon/simplegit.git * [new branch] serverfix -&gt; serverfix 这其实有点像条捷径。Git 自动把 serverfix 分支名扩展为 refs/heads/serverfix:refs/heads/serverfix，意为“取出我在本地的 serverfix 分支，推送到远程仓库的 serverfix 分支中去”。我们将在第九章进一步介绍refs/heads/ 部分的细节，不过一般使用的时候都可以省略它。也可以运行 git push origin serverfix:serferfix 来实现相同的效果，它的意思是“上传我本地的 serverfix 分支到远程仓库中去，仍旧称它为 serverfix 分支”。通过此语法，你可以把本地分支推送到某个命名不同的远程分支：若想把远程分支叫作awesomebranch，可以用 git push origin serverfix:awesomebranch 来推送数据。 接下来，当你的协作者再次从服务器上获取数据时，他们将得到一个新的远程分支 origin/serverfix： 1234567$ git fetch originremote: Counting objects: 20, done.remote: Compressing objects: 100% (14/14), done.remote: Total 15 (delta 5), reused 0 (delta 0)Unpacking objects: 100% (15/15), done.From git@github.com:schacon/simplegit * [new branch] serverfix -&gt; origin/serverfix 值得注意的是，在 fetch 操作下载好新的远程分支之后，你仍然无法在本地编辑该远程仓库中的分支。换句话说，在本例中，你不会有一个新的serverfix 分支，有的只是一个你无法移动的 origin/serverfix 指针。 如果要把该内容合并到当前分支，可以运行 git merge origin/serverfix。如果想要一份自己的 serverfix 来开发，可以在远程分支的基础上分化出一个新的分支来： 123$ git checkout -b serverfix origin/serverfixBranch serverfix set up to track remote branch refs/remotes/origin/serverfix.Switched to a new branch &quot;serverfix&quot; 这会切换到新建的 serverfix 本地分支，其内容同远程分支 origin/serverfix 一致，这样你就可以在里面继续开发了。 跟踪远程分支 从远程分支 checkout 出来的本地分支，称为跟踪分支(tracking branch)。跟踪分支是一种和远程分支有直接联系的本地分支。在跟踪分支里输入git push，Git 会自行推断应该向哪个服务器的哪个分支推送数据。反过来，在这些分支里运行 git pull 会获取所有远程索引，并把它们的数据都合并到本地分支中来。 在克隆仓库时，Git 通常会自动创建一个名为 master 的分支来跟踪 origin/master。这正是git push 和 git pull 一开始就能正常工作的原因。当然，你可以随心所欲地设定为其它跟踪分支，比如origin 上除了 master 之外的其它分支。刚才我们已经看到了这样的一个例子：git checkout -b [分支名] [远程名]/[分支名]。如果你有 1.6.2 以上版本的 Git，还可以用--track 选项简化： 123$ git checkout --track origin/serverfixBranch serverfix set up to track remote branch refs/remotes/origin/serverfix.Switched to a new branch &quot;serverfix&quot; 要为本地分支设定不同于远程分支的名字，只需在前个版本的命令里换个名字： 123$ git checkout -b sf origin/serverfixBranch sf set up to track remote branch refs/remotes/origin/serverfix.Switched to a new branch &quot;sf&quot; 现在你的本地分支 sf 会自动向 origin/serverfix 推送和抓取数据了。 删除远程分支如果不再需要某个远程分支了，比如搞定了某个特性并把它合并进了远程的 master 分支（或任何其他存放稳定代码的地方），可以用这个非常无厘头的语法来删除它：git push [远程名] :[分支名]。如果想在服务器上删除serverfix 分支，运行下面的命令： 123$ git push origin :serverfixTo git@github.com:schacon/simplegit.git - [deleted] serverfix 咚！服务器上的分支没了。你最好特别留心这一页，因为你一定会用到那个命令，而且你很可能会忘掉它的语法。有种方便记忆这条命令的方法：记住我们不久前见过的 git push [远程名] [本地分支]:[远程分支] 语法，如果省略 [本地分支]，那就等于是在说“在这里提取空白然后把它变成[远程分支]”。 分支的衍合 把一个分支整合到另一个分支的办法有两种：merge 和 rebase（译注：rebase 的翻译暂定为“衍合”，大家知道就可以了。）。在本章我们会学习什么是衍合，如何使用衍合，为什么衍合操作如此富有魅力，以及我们应该在什么情况下使用衍合。 基本的衍合操作开发进程分叉到两个不同分支，又各自提交了更新。 之前介绍过，最容易的整合分支的方法是 merge 命令，它会把两个分支最新的快照（C3 和 C4）以及二者最新的共同祖先（C2）进行三方合并，合并的结果是产生一个新的提交对象（C5）。 其实，还有另外一个选择：你可以把在 C3 里产生的变化补丁在 C4 的基础上重新打一遍。在 Git 里，这种操作叫做衍合（rebase）。有了 rebase 命令，就可以把在一个分支里提交的改变移到另一个分支里重放一遍。 在上面这个例子中，运行： 1234$ git checkout experiment$ git rebase masterFirst, rewinding head to replay your work on top of it...Applying: added staged command 它的原理是回到两个分支最近的共同祖先，根据当前分支（也就是要进行衍合的分支 experiment）后续的历次提交对象（这里只有一个 C3），生成一系列文件补丁，然后以基底分支（也就是主干分支master）最后一个提交对象（C4）为新的出发点，逐个应用之前准备好的补丁文件，最后会生成一个新的合并提交对象（C3’），从而改写 experiment 的提交历史，使它成为 master 分支的直接下游，如图 现在回到 master 分支，进行一次快进合并 现在的 C3’ 对应的快照，其实和普通的三方合并，即上个例子中的 C5 对应的快照内容一模一样了。虽然最后整合得到的结果没有任何区别，但衍合能产生一个更为整洁的提交历史。如果视察一个衍合过的分支的历史记录，看起来会更 清楚：仿佛所有修改都是在一根线上先后进行的，尽管实际上它们原本是同时并行发生的。 一般我们使用衍合的目的，是想要得到一个能在远程分支上干净应用的补丁 — 比如某些项目你不是维护者，但想帮点忙的话，最好用衍合：先在自己的一个分支里进行开发，当准备向主项目提交补丁的时候，根据最新的origin/master 进行一次衍合操作然后再提交，这样维护者就不需要做任何整合工作（译注：实际上是把解决分支补丁同最新主干代码之间冲突的责任，化转为由提交补丁的人来解决。），只需根据你提供的仓库地址作一次快进合并，或者直接采纳你提交的补丁。 请注意，合并结果中最后一次提交所指向的快照，无论是通过衍合，还是三方合并，都会得到相同的快照内容，只不过提交历史不同罢了。衍合是按照每行的修改次序重演一遍修改，而合并是把最终结果合在一起。 有趣的衍合 衍合也可以放到其他分支进行，并不一定非得根据分化之前的分支。以图 3-31 的历史为例，我们为了给服务器端代码添加一些功能而创建了特性分支 server，然后提交 C3 和 C4。然后又从 C3 的地方再增加一个client 分支来对客户端代码进行一些相应修改，所以提交了 C8 和 C9。最后，又回到 server 分支提交了 C10。 假设在接下来的一次软件发布中，我们决定先把客户端的修改并到主线中，而暂缓并入服务端软件的修改（因为还需要进一步测试）。这个时候，我们就可以把基于 server 分支而非 master 分支的改变（即 C8 和 C9），跳过 server 直接放到master 分支中重演一遍，但这需要用 git rebase 的 --onto 选项指定新的基底分支master： 1$ git rebase --onto master server client 这好比在说：“取出 client 分支，找出 client 分支和 server 分支的共同祖先之后的变化，然后把它们在master 上重演一遍”。是不是有点复杂？不过它的结果如图 3-32 所示，非常酷（译注：虽然 client 里的 C8, C9 在 C3 之后，但这仅表明时间上的先后，而非在 C3 修改的基础上进一步改动，因为server 和 client 这两个分支对应的代码应该是两套文件，虽然这么说不是很严格，但应理解为在 C3 时间点之后，对另外的文件所做的 C8，C9 修改，放到主干重演。）： 现在可以快进 master 分支了： 12$ git checkout master$ git merge client 现在我们决定把 server 分支的变化也包含进来。我们可以直接把 server 分支衍合到 master，而不用手工切换到 server 分支后再执行衍合操作 — git rebase [主分支] [特性分支]命令会先取出特性分支server，然后在主分支 master 上重演： 1$ git rebase master server 于是，server 的进度应用到 master 的基础上， 然后就可以快进主干分支 master 了： 12$ git checkout master$ git merge server 现在 client 和 server 分支的变化都已经集成到主干分支来了，可以删掉它们了。最终我们的提交历史会变成图 3-35 的样子： 12$ git branch -d client$ git branch -d server 衍合的风险 呃，奇妙的衍合也并非完美无缺，要用它得遵守一条准则： 一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行衍合操作。 如果你遵循这条金科玉律，就不会出差错。否则，人民群众会仇恨你，你的朋友和家人也会嘲笑你，唾弃你。 在进行衍合的时候，实际上抛弃了一些现存的提交对象而创造了一些类似但不同的新的提交对象。如果你把原来分支中的提交对象发布出去，并且其他人更新下载后在其基础上开展工作，而稍后你又用git rebase 抛弃这些提交对象，把新的重演后的提交对象发布出去的话，你的合作者就不得不重新合并他们的工作，这样当你再次从他们那里获取内容时，提交历史就会变得一团糟。 下面我们用一个实际例子来说明为什么公开的衍合会带来问题。假设你从一个中央服务器克隆然后在它的基础上搞了一些开发，提交历史 现在，某人在 C1 的基础上做了些改变，并合并他自己的分支得到结果 C6，推送到中央服务器。当你抓取并合并这些数据到你本地的开发分支中后，会得到合并结果 C7，历史提交会变成 接下来，那个推送 C6 上来的人决定用衍合取代之前的合并操作；继而又用 git push --force 覆盖了服务器上的历史，得到 C4’。而之后当你再从服务器上下载最新提交后，会得到： 下载更新后需要合并，但此时衍合产生的提交对象 C4’ 的 SHA-1 校验值和之前 C4 完全不同，所以 Git 会把它们当作新的提交对象处理，而实际上此刻你的提交历史 C7 中早已经包含了 C4 的修改内容，于是合并操作会把 C7 和 C4’ 合并为 C8 C8 这一步的合并是迟早会发生的，因为只有这样你才能和其他协作者提交的内容保持同步。而在 C8 之后，你的提交历史里就会同时包含 C4 和 C4’，两者有着不同的 SHA-1 校验值，如果用git log 查看历史，会看到两个提交拥有相同的作者日期与说明，令人费解。而更糟的是，当你把这样的历史推送到服务器后，会再次把这些衍合后的提交引入到中央服务 器，进一步困扰其他人（译注：这个例子中，出问题的责任方是那个发布了 C6 后又用衍合发布 C4’ 的人，其他人会因此反馈双重历史到共享主干，从而混淆大家的视听。）。 如果把衍合当成一种在推送之前清理提交历史的手段，而且仅仅衍合那些尚未公开的提交对象，就没问题。如果衍合那些已经公开的提交对象，并且已经有人基于这些提交对象开展了后续开发工作的话，就会出现叫人沮丧的麻烦。 小结 读到这里，你应该已经学会了如何创建分支并切换到新分支，在不同分支间转换，合并本地分支，把分支推送到共享服务器上，使用共享分支与他人协作，以及在分享之前进行衍合。 转载链接Git详解三Git分支]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git知识点详解]]></title>
    <url>%2F2018%2F07%2F27%2Fgit%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Git知识点详解 文件状态现在我们手上已经有了一个真实项目的 Git 仓库，并从这个仓库中取出了所有文件的工作拷贝。接下来，对这些文件作些修改，在完成了一个阶段的目标之后，提交本次更新到仓库。 请记住，工作目录下面的所有文件都不外乎这两种状态：已跟踪或未跟踪。已跟踪的文件是指本来就被纳入版本控制管理的文件，在上次快照中有它们的记 录，工作一段时间后，它们的状态可能是未更新，已修改或者已放入暂存区。而所有其他文件都属于未跟踪文件。它们既没有上次更新时的快照，也不在当前的暂存 区域。初次克隆某个仓库时，工作目录中的所有文件都属于已跟踪文件，且状态为未修改。 在编辑过某些文件之后，Git 将这些文件标为已修改。我们逐步把这些修改过的文件放到暂存区域，直到最后一次性提交所有这些暂存起来的文件，如此重复。如下图： 检查当前文件状态要确定哪些文件当前处于什么状态，可以用 git status 命令。如果在克隆仓库之后立即执行此命令，会看到类似这样的输出： 123$ git status# On branch masternothing to commit (working directory clean) 这说明你现在的工作目录相当干净。换句话说，当前没有任何跟踪着的文件，也没有任何文件在上次提交后更改过。此外，上面的信息还表明，当前目录下没 有出现任何处于未跟踪的新文件，否则 Git 会在这里列出来。最后，该命令还显示了当前所在的分支是 master，这是默认的分支名称，实际是可以修改的，现在先不用考虑。下一章我们就会详细讨论分支和引用。 现在让我们用 vim 编辑一个新文件 README，保存退出后运行 git status 会看到该文件出现在未跟踪文件列表中： 12345678$ vim README$ git status# On branch master# Untracked files:# (use &quot;git add ...&quot; to include in what will be committed)## READMEnothing added to commit but untracked files present (use &quot;git add&quot; to track) 就是在“Untracked files”这行下面。Git 不会自动将之纳入跟踪范围，除非你明明白白地告诉它“我需要跟踪该文件”，因而不用担心把临时文件什么的也归入版本管理。不过现在的例子中，我们确实想要跟踪管理 README 这个文件。 跟踪新文件 使用命令 git add 开始跟踪一个新文件。所以，要跟踪 README 文件，运行： 1$ git add README 此时再运行 git status 命令，会看到 README 文件已被跟踪，并处于暂存状态： 1234567$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# 只要在 “Changes to be committed” 这行下面的，就说明是已暂存状态。如果此时提交，那么该文件此时此刻的版本将被留存在历史记录中。你可能会想起之前我们使用git init 后就运行了 git add 命令，开始跟踪当前目录下的文件。在 git add 后面可以指明要跟踪的文件或目录路径。如果是目录的话，就说明要递归跟踪该目录下的所有文件。（译注：其实git add 的潜台词就是把目标文件快照放入暂存区域，也就是 add file into staged area，同时未曾跟踪过的文件标记为需要跟踪。这样就好理解后续 add 操作的实际意义了。） 暂存已修改文件 现在我们修改下之前已跟踪过的文件 benchmarks.rb，然后再次运行 status 命令，会看到这样的状态报告： 123456789101112$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)## modified: benchmarks.rb# 文件 benchmarks.rb 出现在 “Changed but not updated” 这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。要暂存这次更新，需要运行git add 命令（这是个多功能命令，根据目标文件的状态不同，此命令的效果也不同：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等）。现在让我们运行git add 将 benchmarks.rb 放到暂存区，然后再看看 git status 的输出： 123456789$ git add benchmarks.rb$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb# 现在两个文件都已暂存，下次提交时就会一并记录到仓库。假设此时，你想要在 benchmarks.rb 里再加条注释，重新编辑存盘后，准备好提交。不过且慢，再运行git status 看看： 1234567891011121314$ vim benchmarks.rb $ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)## modified: benchmarks.rb# 怎么回事？benchmarks.rb 文件出现了两次！一次算未暂存，一次算已暂存，这怎么可能呢？好吧，实际上 Git 只不过暂存了你运行 git add 命令时的版本，如果现在提交，那么提交的是添加注释前的版本，而非当前工作目录中的版本。所以，运行了git add 之后又作了修订的文件，需要重新运行 git add 把最新版本重新暂存起来： 123456789$ git add benchmarks.rb$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb# 忽略某些文件一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。来看一个实际的例子： 123$ cat .gitignore*.[oa]*~ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的，我们用不着跟踪它们的版本。第二行告诉 Git 忽略所有以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如[0-9] 表示匹配所有 0 到 9 的数字）。 我们再看一个 .gitignore 文件的例子： 123456# 此为注释 – 将被 Git 忽略*.a # 忽略所有 .a 结尾的文件!lib.a # 但 lib.a 除外/TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODObuild/ # 忽略 build/ 目录下的所有文件doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 查看已暂存和未暂存的更新实际上 git status 的显示比较简单，仅仅是列出了修改过的文件，如果要查看具体修改了什么地方，可以用 git diff 命令。稍后我们会详细介绍git diff，不过现在，它已经能回答我们的两个问题了：当前做的哪些更新还没有暂存？有哪些更新已经暂存起来准备好了下次提交？ git diff 会使用文件补丁的格式显示具体添加和删除的行。 假如再次修改 README 文件后暂存，然后编辑 benchmarks.rb 文件后先别暂存，运行 status命令，会看到： 123456789101112$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)## modified: benchmarks.rb# 要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff： 12345678910111213141516$ git diffdiff --git a/benchmarks.rb b/benchmarks.rbindex 3cb747f..da65585 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -36,6 +36,10 @@ def main @commit.parents[0].parents[0].parents[0] end+ run_code(x, &apos;commits 1&apos;) do+ git.commits.size+ end+ run_code(x, &apos;commits 2&apos;) do log = git.commits(&apos;master&apos;, 15) log.size 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容。 若要看已经暂存起来的文件和上次提交时的快照之间的差异，可以用 git diff --cached 命令。（Git 1.6.1 及更高版本还允许使用git diff --staged，效果是相同的，但更好记些。）来看看实际的效果： 123456789101112$ git diff --cacheddiff --git a/README b/READMEnew file mode 100644index 0000000..03902a1--- /dev/null+++ b/README2@@ -0,0 +1,5 @@+grit+ by Tom Preston-Werner, Chris Wanstrath+ http://github.com/mojombo/grit++Grit is a Ruby library for extracting information from a Git repository 请注意，单单 git diff 不过是显示还没有暂存起来的改动，而不是这次工作和上次提交之间的差异。所以有时候你一下子暂存了所有更新过的文件后，运行git diff 后却什么也没有，就是这个原因。 像之前说的，暂存 benchmarks.rb 后再编辑，运行 git status 会看到暂存前后的两个版本： 12345678910111213$ git add benchmarks.rb$ echo &apos;# test line&apos; &gt;&gt; benchmarks.rb$ git status# On branch master## Changes to be committed:## modified: benchmarks.rb## Changed but not updated:## modified: benchmarks.rb# 现在运行 git diff 看暂存前后的变化： 12345678910$ git diffdiff --git a/benchmarks.rb b/benchmarks.rbindex e445e28..86b2f7c 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -127,3 +127,4 @@ end main() ##pp Grit::GitRuby.cache_client.stats+# test line 然后用 git diff --cached 查看已经暂存起来的变化： 12345678910111213141516$ git diff --cacheddiff --git a/benchmarks.rb b/benchmarks.rbindex 3cb747f..e445e28 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -36,6 +36,10 @@ def main @commit.parents[0].parents[0].parents[0] end+ run_code(x, &apos;commits 1&apos;) do+ git.commits.size+ end+ run_code(x, &apos;commits 2&apos;) do log = git.commits(&apos;master&apos;, 15) log.size 提交更新现在的暂存区域已经准备妥当可以提交了。在此之前，请一定要确认还有什么修改过的或新建的文件还没有 git add 过，否则提交的时候不会记录这些还没暂存起来的变化。所以，每次准备提交前，先用git status 看下，是不是都已暂存起来了，然后再运行提交命令 git commit： 1$ git commit 这种方式会启动文本编辑器以便输入本次提交的说明。（默认会启用 shell 的环境变量 $EDITOR 所指定的软件，一般都是 vim 或 emacs。当然也可以按照第一章介绍的方式，使用git config --global core.editor 命令设定你喜欢的编辑软件。） 编辑器会显示类似下面的文本信息（本例选用 Vim 的屏显方式展示）： 123456789101112# Please enter the commit message for your changes. Lines starting# with &apos;#&apos; will be ignored, and an empty message aborts the commit.# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb~~~&quot;.git/COMMIT_EDITMSG&quot; 10L, 283C 可以看到，默认的提交消息包含最后一次运行 git status 的输出，放在注释行里，另外开头还有一空行，供你输入提交说明。你完全可以去掉这些注释行，不过留着也没关系，多少能帮你回想起这次更新的内容有哪些。（如果觉得这还不够，可以用-v 选项将修改差异的每一行都包含到注释中来。）退出编辑器时，Git 会丢掉注释行，将说明内容和本次更新提交到仓库。 另外也可以用 -m 参数后跟提交说明的方式，在一行命令中提交更新： 1234$ git commit -m &quot;Story 182: Fix benchmarks for speed&quot;[master]: created 463dc4f: &quot;Fix benchmarks for speed&quot; 2 files changed, 3 insertions(+), 0 deletions(-) create mode 100644 README 好，现在你已经创建了第一个提交！可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整 SHA-1 校验和是什么（463dc4f），以及在本次提交中，有多少文件修订过，多少行添改和删改过。 记住，提交时记录的是放在暂存区域的快照，任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 跳过使用暂存区域尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。Git 提供了一个跳过使用暂存区域的方式，只要在提交的时候，给 git commit 加上-a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤： 看到了吗？提交之前不再需要 git add 文件 benchmarks.rb 了。 移除文件要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 如果只是简单地从工作目录中手工删除文件，运行 git status 12345678910$ git status# On branch master## Changed but not updated:## modified: benchmarks.rb#$ git commit -a -m &apos;added new benchmarks&apos;[master 83e38c7] added new benchmarks 1 files changed, 5 insertions(+), 0 deletions(-) 时就会在 “Changed but not updated” 部分（也就是_未暂存_清单）看到： 123456789$ rm grit.gemspec$ git status# On branch master## Changed but not updated:# (use &quot;git add/rm ...&quot; to update what will be committed)## deleted: grit.gemspec# 然后再运行 git rm 记录此次移除文件的操作： 12345678910$ git rm grit.gemspecrm &apos;grit.gemspec&apos;$ git status# On branch master## Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## deleted: grit.gemspec# 最后提交的时候，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（译注：即 force 的首字母），以防误删除文件后丢失修改的内容。 另外一种情况是，我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。换句话说，仅是从跟踪清单中删除。比如一些大型日志文件或者一堆.a 编译文件，不小心纳入仓库后，要移除跟踪但不删除文件，以便稍后在 .gitignore 文件中补上，用 --cached 选项即可： 1$ git rm --cached readme.txt 后面可以列出文件或者目录的名字，也可以使用 glob 模式。比方说： 1$ git rm log/\*.log 注意到星号 * 之前的反斜杠 \，因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开（译注：实际上不加反斜杠也可以运行，只不过按照 shell 扩展的话，仅仅删除指定目录下的文件而不会递归匹配。上面的例子本来就指定了目录，所以效果等同，但下面的例子就会用递归方式匹配，所以必须加反斜 杠。）。此命令删除所有log/ 目录下扩展名为 .log 的文件。类似的比如： 1$ git rm \*~ 会递归删除当前目录及其子目录中所有 ~ 结尾的文件。 移动文件不像其他的 VCS 系统，Git 并不跟踪文件移动操作。如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。不过 Git 非常聪明，它会推断出究竟发生了什么，至于具体是如何做到的，我们稍后再谈。 既然如此，当你看到 Git 的 mv 命令时一定会困惑不已。要在 Git 中对文件改名，可以这么做： 1$ git mv file_from file_to 它会恰如预期般正常工作。实际上，即便此时查看状态信息，也会明白无误地看到关于重命名操作的说明： 12345678910$ git mv README.txt README$ git status# On branch master# Your branch is ahead of &apos;origin/master&apos; by 1 commit.## Changes to be committed:# (use &quot;git reset HEAD..&quot; to unstage)## renamed: README.txt -&gt; README# 其实，运行 git mv 就相当于运行了下面三条命令： 123$ mv README.txt README$ git rm README.txt$ git add README 如此分开操作，Git 也会意识到这是一次改名，所以不管何种方式都一样。当然，直接用 git mv轻便得多，不过有时候用其他工具批处理改名的话，要记得在提交前删除老的文件名，再添加新的文件名。 查看提交历史​ 在提交了若干更新之后，又或者克隆了某个项目，想回顾下提交历史，可以使用 git log 命令查看。 接下来的例子会用我专门用于演示的 simplegit 项目，运行下面的命令获取该项目源代码： 1git clone git://github.com/schacon/simplegit-progit.git 然后在此项目中运行 git log，应该会看到下面的输出： 123456789101112131415161718192021222324252627$ git logcommit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version numbercommit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test codecommit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sat Mar 15 10:31:28 2008 -0700 first commit 默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。看到了吗，每次更新都有一个 SHA-1 校验和、作者的名字和电子邮件地址、提交时间，最后缩进一个段落显示提交说明。 git log 有许多选项可以帮助你搜寻感兴趣的提交，接下来我们介绍些最常用的。 我们常用 -p 选项展开显示每次提交的内容差异，用 -2 则仅显示最近的两次更新： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687$ git log -p -2commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version numberdiff --git a/Rakefile b/Rakefileindex a874b73..8f94139 100644--- a/Rakefile+++ b/Rakefile@@ -5,7 +5,7 @@ require &apos;rake/gempackagetask&apos; spec = Gem::Specification.new do |s|- s.version = &quot;0.1.0&quot;+ s.version = &quot;0.1.1&quot; s.author = &quot;Scott Chacon&quot;commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test codediff --git a/lib/simplegit.rb b/lib/simplegit.rbindex a0a60ae..47c6340 100644--- a/lib/simplegit.rb+++ b/lib/simplegit.rb@@ -18,8 +18,3 @@ class SimpleGit end end--if $0 == __FILE__- git = SimpleGit.new- puts git.show-end\ No newline at end of file在做代码审查，或者要快速浏览其他协作者提交的更新都作了哪些改动时，就可以用这个选项。此外，还有许多摘要选项可以用，比如 --stat，仅显示简要的增改行数统计：$ git log --stat commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number Rakefile | 2 +- 1 files changed, 1 insertions(+), 1 deletions(-)commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test code lib/simplegit.rb | 5 ----- 1 files changed, 0 insertions(+), 5 deletions(-)commit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 10:31:28 2008 -0700 first commit README | 6 ++++++ Rakefile | 23 +++++++++++++++++++++++ lib/simplegit.rb | 25 +++++++++++++++++++++++++ 3 files changed, 54 insertions(+), 0 deletions(-) ​ 每个提交都列出了修改过的文件，以及其中添加和移除的行数，并在最后列出所有增减行数小计。还有个常用的 --pretty 选项，可以指定使用完全不同于默认格式的方式展示提交历史。比如用oneline 将每个提交放在一行显示，这在提交数很大时非常有用。另外还有 short，full 和fuller 可以用，展示的信息或多或少有些不同，请自己动手实践一下看看效果如何。 但最有意思的是 format，可以定制要显示的记录格式，这样的输出便于后期编程提取分析，像这样： 12345678$ git log --pretty=format:&quot;%h - %an, %ar : %s&quot;$ git log --pretty=onelineca82a6dff817ec66f44342007202690a93763949 changed the version number085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary test codea11bef06a3f659402fe7563abf99ad00de2209e6 first commitca82a6d - Scott Chacon, 11 months ago : changed the version number085bb3b - Scott Chacon, 11 months ago : removed unnecessary test codea11bef0 - Scott Chacon, 11 months ago : first commit 表 列出了常用的格式占位符写法及其代表的意义。 1 你一定奇怪_作者（author）_和_提交者（committer）_之间究竟有何差别，其实作者指的是实际作出修改的人，提交者指的是最后将此 工作成果提交到仓库的人。所以，当你为某个项目发布补丁，然后某个核心成员将你的补丁并入项目时，你就是作者，而那个核心成员就是提交者。我们会在第五章 再详细介绍两者之间的细微差别。 用 oneline 或 format 时结合 --graph 选项，可以看到开头多出一些 ASCII 字符串表示的简单图形，形象地展示了每个提交所在的分支及其分化衍合情况。在我们之前提到的 Grit 项目仓库中可以看到： 1234567891011$ git log --pretty=format:&quot;%h %s&quot; --graph* 2d3acf9 ignore errors from SIGCHLD on trap* 5e3ee11 Merge branch &apos;master&apos; of git://github.com/dustin/grit|\| * 420eac9 Added a method for getting the current branch.* | 30e367c timeout code and tests* | 5a09431 add timeout protection to grit* | e1193f8 support for heads with slashes in them|/* d6016bc require time for xmlschema* 11d191e Merge b 12345678910111213141516选项 说明%H 提交对象（commit）的完整哈希字串%h 提交对象的简短哈希字串%T 树对象（tree）的完整哈希字串%t 树对象的简短哈希字串%P 父对象（parent）的完整哈希字串%p 父对象的简短哈希字串%an 作者（author）的名字%ae 作者的电子邮件地址%ad 作者修订日期（可以用 -date= 选项定制格式）%ar 作者修订日期，按多久以前的方式显示%cn 提交者(committer)的名字%ce 提交者的电子邮件地址%cd 提交日期%cr 提交日期，按多久以前的方式显示%s 提交说明 1ranch &apos;defunkt&apos; into local 以上只是简单介绍了一些 git log 命令支持的选项。表 2-2 还列出了一些其他常用的选项及其释义。 123456789-p 按补丁格式显示每个更新之间的差异。--stat 显示每次更新的文件修改统计信息。--shortstat 只显示 --stat 中最后的行数修改添加移除统计。--name-only 仅在提交信息后显示已修改的文件清单。--name-status 显示新增、修改、删除的文件清单。--abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。--relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。--graph 显示 ASCII 图形表示的分支合并历史。--pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式） 限制输出长度除了定制输出格式的选项之外，git log 还有许多非常实用的限制输出长度的选项，也就是只输出部分提交信息。之前我们已经看到过 -2 了，它只显示最近的两条提交，实际上，这是 -选项的写法，其中的 n 可以是任何自然数，表示仅显示最近的若干条提交。不过实践中我们是不太用这个选项的，Git 在输出所有提交时会自动调用分页程序（less），要看更早的更新只需翻到下页即可。 另外还有按照时间作限制的选项，比如 --since 和 --until。下面的命令列出所有最近两周内的提交： 1$ git log --since=2.weeks 你可以给出各种时间格式，比如说具体的某一天（“2008-01-15”），或者是多久以前（“2 years 1 day 3 minutes ago”）。 还可以给出若干搜索条件，列出符合的提交。用 --author 选项显示指定作者的提交，用 --grep选项搜索提交说明中的关键字。（请注意，如果要得到同时满足这两个选项搜索条件的提交，就必须用--all-match 选项。） 如果只关心某些文件或者目录的历史提交，可以在 git log 选项的最后指定它们的路径。因为是放在最后位置上的选项，所以用两个短划线（--）隔开之前的选项和后面限定的路径名。 表 2-3 还列出了其他常用的类似选项。 123456选项 说明-(n) 仅显示最近的 n 条提交--since, --after 仅显示指定时间之后的提交。--until, --before 仅显示指定时间之前的提交。--author 仅显示指定作者相关的提交。--committer 仅显示指定提交者相关的提交。 来看一个实际的例子，如果要查看 Git 仓库中，2008 年 10 月期间，Junio Hamano 提交的但未合并的测试脚本（位于项目的 t/ 目录下的文件），可以用下面的查询命令： 12345678$ git log --pretty=&quot;%h - %s&quot; --author=gitster --since=&quot;2008-10-01&quot; \ --before=&quot;2008-11-01&quot; --no-merges -- t/5610e3b - Fix testcase failure when extended attributeacd3b9e - Enhance hold_lock_file_for_&#123;update,append&#125;()f563754 - demonstrate breakage of detached checkout wid1a43f2 - reset --hard/read-tree --reset -u: remove un51a94af - Fix &quot;checkout --track -b newbranch&quot; on detacb0ad11e - pull: allow &quot;git pull origin $something:$cur Git 项目有 20,000 多条提交，但我们给出搜索选项后，仅列出了其中满足条件的 6 条。 使用图形化工具查阅提交历史有时候图形化工具更容易展示历史提交的变化，随 Git 一同发布的 gitk 就是这样一种工具。它是用 Tcl/Tk 写成的，基本上相当于 git log 命令的可视化版本，凡是git log 可以用的选项也都能用在 gitk 上。在项目工作目录中输入 gitk 命令后，就会启动 上半个窗口显示的是历次提交的分支祖先图谱，下半个窗口显示当前点选的提交对应的具体差异。 撤销操作任何时候，你都有可能需要撤消刚才所做的某些操作。接下来，我们会介绍一些基本的撤消操作相关的命令。请注意，有些操作并不总是可以撤消的，所以请务必谨慎小心，一旦失误，就有可能丢失部分工作成果。 修改最后一次提交有时候我们提交完了才发现漏掉了几个文件没有加，或者提交信息写错了。想要撤消刚才的提交操作，可以使用 --amend 选项重新提交： 1$ git commit --amend 此命令将使用当前的暂存区域快照提交。如果刚才提交完没有作任何改动，直接运行此命令的话，相当于有机会重新编辑提交说明，但将要提交的文件快照和之前的一样。 启动文本编辑器后，会看到上次提交时的说明，编辑它确认没问题后保存退出，就会使用新的提交说明覆盖刚才失误的提交。 如果刚才提交时忘了暂存某些修改，可以先补上暂存操作，然后再运行 --amend 提交： 123$ git commit -m &apos;initial commit&apos;$ git add forgotten_file$ git commit --amend 上面的三条命令最终只是产生一个提交，第二个提交命令修正了第一个的提交内容。 取消已经暂存的文件接下来的两个小节将演示如何取消暂存区域中的文件，以及如何取消工作目录中已修改的文件。不用担心，查看文件状态的时候就提示了该如何撤消，所以不需要死记硬背。来看下面的例子，有两个修改过的文件，我们想要分开提交，但不小心用git add . 全加到了暂存区域。该如何撤消暂存其中的一个文件呢？其实，git status 的命令输出已经告诉了我们该怎么做： 123456789$ git add .$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: README.txt# modified: benchmarks.rb# 就在 “Changes to be committed” 下面，括号中有提示，可以使用 git reset HEAD ...的方式取消暂存。好吧，我们来试试取消暂存 benchmarks.rb 文件： 123456789101112131415$ git reset HEAD benchmarks.rbbenchmarks.rb: locally modified$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: README.txt## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)# (use &quot;git checkout -- ...&quot; to discard changes in working directory)## modified: benchmarks.rb# 这条命令看起来有些古怪，先别管，能用就行。现在 benchmarks.rb 文件又回到了之前已修改未暂存的状态。 取消对文件的修改如果觉得刚才对 benchmarks.rb 的修改完全没有必要，该如何取消修改，回到之前的状态（也就是修改之前的版本）呢？git status 同样提示了具体的撤消方法，接着上面的例子，现在未暂存区域看起来像这样： 123456# Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)# (use &quot;git checkout -- ...&quot; to discard changes in working directory)## modified: benchmarks.rb# 在第二个括号中，我们看到了抛弃文件修改的命令（至少在 Git 1.6.1 以及更高版本中会这样提示，如果你还在用老版本，我们强烈建议你升级，以获取最佳的用户体验），让我们试试看： 12345678$ git checkout -- benchmarks.rb$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: README.txt# 可以看到，该文件已经恢复到修改前的版本。你可能已经意识到了，这条命令有些危险，所有对文件的修改都没有了，因为我们刚刚把之前版本的文件复制过 来重写了此文件。所以在用这条命令前，请务必确定真的不再需要保留刚才的修改。如果只是想回退版本，同时保留刚才的修改以便将来继续工作，可以用下章介绍 的 stashing 和分支来处理，应该会更好些。 记住，任何已经提交到 Git 的都可以被恢复。即便在已经删除的分支中的提交，或者用 --amend重新改写的提交，都可以被恢复（关于数据恢复的内容见第九章）。所以，你可能失去的数据，仅限于没有提交过的，对 Git 来说它们就像从未存在过一样。 远程仓库的使用 要参与任何一个 Git 项目的协作，必须要了解该如何管理远程仓库。远程仓库是指托管在网络上的项目仓库，可能会有好多个，其中有些你只能读，另外有些可以写。同他人协作开发某 个项目时，需要管理这些远程仓库，以便推送或拉取数据，分享各自的工作进展。管理远程仓库的工作，包括添加远程库，移除废弃的远程库，管理各式远程库分 支，定义是否跟踪这些分支，等等。本节我们将详细讨论远程库的管理和使用。 查看当前的远程库 要查看当前配置有哪些远程仓库，可以用 git remote 命令，它会列出每个远程库的简短名字。在克隆完某个项目后，至少可以看到一个名为 origin 的远程库，Git 默认使用这个名字来标识你所克隆的原始仓库： 12345678910$ git clone git://github.com/schacon/ticgit.gitInitialized empty Git repository in /private/tmp/ticgit/.git/remote: Counting objects: 595, done.remote: Compressing objects: 100% (269/269), done.remote: Total 595 (delta 255), reused 589 (delta 253)Receiving objects: 100% (595/595), 73.31 KiB | 1 KiB/s, done.Resolving deltas: 100% (255/255), done.$ cd ticgit$ git remoteorigin 也可以加上 -v 选项（译注：此为 --verbose 的简写，取首字母），显示对应的克隆地址： 12$ git remote -vorigin git://github.com/schacon/ticgit.git 如果有多个远程仓库，此命令将全部列出。比如在我的 Grit 项目中，可以看到： 1234567$ cd grit$ git remote -vbakkdoor git://github.com/bakkdoor/grit.gitcho45 git://github.com/cho45/grit.gitdefunkt git://github.com/defunkt/grit.gitkoke git://github.com/koke/grit.gitorigin git@github.com:mojombo/grit.git 这样一来，我就可以非常轻松地从这些用户的仓库中，拉取他们的提交到本地。请注意，上面列出的地址只有 origin 用的是 SSH URL 链接，所以也只有这个仓库我能推送数据上去（我们会在第四章解释原因）。 添加远程仓库 要添加一个新的远程仓库，可以指定一个简单的名字，以便将来引用，运行 git remote add [shortname] [url]： 123456$ git remoteorigin$ git remote add pb git://github.com/paulboone/ticgit.git$ git remote -vorigin git://github.com/schacon/ticgit.gitpb git://github.com/paulboone/ticgit.git 现在可以用字串 pb 指代对应的仓库地址了。比如说，要抓取所有 Paul 有的，但本地仓库没有的信息，可以运行 git fetch pb： 12345678$ git fetch pbremote: Counting objects: 58, done.remote: Compressing objects: 100% (41/41), done.remote: Total 44 (delta 24), reused 1 (delta 0)Unpacking objects: 100% (44/44), done.From git://github.com/paulboone/ticgit * [new branch] master -&gt; pb/master * [new branch] ticgit -&gt; pb/ticgit 现在，Paul 的主干分支（master）已经完全可以在本地访问了，对应的名字是 pb/master，你可以将它合并到自己的某个分支，或者切换到这个分支，看看有些什么有趣的更新。 从远程仓库抓取数据正如之前所看到的，可以用下面的命令从远程仓库抓取数据到本地： 1$ git fetch [remote-name] 此命令会到远程仓库中拉取所有你本地仓库中还没有的数据。运行完成后，你就可以在本地访问该远程仓库中的所有分支，将其中某个分支合并到本地，或者只是取出某个分支，一探究竟。（我们会在第三章详细讨论关于分支的概念和操作。） 如果是克隆了一个仓库，此命令会自动将远程仓库归于 origin 名下。所以，git fetch origin 会抓取从你上次克隆以来别人上传到此远程仓库中的所有更新（或是上次 fetch 以来别人提交的更新）。有一点很重要，需要记住，fetch 命令只是将远端的数据拉到本地仓库，并不自动合并到当前工作分支，只有当你确实准备好了，才能手工合并。 如果设置了某个分支用于跟踪某个远端仓库的分支（参见下节及第三章的内容），可以使用 git pull 命令自动抓取数据下来，然后将远端分支自动合并到本地仓库中当前分支。在日常工作中我们经常这么用，既快且好。实际上，默认情况下git clone 命令本质上就是自动创建了本地的 master 分支用于跟踪远程仓库中的 master 分支（假设远程仓库确实有 master 分支）。所以一般我们运行git pull，目的都是要从原始克隆的远端仓库中抓取数据后，合并到工作目录中的当前分支。 推送数据到远程仓库项目进行到一个阶段，要同别人分享目前的成果，可以将本地仓库中的数据推送到远程仓库。实现这个任务的命令很简单： git push [remote-name] [branch-name]。如果要把本地的 master 分支推送到origin 服务器上（再次说明下，克隆操作会自动使用默认的 master 和 origin 名字），可以运行下面的命令： 1$ git push origin master 只有在所克隆的服务器上有写权限，或者同一时刻没有其他人在推数据，这条命令才会如期完成任务。如果在你推数据前，已经有其他人推送了若干更新，那 你的推送操作就会被驳回。你必须先把他们的更新抓取到本地，合并到自己的项目中，然后才可以再次推送。有关推送数据到远程仓库的详细内容见第三章。 查看远程仓库信息我们可以通过命令 git remote show [remote-name] 查看某个远程仓库的详细信息，比如要看所克隆的 origin 仓库，可以运行： 12345678$ git remote show origin* remote origin URL: git://github.com/schacon/ticgit.git Remote branch merged with &apos;git pull&apos; while on branch master master Tracked remote branches master ticgit 除了对应的克隆地址外，它还给出了许多额外的信息。它友善地告诉你如果是在 master 分支，就可以用 git pull 命令抓取数据合并到本地。另外还列出了所有处于跟踪状态中的远端分支。 上面的例子非常简单，而随着使用 Git 的深入，git remote show 给出的信息可能会像这样： 123456789101112131415161718192021$ git remote show origin* remote origin URL: git@github.com:defunkt/github.git Remote branch merged with &apos;git pull&apos; while on branch issues issues Remote branch merged with &apos;git pull&apos; while on branch master master New remote branches (next fetch will store in remotes/origin) caching Stale tracking branches (use &apos;git remote prune&apos;) libwalker walker2 Tracked remote branches acl apiv2 dashboard2 issues master postgres Local branch pushed with &apos;git push&apos; master:master 它告诉我们，运行 git push 时缺省推送的分支是什么（译注：最后两行）。它还显示了有哪些远端分支还没有同步到本地（译注：第六行的caching 分支），哪些已同步到本地的远端分支在远端服务器上已被删除（译注：Stale tracking branches 下面的两个分支），以及运行git pull 时将自动合并哪些分支（译注：前四行中列出的 issues 和 master 分支）。 远程仓库的删除和重命名在新版 Git 中可以用 git remote rename 命令修改某个远程仓库在本地的简短名称，比如想把 pb改成paul，可以这么运行： 1234$ git remote rename pb paul$ git remoteoriginpaul 注意，对远程仓库的重命名，也会使对应的分支名称发生变化，原来的 pb/master 分支现在成了 paul/master。 碰到远端仓库服务器迁移，或者原来的克隆镜像不再使用，又或者某个参与者不再贡献代码，那么需要移除对应的远端仓库，可以运行 git remote rm 命令： 123$ git remote rm paul$ git remoteorigin 打标签同大多数 VCS 一样，Git 也可以对某一时间点上的版本打上标签。人们在发布某个软件版本（比如 v1.0 等等）的时候，经常这么做。本节我们一起来学习如何列出所有可用的标签，如何新建标签，以及各种不同类型标签之间的差别。 列出已有的标签 列出现有标签的命令非常简单，直接运行 git tag 即可： 123$ git tagv0.1v1.3 显示的标签按字母顺序排列，所以标签的先后并不表示重要程度的轻重。 我们可以用特定的搜索模式列出符合条件的标签。在 Git 自身项目仓库中，有着超过 240 个标签，如果你只对 1.4.2 系列的版本感兴趣，可以运行下面的命令： 12345$ git tag -l &apos;v1.4.2.*&apos;v1.4.2.1v1.4.2.2v1.4.2.3v1.4.2.4 新建标签 Git 使用的标签有两种类型：轻量级的（lightweight）和含附注的（annotated）。轻量级标签就像是个不会变化的分支，实际上它就是个指向特 定提交对象的引用。而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标 签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。一般我们都建议使用含附注型的标签，以便保留相关信息；当然，如果只是临时性加注标签，或者不需要旁注额外信息，用轻量级标签也没问题。 含附注的标签创建一个含附注类型的标签非常简单，用 -a （译注：取 annotated 的首字母）指定标签名字即可： 12345$ git tag -a v1.4 -m &apos;my version 1.4&apos;$ git tagv0.1v1.3v1.4 而 -m 选项则指定了对应的标签说明，Git 会将此说明一同保存在标签对象中。如果没有给出该选项，Git 会启动文本编辑软件供你输入标签说明。 可以使用 git show 命令查看相应标签的版本信息，并连同显示打标签时的提交对象。 12345678910111213$ git show v1.4tag v1.4Tagger: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Feb 9 14:45:11 2009 -0800my version 1.4commit 15027957951b64cf874c3557a0f3547bd83b3ff6Merge: 4a447f7... a6b4c97...Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sun Feb 8 19:02:46 2009 -0800 Merge branch &apos;experiment&apos; 我们可以看到在提交对象信息上面，列出了此标签的提交者和提交时间，以及相应的标签说明。 轻量级标签轻量级标签实际上就是一个保存着对应提交对象的校验和信息的文件。要创建这样的标签，一个 -a，-s 或 -m 选项都不用，直接给出标签名字即可： 1234567$ git tag v1.4-lw$ git tagv0.1v1.3v1.4v1.4-lwv1.5 现在运行 git show 查看此标签信息，就只有相应的提交对象摘要： 12345678910$ git show v1.4-lwcommit 15027957951b64cf874c3557a0f3547bd83b3ff6Merge: 4a447f7... a6b4c97...Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sun Feb 8 19:02:46 2009 -0800 Merge branch &apos;experiment&apos; 验证标签可以使用 git tag -v [tag-name] （译注：取 verify 的首字母）的方式验证已经签署的标签。此命令会调用 GPG 来验证签名，所以你需要有签署者的公钥，存放在 keyring 中，才能验证： 123456789$ git tag -v v1.4.2.1object 883653babd8ee7ea23e6a5c392bb739348b1eb61type committag v1.4.2.1tagger Junio C Hamano &lt;junkio@cox.net&gt; 1158138501 -0700 GIT 1.4.2.1Minor fixes since 1.4.2, including git-mv and git-http with alternates.gpg: Signature made Wed Sep 13 02:08:25 2006 PDT using DSA key ID F3119B9Agpg: Good signature from &quot;Junio C Hamano &lt;junkio@cox.net&gt;&quot;gpg: aka &quot;[jpeg image of size 1513]&quot;Primary key fingerprint: 3565 2A26 2040 E066 C9A7 4A7D C0C6 D9A4 F311 9B9A 分享标签默认情况下，git push 并不会把标签传送到远端服务器上，只有通过显式命令才能分享标签到远端仓库。其命令格式如同推送分支，运行git push origin [tagname] 即可： 1234567$ git push origin v1.5Counting objects: 50, done.Compressing objects: 100% (38/38), done.Writing objects: 100% (44/44), 4.56 KiB, done.Total 44 (delta 18), reused 8 (delta 1)To git@github.com:schacon/simplegit.git* [new tag] v1.5 -&gt; v1.5 如果要一次推送所有本地新增的标签上去，可以使用 --tags 选项： 1234567891011$ git push origin --tagsCounting objects: 50, done.Compressing objects: 100% (38/38), done.Writing objects: 100% (44/44), 4.56 KiB, done.Total 44 (delta 18), reused 8 (delta 1)To git@github.com:schacon/simplegit.git * [new tag] v0.1 -&gt; v0.1 * [new tag] v1.2 -&gt; v1.2 * [new tag] v1.4 -&gt; v1.4 * [new tag] v1.4-lw -&gt; v1.4-lw * [new tag] v1.5 -&gt; v1.5 现在，其他人克隆共享仓库或拉取数据同步后，也会看到这些标签。 技巧和窍门在结束本章之前，我还想和大家分享一些 Git 使用的技巧和窍门。很多使用 Git 的开发者可能根本就没用过这些技巧，我们也不是说在读过本书后非得用这些技巧不可，但至少应该有所了解吧。说实话，有了这些小窍门，我们的工作可以变得更简单，更轻松，更高效。 自动补全如果你用的是 Bash shell，可以试试看 Git 提供的自动完成脚本。下载 Git 的源代码，进入 contrib/completion 目录，会看到一个git-completion.bash 文件。将此文件复制到你自己的用户主目录中（译注：按照下面的示例，还应改名加上点：cp git-completion.bash ~/.git-completion.bash），并把下面一行内容添加到你的.bashrc 文件中： 1source ~/.git-completion.bash 也可以为系统上所有用户都设置默认使用此脚本。Mac 上将此脚本复制到 /opt/local/etc/bash_completion.d 目录中，Linux 上则复制到/etc/bash_completion.d/ 目录中。这两处目录中的脚本，都会在 Bash 启动时自动加载。 如果在 Windows 上安装了 msysGit，默认使用的 Git Bash 就已经配好了这个自动完成脚本，可以直接使用。 在输入 Git 命令的时候可以敲两次跳格键（Tab），就会看到列出所有匹配的可用命令建议： 12$ git co commit config 此例中，键入 git co 然后连按两次 Tab 键，会看到两个相关的建议（命令） commit 和 config。继而输入 m会自动完成git commit 命令的输入。 命令的选项也可以用这种方式自动完成，其实这种情况更实用些。比如运行 git log 的时候忘了相关选项的名字，可以输入开头的几个字母，然后敲 Tab 键看看有哪些匹配的： 123$ git log --s --shortstat --since= --src-prefix= --stat --summary 这个技巧不错吧，可以节省很多输入和查阅文档的时间。 Git命令别名Git 并不会推断你输入的几个字符将会是哪条命令，不过如果想偷懒，少敲几个命令的字符，可以用 git config 为命令设置别名。来看看下面的例子： 1234$ git config --global alias.co checkout$ git config --global alias.br branch$ git config --global alias.ci commit$ git config --global alias.st status 现在，如果要输入 git commit 只需键入 git ci 即可。而随着 Git 使用的深入，会有很多经常要用到的命令，遇到这种情况，不妨建个别名提高效率。 使用这种技术还可以创造出新的命令，比方说取消暂存文件时的输入比较繁琐，可以自己设置一下： 1$ git config --global alias.unstage &apos;reset HEAD --&apos; 这样一来，下面的两条命令完全等同： 12$ git unstage fileA$ git reset HEAD fileA 显然，使用别名的方式看起来更清楚。另外，我们还经常设置 last 命令： 1$ git config --global alias.last &apos;log -1 HEAD&apos; 然后要看最后一次的提交信息，就变得简单多了： 1234$ git lastcommit 66938dae3329c7aebe598c2246a8e6af90d04646Author: Josh Goebel &lt;dreamer3@example.com&gt;Date: Tue Aug 26 19:48:51 2008 +0800 可以看出，实际上 Git 只是简单地在命令中替换了你设置的别名。不过有时候我们希望运行某个外部命令，而非 Git 的附属工具，这个好办，只需要在命令前加上 ! 就行。如果你自己写了些处理 Git 仓库信息的脚本的话，就可以用这种技术包装起来。作为演示，我们可以设置用 git visual启动gitk： 1$ git config --global alias.visual &quot;!gitk&quot; 到目前为止，你已经学会了最基本的 Git 操作：创建和克隆仓库，做出更新，暂存并提交这些更新，以及查看所有历史更新记录。接下来，我们将学习 Git 的必杀技特性：分支模型。 转载链接Git细节拾遗]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git和svn的详细对比表]]></title>
    <url>%2F2018%2F07%2F27%2Fgit%E5%92%8Csvn%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AF%B9%E6%AF%94%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[刚开始工作那会，工作做版本控制的选型，几个tl最后选的git，当时不是很懂，只知道git性能多一些，现在回头看了一下这个表格，更加明白他们之间的差异了，git完胜！ 版本工具差异 svn git 系统特点 1.集中式版本控制系统（文档管理很方便）2.企业内部并行集中开发3.windows系统上开发推荐使用4.克隆一个拥有将近一万个提交(commit),五个分支,每个分支有大约1500个文件，用时将近一个小时 1.分布式系统（代码管理很方便）2.开源项目开发3.mac,Linux系统上开发推荐使用4.克隆一个拥有将近一万个提交(commit),五个分支,每个分支有大约1500个文件，用时1分钟 灵活性 1.搭载svn的服务器出现故障，无法与之交互2.所有的svn操作都需要中央仓库交互（例：拉分支，看日志等） 1.可以单机操作，git服务器故障也可以在本地git仓库工作2.除了push和pull（或fetch）操作，其他都可以在本地操作3.根据自己开发任务任意在本地创建分支4.日志都是在本地查看，效率较高 安全性 较差，定期备份，并且是整个svn都得备份 较高，每个开发者的本地就是一套完整版本库，记录着版本库的所有信息（gitlab集成了备份功能） 分支方面 1.拉分支更像是copy一个路径2.可针对任何子目录进行branch3.拉分支的时间较慢，因为拉分支相当于copy4.创建完分支后，影响全部成员，每个人都会拥有这个分支5.多分支并行开发较重（工作较多而且繁琐） 1.我可以在Git的任意一个提交点（commit point）开启分支！（git checkout -b newbranch HashId）2.拉分支时间较快，因为拉分支只是创建文件的指针和HEAD3.自己本地创建的分支不会影响其他人4.比较适合多分支并行开发5.git checkout hash值(切回之前的版本，无需版本回退)6.强大的cherry-pick 版本控制 1.保存前后变化的差异数据，作为版本控制2.版本号进行控制，每次操作都会产生一个高版本号（svn的全局版本号，这是svn一个较大的特点，git是hash值） 1.git只关心文件数据的整体发生变化，更像是把文件做快照，文件没有改变时，分支只想这个文件的指针不会改变，文件发生改变，指针指向新版本2. 40 位长的哈希值作为版本号，没有先后之分3.git rebase操作可以更好的保持提交记录的整洁 工作流程 1.每次更改文件之前都得update操作，有的时候修改过程中这个文件有更新，commit不会成功2.有冲突，会打断提交动作（冲突解决是一个提交速度的竞赛：手快者，先提交，平安无事；手慢者，后提交，可能遇到麻烦的冲突解决。） 1.开始工作前进行fetch操作，完成开发工作后push操作，有冲突解决冲突2.git的提交过程不会被打断，有冲突会标记冲突文件3.gitflow流程（经典） 内容管理 svn对中文支持好，操作简单，适用于大众 对程序的源代码管理方便，代码库占用的空间少，易于分支化管理 学习成本 使用起来更方便，svn对中文支持好，操作简单，适用于大众 更在乎效率而不是易用性，成本较高（有很多独有的命令，rebase，远程仓库交互的命令，等等） 权限管理 svn的权限管理相当严格，可以按组、个人针对某个子目录的权限控制（每个目录下都会有个.svn的隐藏文件） git没有严格的权限管理控制，只有账号角色划分（在项目的home文件下有且只有一个.svn目录） 管理平台 有吧（这个“吧”字，肯定有，但本人没有接触过） gitlab（建议使用，集成的功能较多，API开发），gerrit，github等 转载链接： git和svn的详细对比]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git的基础使用]]></title>
    <url>%2F2018%2F07%2F27%2Fgit%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Git是一个分布式的版本控制工具，本篇文章从介绍Git开始，重点在于介绍Git的基本命令和使用技巧，让你尝试使用Git的同时，体验到原来一个版 本控制工具可以对开发产生如此之多的影响，文章分为两部分，第一部分介绍Git的一些常用命令，其中穿插介绍Git的基本概念和原理，第二篇重点介绍 Git的使用技巧，最后会在Git Hub上创建一个开源项目开启你的Git实战之旅。 Git是什么 Git在Wikipedia上的定义：它是一个免费的、分布式的版本控制工具，或是一个强调了速度快的源代码管理工具。Git最初被Linus Torvalds开发出来用于管理Linux内核的开发。每一个Git的工作目录都是一个完全独立的代码库，并拥有完整的历史记录和版本追踪能力，不依赖 于网络和中心服务器。 ​ Git的出现减轻了许多开发者和开源项目对于管理分支代码的压力，由于对分支的良好控制，更鼓励开发者对自己感兴趣的项目做出贡献。其实许多开源项目 包括Linux kernel, Samba, X.org Server, Ruby on Rails，都已经过渡到使用Git作为自己的版本控制工具。对于我们这些喜欢写代码的开发者嘛，有两点最大的好处，我们可以在任何地点(在上班的地铁 上)提交自己的代码和查看代码版本;我们可以开许许多多个分支来实践我们的想法，而合并这些分支的开销几乎可以忽略不计。 Git 初始化​ 现在进入本篇文章真正的主题，介绍一下Git的基本命令和操作，会从Git的版本库的初始化，基本操作和独有的常用命令三部分着手，让大家能够开始使用Git。 ​ Git通常有两种方式来进行初始化: ​ git clone: 这是较为简单的一种初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份，例如’git clone git://github.com/someone/some_project.git some_project’命令就是将’git://github.com/someone/some_project.git’这个URL地址的远程版 本库完全克隆到本地some_project目录下面 ​ git init和git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用 git init 命令进行初始化，Git以后就会对该目录下的文件进行版本控制，这时候如果你需要将它放到远程服务器上，可以在远程服务器上创建一个目录，并把 可访问的URL记录下来，此时你就可以利用 git remote add 命令来增加一个远程服务器端，例如’git remote add origin git://github.com/someone/another_project.git’这条命令就会增加URL地址为’git: //github.com/someone/another_project.git’，名称为origin的远程服务器，以后提交代码的时候只需要使用 origin别名即可 Git 基本命令​ 现在我们有了本地和远程的版本库，让我们来试着用用Git的基本命令吧： ​ git pull：从版本库(既可以是远程的也可以是本地的)将代码更新到本地，例如：’git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝，该功能类似于SVN的update ​ git add：将所有改动的文件（新增和有变动的）放在暂存区，由git进行管理 ​ git rm：从当前的工作空间中和索引中删除文件，例如’git rm app/model/user.rb’，移除暂存区 ​ git commit：提交当前工作空间的修改内容，类似于SVN的commit命令，例如’git commit -m “story #3, add user model”‘，提交的时候必须用-m来输入一条提交信息 ​ git push：将本地commit的代码更新到远程版本库中，例如’git push origin branchname’就会将本地的代码更新到名为orgin的远程版本库中 ​ git log：查看历史日志 ​ git revert：还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值、 ​ 上面的命令几乎都是每个版本控制工具所公有的，下面就开始尝试一下Git独有的一些命令： Git 独有命令​ git branch：对分支的增、删、查等操作，例如 git branch new_branch 会从当前的工作版本创建一个叫做new_branch的新分支，git branch -D new_branch 就会强制删除叫做new_branch的分支，git branch 就会列出本地所有的分支 ​ git checkout：Git的checkout有两个作用，其一是在 不同的branch之间进行切换，例如 ‘git checkout new_branch’就会切换到new_branch的分支上去;另一个功能是 还原代码的作用，例如git checkout app/model/user.rb 就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚 ​ git rebase：用下面两幅图解释会比较清楚一些，rebase命令执行后，实际上是将分支点从C移到了G，这样分支也就具有了从C到G的功能 （使历史更加简洁明了） ​ git reset：回滚到指定的版本号，我们有A-G提交的版本，其中C 的版本号是 bbaf6fb，我们执行了’git reset bbaf6fb’那么结果就只剩下了A-C三个提交的版本 ​ git stash：将当前未提交的工作存入Git工作栈中，时机成熟的时候再应用回来，这里暂时提一下这个命令的用法，后面在技巧篇会重点讲解 ​ git config：新增、更改Git的各种设置，例如：git config branch.master.remote origin 就将master的远程版本库设置为别名叫做origin版本库 ​ git tag：将某个版本打上一个标签，例如：git tag revert_version bbaf6fb50 来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了 Git其他命令add #添加文件内容至索引 branch #列出、创建或删除分支 checkout #检出一个分支或路径到工作区 clone #克隆一个版本库到一个新目录 commit #最近一次的提交，–amend修改最近一次提交说明 diff #显示提交之间、提交和工作区之间等的差异 fetch #从另外一个版本库下载对象和引用 init #创建一个空的 Git 版本库或重新初始化一个已存在的版本库 log #显示提交日志 –stat 具体文件的改动 reflog #记录丢失的历史 merge #合并两个或更多开发历史，–squash 把分支所有提交合并成一个提交 mv #移动或重命名一个文件、目录或符号链接 pull #获取并合并另外的版本库或一个本地分支（相当于git fetch和git merge） push #更新远程引用和相关的对象 rebase #本地提交转移至更新后的上游分支中 reset #重置当前HEAD到指定状态 rm #从工作区和索引中删除文件 show #显示各种类型的对象 status #显示工作区状态 tag #创建、列出、删除或校验一个GPG签名的 tag 对象 cherry-pick #从其他分支复制指定的提交，然后导入到现在的分支 git分支命令创建分支： git branch linux #创建分支 git checkout linux #切换分支 git branch #查看当前分支情况,当前分支前有*号 git add readme.txt #提交到暂存区 git commit -m “new branch” #提交到git版本仓库 git checkout master #我们在提交文件后再切回master分支 分支合并：（合并前必须保证在master主干上） git branch #查看在哪个位置 git merge Linux #合并创建的Linux分支（–no–ff默认情况下，Git执行”快进式合并”（fast-farward merge），会直接将Master分支指向Develop分支。使用–no–ff参数后，会执行正常合并，在Master分支上生成一个新节点。） git branch -d linux #确认合并后删除分支 如果有冲突： git merge linux #合并Linux分支(冲突) Auto-merging readme.txt CONFLICT (content): Merge conflict in readme.txt Automatic merge failed; fix conflicts and then commit the result. 那么此时，我们在master与linux分支上都分别对中readme文件进行了修改并提交了，那这种情况下Git就没法再为我们自动的快速合并了，它只能告诉我们readme文件的内容有冲突，需要手工处理冲突的内容后才能继续合并 自己修改完readme.txt文件后再次提交 git全局配置1234567891011yum install git #安装Gitgit config –global user.name “xubusi” #配置git使用用户git config –global user.email “xubusi@mail.com” #配置git使用邮箱git config –global color.ui true #加颜色 git config –list #所有配置的信息（上面的结果）user.name=xubusiuser.email=xubusi@mail.comcolor.ui=true .git目录结构 Git之所以能够提供方便的本地分支等特性，是与它的文件存储机制有关的。Git存储版本控制信息时使用它自己定义的一套文件系统存储机制，在代码根目录下有一个.git文件夹，会有如下这样的目录结构： 123456789HEADbranches/configdescriptionhooks/indexinfo/objects/refs/ 有几个比较重要的文件和目录需要解释一下： HEAD：文件存放根节点的信息，其实目录结构就表示一个树型结构，Git采用这种树形结构来存储版本信息， 那么HEAD就表示根; refs：目录存储了你在当前版本控制目录下的各种不同引用(引用指的是你本地和远程所用到的各个树分支的信息)，它有heads、 remotes、stash、tags四个子目录，分别存储对不同的根、远程版本库、Git栈和标签的四种引用，你可以通过命令’git show-ref’更清晰地查看引用信息; logs：目录根据不同的引用存储了日志信息。因此，Git只需要代码根目录下的这一个.git目录就可以记录完 整的版本控制信息，而不是像SVN那样根目录和子目录下都有.svn目录。那么下面就来看一下Git与SVN的区别吧 .gitigmore: 放一些不需要git管理的文件（例：IDE的工作目录 .idea，） git与svn的不同VN(Subversion)是当前使用最多的版本控制工具。与它相比较，Git最大的优势在于两点：易于本地增加分支和分布式的特性。 ​ 下面两幅图可以形象的展示Git与SVN的不同之处 GIT对于易于本地增加分支，图中Git本地和服务器端结构都很灵活，所有版本都存储在一个目录中，你只需要进行分支的切换即可达到在某个分支工作的效果。 SVN则完全不同，如果你需要在本地试验一些自己的代码，只能本地维护多个不同的拷贝，每个拷贝对应一个SVN服务器地址。 分布式对于Git而言，你可以本地提交代码，所以在上面的图中，Git有利于将一个大任务分解，进行本地的多次提交，而SVN只能在本地进行大量的一 次性更改，导致将来合并到主干上造成巨大的风险。Git的代码日志是在本地的，可以随时查看。SVN的日志在服务器上的，每次查看日志需要先从服务器上下 载下来。我工作的小组，代码服务器在美国，每次查看小组几年前所做的工作时，日志下载就需要十分钟，这不能不说是一个痛苦。后来我们迁移到Git上，利用 Git日志在本地的特性，我用Ruby编写了一个Rake脚本，可以查看某个具体任务的所有代码历史，每次只需要几秒钟，大大方便我的工作。当然分布式并 不是说用了Git就不需要一个代码中心服务器，如果你工作在一个团队里，还是需要一个服务器来保存所有的代码的。 实际的例子： 以前我所 在的小组使用SVN作为版本控制工具，当我正在试图增强一个模块，工作做到一半，由于会改变原模块的行为导致代码服务器上许多测试的失败，所以并没有提交 代码。这时候上级对我说，现在有一个很紧急的Bug需要处理， 必须在两个小时内完成。我只好将本地的所有修改diff，并输出成为一个patch文件，然后回滚有关当前任务的所有代码，再开始修改Bug的任务，等到 修改好后，在将patch应用回来。前前后后要完成多个繁琐的步骤，这还不计中间代码发生冲突所要进行的工作量。 可是如果使用Git， 我们只需要开一个分支或者转回到主分支上，就可以随时开始Bug修改的任务，完成之后，只要切换到原来的分支就可以优雅的继续以前的任务。只要你愿意，每 一个新的任务都可以开一个分支，完成后，再将它合并到主分支上，轻松而优雅。 gitlab介绍安装服务相关命令安装有可能的依赖： yum install openssh-server yum install postfix yum install cronie 安装gitlab： curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh #下载数据源 yum install gitlab-ce 安装完成后： gitlab-ctl reconfigure #使配置文件生效 但是会初始化除了gitlab.rb之外的所有文件 gitlab-ctl status #查看状态 gitlab-ctl stop #停服务 gitlab-ctl start #起服务 gitlab-ctl tail #查看日志的命令（Gitlab 默认的日志文件存放在/var/log/gitlab 目录下） 如下表示启动成功：（全是run，有down表示有的服务没启动成功） 然后打开浏览器输入ip或者域名 相关目录.git/config #版本库特定的配置设置，可用–file修改 ~/.gitconfig #用户特定的配置设置，可用–global修改 /var/opt/gitlab/git-data/repositories/root #库默认存储目录 /opt/gitlab #是gitlab的应用代码和相应的依赖程序 /var/opt/gitlab #此目录下是运行gitlab-ctl reconfigure命令编译后的应用数据和配置文件，不需要人为修改配置/etc/gitlab #此目录下存放了以omnibus-gitlab包安装方式时的配置文件，这里的配置文件才需要管理员手动编译配置/var/log/gitlab #此目录下存放了gitlab各个组件产生的日志 /var/opt/gitlab/backups/ #备份文件生成的目录 相关文件/opt/gitlab/embedded/service/gitlab-rails/config #配置文件（修改clone的ip地址） /etc/gitlab/gitlab.rb #设置相关选项进行配置（gitlab地址就在这） /var/opt/gitlab/git-data #Git存储库数据（默认) 转载链接Git使用基础篇]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github向导]]></title>
    <url>%2F2018%2F07%2F27%2Fgit-github%E5%90%91%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[Hello World项目是计算机编程的悠久传统。这是一个简单的练习，让你开始学习新的东西。让我们开始使用GitHub！ 你将学到如下内容： 1: 创建和使用一个仓库。 2: 开始和管理一个分支。 3: 更改一个文件，然后推送到仓库，并且附带一些注释。 4: 打开和合并一个推送请求。 创建一个仓库仓库用来组织一个单一的项目，可以包含目录和文件，图片，视频，表格和数据集等所有项目所需要的内容。建议增加一个README文件用来描述项目相关信息。github可以直接生成一个空的README文件。 1: 进入github，在右上角找到+号，然后选择新建项目。 2: 输入项目名称，比如hello-world。 3: 写一些简短的描述。 4: 选择可见等级； 5: 单击创建项目，即可完成创建。 创建完成后，如果是空的项目，会显示一个命令列表，以帮助用户通过git进行操作： 命令行指令 Git 全局设置 12git config --global user.name &quot;clay&quot;git config --global user.email &quot;clay@clay.com&quot; 创建新版本库 123456git clone http://clay.com/clay/hello-world.gitcd hello-worldtouch README.mdgit add README.mdgit commit -m &quot;add README&quot;git push -u origin master 已存在的文件夹 123456cd existing_foldergit initgit remote add origin http://clay.com/clay/hello-world.gitgit add .git commitgit push -u origin master 已存在的 Git 版本库 1234cd existing_repogit remote add origin http://clay.com/clay/hello-world.gitgit push -u origin --allgit push -u origin --tags 创建一个分支github上默认分支为master。并且还提供了将分支合并的功能。 1: 打开hello-world仓库首页。 2: 在项目名称之后单击+号，弹出菜单，并选择新分支，赚到分支创建页。 3：输入分支名称，单击绿色创建分支按钮，即可创建成功。 4: 创建成功后，回到hello-world项目首页，可以看到新创建的分支。 更改和提交更新1: 在hello-world项目首页，在对应项目名称的后面单击+号，弹出菜单，并选择新文件（也可以选择上传文件以上传一个新的本地文件，或者单击新目录以创建一个新目录）。 或者如果有文件存在，打开对应的文件，然后单击编辑按钮，以开始编辑一个存在的文件。 2: 我们以新文件为例，如下图，输入文件名称，文件内容，并且在下方输入注释，然后单击提交修改即可完成新文件或者修改文件的功能： 开启一个推送请求如果将某个分支的更改情况推送到另外一个分支，或者master，需要提交一个推送请求。 1: 打开hello-world项目首页，单击最上头的合并请求。 2: 单击绿色的新建合并请求。 3: 选择来源分支（即当前分支newbranch）与目标分支（比如master），单击比较分支后继续。 4: 填写标题和描述，确定来源分支和目标分支，以及确定最下方的提交和变更内容，最后单击绿色的提交新的合并请求。 合并一个推送请求经过步骤3之后，项目的所有者或者在上述步骤中指定了指派人，会收到一个合并请求的通知。 当确认后，会进行具体的合并过程。 此过程，也可以通过命令行来完成，具体过程如下 检出，在本地审查和合并 Step 1. 获取并检出此合并请求的分支 12git fetch origingit checkout -b newbranch origin/newbranch Step 2. 本地审查变更 Step 3. 合并分支并修复出现的任何冲突 Step 4. 推送合并的结果到 GitLab 1git push origin master 常用的命令行功能1: 更新 12$ git fetch origin 更新主分支的更新$ git fetch 更新所有内容 2: 克隆 1$ git clone http://clay.com/clay/hello-world.git 3: 在某个分支上克隆 1$ git clone -b newbranch http://clay.com/clay/hello-world.git 4: 合并 $ git merge origin/master 5: 更新，然后合并 $ git pull 6: 添加文件 $ git add [file.name](http://file.name) 7: 删除文件 $ git rm [file.name](http://file.name) 8: 添加注释 $ git commit -m ‘add a new file’ 9: 推送更改 $ git push -u origin/master]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈CSS（相对定位、绝对定位、固定定位、z-index）]]></title>
    <url>%2F2018%2F07%2F27%2Fcss5%2F</url>
    <content type="text"><![CDATA[今天接着来学习CSS，go！本节内容有相对定位、绝对定位、固定定位和z-index。 定位有三种，分别是相对定位、绝对定位、固定定位。 相对定位： 1position:relative; 绝对定位： 1position:absolute; 固定定位： 1position:fixed; 相对定位 认识相对定位 相对定位，就是微调元素位置的。让元素相对自己原来的位置，进行位置调整。 不脱标，老家留坑，形影分离 相对定位不脱标，真实位置是在老家，只不过影子出去了，可以到处飘 相对定位用途 相对定位有坑，所以一般不用于做“压盖”效果。页面中，效果极小。就两个作用： 1） 微调元素 2） 做绝对定位的参考，子绝父相。 相对定位的定位植 可以用left、right来描述盒子右、左的移动； 可以用top、bottom来描述盒子的下、上的移动。 ↘： 123position:relative;top:10px;left:40px; ↙： 123position:relative;right:100px; → 往左边移动top: 100px; ↖： 123position: relative; right: 100px;bottom: 100px; → 移动方向是向上。 ↗： 123position: relative;top: -200px; → 负数就是相反的方向，如果是正，就是下边，如果是负数就是上边right: -200px; ↗： 123position: relative;right: -300px;bottom: 300px; 完全等价于： 123position: relative;left: 300px;bottom: 300px; 绝对定位绝对定位比相对定位更灵活。 绝对定位脱标 对定位的盒子，是脱离标准文档流的。所以，所有的标准文档流的性质，绝对定位之后都不遵守了。 绝对定位之后，标签就不区分所谓的行内元素、块级元素了，不需要display:block;就可以设置宽、高了： 12345678span&#123;position: absolute;top: 100px;left: 100px;width: 100px;height: 100px;background-color: pink;&#125; 参考点 绝对定位的参考点，如果用top描述，那么定位参考点就是页面的左上角，而不是浏览器的左上角： 如果用bottom描述，那么就是浏览器首屏窗口尺寸，对应的页面的左下角： 面试题： 以盒子为参考点 一个绝对定位的元素，如果父辈元素中出现了也定位了的元素，那么将以父辈这个元素，为参考点。 要听最近的已经定位的祖先元素的，不一定是父亲，可能是爷爷： 12345&lt;div class="box1"&gt; → 相对定位 &lt;div class="box2"&gt; → 没有定位 &lt;p&gt;&lt;/p&gt; → 绝对定位，将以box1为参考，因为box2没有定位，box1就是最近的父辈元素 &lt;/div&gt;&lt;/div&gt; 12345&lt;div class="box1"&gt; → 相对定位 &lt;div class="box2"&gt; → 相对定位 &lt;p&gt;&lt;/p&gt; → 绝对定位，将以box2为参考，因为box2是自己最近的父辈元素 &lt;/div&gt;&lt;/div&gt; 不一定是相对定位，任何定位，都可以作为参考点 123&lt;div&gt; → 绝对定位 &lt;p&gt;&lt;/p&gt; → 绝对定位，将以div作为参考点。因为父亲定位了。&lt;/div&gt; 子绝父绝、子绝父相、子绝父固，都是可以给儿子定位的。但是，工程上子绝、父绝，没有一个盒子在标准流里面了，所以页面就不稳固，没有任何实战用途。工程上，“子绝父相”有意义，父亲没有脱标，儿子脱标在父亲的范围里面移动。 绝对定位的儿子，无视参考的那个盒子的padding。 下图中，绿色部分是div的padding，蓝色部分是div的内容区域。那么此时，div相对定位，p绝对定位。 p将无视父亲的padding，在border内侧为参考点，进行定位： 绝对定位的盒子居中 绝对定位之后，所有标准流的规则，都不适用了。所以margin:0 auto;失效。 123456width: 600px;height: 60px;position: absolute;left: 50%;top: 0;margin-left: -300px; → 宽度的一半 非常简单，当做公式记忆下来。就是left:50%;margin-left:负的宽度的一半。 固定定位固定定位，就是相对浏览器窗口定位。页面如何滚动，这个盒子显示的位置不变。 固定定位脱标！ z-index● z-index值表示谁压着谁。数值大的压盖住数值小的。 ● 只有定位了的元素，才能有z-index值。也就是说，不管相对定位、绝对定位、固定定位，都可以使用z-index值。而浮动的东西不能用。 ● z-index值没有单位，就是一个正整数。默认的z-index值是0。 ● 如果大家都没有z-index值，或者z-index值一样，那么谁写在HTML后面，谁在上面能压住别人。定位了的元素，永远能够压住没有定位的元素。 ● 从父现象：父亲怂了，儿子再牛逼也没用。 没有单位： 1z-index:888]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈CSS（行高和字号、超链接的美化、background系列属性）]]></title>
    <url>%2F2018%2F07%2F27%2Fcss4%2F</url>
    <content type="text"><![CDATA[好了，今天继续学习CSS，go！本节内容有行高和字号、超链接的美化、background系列属性。 行高和字号行高CSS中，所有的行，都有行高。盒模型的padding，绝对不是直接作用在文字上的，而是作用在“行”上的。 1line-height: 20px; 文字，是在自己的行里面居中的。 为了严格行高、字号，一般都是偶数。这样，它们的差，就是偶数，就能够被2整除。 单行文本垂直居中文本在行里面居中 公式：行高：盒子高 需要注意的是，这个小技巧，行高=盒子高，只适用于单行文本垂直居中！！不适用于多行。 如果想让多行文本垂直居中，需要设置盒子的padding。 font属性使用font属性，能够将字号、行高、字体，能够一起设置 1font: 14px/24px "宋体"; 等价于三行语句： 123font-size:14px;line-height:24px;font-family:"宋体"; font-family就是“字体”。family是“家庭”、“伐木累”的意思。 网页中不是所有字体都能用哦，因为这个字体要看用户的电脑里面装没装，比如你设置 1font-family:"华文彩云" 如果用户电脑里面没有这个字体，那么就会变成宋体。 页面中，中文我们只使用： 微软雅黑、宋体、黑体。 如果页面中，需要其他的字体，那么需要切图。 为了防止用户电脑里面，没有微软雅黑这个字体。就要用英语的逗号，隔开备选字体，就是说如果用户电脑里面，没有安装微软雅黑字体，那么就是宋体： 1font-family:"微软雅黑","宋体"; 备选字体可以有无数个，用逗号隔开。 我们要将英语字体，放在最前面，这样所有的中文，就不能匹配英语字体，就自动的变为后面的中文字体： 1font-family: "Times New Roman","微软雅黑","宋体"; 所有的中文字体，都有英语别名，我们也要知道 微软雅黑的英语别名：Microsoft YaHei 宋体的英语别名：SimSun 行高可以用百分比，表示字号的百分之多少。一般来说，都是大于100%的，因为行高一定要大于字号: 1font:12px/200% "SimSun" 超级链接的美化超级链接就是a标签 伪类也就是说，同一个标签，根据用户的某种状态不同，有不同的样式。这就叫做“伪类”。 类就是工程师加的，比如div属于box类，很明确，就是属于box类。但是a属于什么类？不明确。因为要看用户有没有点击、有没有触碰。所以，就叫做“伪类”。 伪类用冒号来表示。 a标签有4种伪类： 123456789101112a:link&#123; color:red;&#125;a:visited&#123; color:orange;&#125;a:hover&#123; color:green;&#125;a:active&#123; color:black;&#125; :link 表示， 用户没有点击过这个链接的样式。是英语“链接”的意思。 :visited 表示，用户访问过了这个链接的样式。 是英语“访问过的”的意思。 :hover 表示，用户鼠标悬停的时候链接的样式。 是英语“悬停”的意思。 :active 表示，用户用鼠标点击这个链接，但是不松手，此刻的样式。 是英语“激活”的意思。 记住，这四种状态，在css中，必须按照固定的顺序写： a:link、a:visited 、a:hover 、a:active 如果不按照顺序，那么将失效。“爱恨准则” love hate。必须先爱，后恨。 超链接美化a标签在使用的时候，非常的难。因为不仅仅要控制a这个盒子，也要控制它的伪类。 我们一定要将a标签写在前面，:link、:visited、:hover、:active这些伪类写在后面。 a标签中，描述盒子；伪类中描述文字的样式、背景。 123456789101112131415.nav ul li a&#123; display: block; width: 120px; height: 40px;&#125;.nav ul li a:link ,.nav ul li a:visited&#123; text-decoration: none; background-color: yellowgreen; color:white;&#125;.nav ul li a:hover&#123; background-color: purple; font-weight: bold; color:yellow;&#125; 记住，所有的a不继承text、font这些东西。因为a自己有一个伪类的权重。 最标准的，就是把link、visited、hover都要写。但是前端开发工程师在大量的实践中，发现不写link、visited浏览器也挺兼容。所以这些“老油条”们，就把a标签简化了： a:link、a:visited都是可以省略的，简写在a标签里面。也就是说，a标签涵盖了link、visited的状态 1234567891011.nav ul li a&#123; display: block; width: 120px; height: 50px; text-decoration: none; background-color: purple; color:white;&#125;.nav ul li a:hover&#123; background-color: orange;&#125; background系列属性background-color属性背景颜色属性。 css2.1中，颜色的表示方法有哪些？一共有三种：单词、rgb表示法、十六进制表示法。 用英语单词来表示能够用英语单词来表述的颜色，都是简单颜色。 红色： 1background-color:red; 用rgb方法来表示红色： 1background-color:rgb(255,0,0); rgb表示三原色“红”red、“绿”green、“蓝”blue。光学显示器，每个像素都是由三原色的发光原件组成的，靠明亮度不同调成不同的颜色的。 用逗号隔开，r、g、b的值，每个值的取值范围0~255，一共256个值。 如果此项的值，是255，那么就说明是纯色： 绿色： 1background-color: rgb(0,255,0); 蓝色： 1background-color: rgb(0,0,255); 黑色： 1background-color: rgb(0,0,0); 光学显示器，每个元件都不发光，黑色的。 白色： 1background-color: rgb(255,255,255); 颜色可以叠加，比如黄色就是红色和绿色的叠加： 1background-color: rgb(255,255,0); 十六进制表示法红色： 1background-color:#ff0000; 所有用#开头的值，都是16进制的。 #ff0000 background-image用于给盒子加上背景图片： 1background-image:url(images/wuyifan.jpg); url()表示网址，uniform resouces locator 同意资源定位符 images/wuyifan.jpg 就是相对路径。 背景天生是会被平铺满的。 padding的区域有背景图。 background-repeat属性设置背景图是否重复的，重复方式的。 repeat表示“重复”。 也就是说，background-repeat属性，有三种值： 123background-repeat:no-repeat; 不重复background-repeat:repeat-x; 横向重复background-repeat:repeat-y; 纵向重复 background-position属性属性的意思背景定位属性，是最难的属性。 position就是“位置”的意思。background-position就是背景定位属性。 1background-position:向右移动量 向下移动量; 定位属性可以是负数。 用单词描述1background-position: 描述左右的词儿 描述上下的词儿; 描述左右的词儿： left、 center、right 描述上下的词儿： top 、center、bottom background-attachment背景是否固定。 1background-attachment:fixed 背景就会被固定住，不会被滚动条滚走。 background综合属性background属性和border一样，是一个综合属性： 1background:red url(1.jpg) no-repeat 100px 100px fixed; 等价于： 12345background-color:red;background-image:url(1.jpg);background-repeat:no-repeat;background-position:100px 100px;background-attachment:fixed;]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈CSS(css性质、盒模型、标准文档流、浮动)]]></title>
    <url>%2F2018%2F07%2F10%2Fcss3%2F</url>
    <content type="text"><![CDATA[今天继续学习CSS，go！本节内容有css性质、盒模型、标准文档流、浮动。 CSS的性质1）继承性。有一些属性给祖先元素，所有的后代元素都集成上了。哪些属性能继承：color、font-、text-、line- 2）层叠性。层叠性是一种能力，就是处理冲突的能力。当不同选择器，对一个标签的同一个样式，有不同的值，听谁的？这就是冲突。css有着严格的处理冲突的机制： 选择上了，数权重，(id的数量，类的数量，标签的数量)。如果权重一样，谁写在后面听谁的。 没有选择上，通过继承影响的，就近原则，谁描述的近听谁的。如果描述的一样近，比如选择器权重，如果权重再一样重，谁写在后面听谁的。 盒模型盒子中的区域一个盒子中主要的属性就5个：width、height、padding、border、margin。 width是“宽度”的意思，CSS中width指的是内容的宽度，而不是盒子的宽度。 height是“高度”的意思，CSS中height指的是内容的高度，而不是盒子的高度 padding是“内边距”的意思 border是“边框” margin是“外边距” padding小属性： 1234padding-top: 30px;padding-right: 20px;padding-bottom: 40px;padding-left: 100px; top上、right右、bottom下、left左。 这种属性，就是复合属性。比如不写padding-left那么就是没有左内边距。 快捷键就是pdt、pdr、pdb、pdl 然后按tab。 综合属性： 如果写了4个值 1padding:30px 20px 40px 100px; 上、右、下、左 border就是边框。边框有三个要素：粗细、线型、颜色。 按3要素拆开： 123border-width:10px; → 边框宽度border-style:solid; → 线型border-color:red; → 颜色。 marginmargin的塌陷现象标准文档流中，竖直方向的margin不叠加，以较大的为准。 如果不在标准流，比如盒子都浮动了，那么两个盒子之间是没有塌陷现象的。 盒子居中margin:0 atuo;margin的值可以为auto，表示自动。当left、right两个方向，都是auto的时候，盒子居中了。 注意： 1） 使用margin:0 auto; 的盒子，必须有width，有明确的width 2） 只有标准流的盒子，才能使用margin:0 auto; 居中。 也就是说，当一个盒子浮动了、绝对定位了、固定定位了，都不能使用margin:0auto; 3） margin:0 auto;是在居中盒子，不是居中文本。 文本的居中，要使用 1text-align:center 善于使用父亲的padding，而不是儿子的margin如果父亲没有border，那么儿子的margin实际上踹的是“流”，踹的是这“行”。所以，父亲整体也掉下来了 这个p有一个margin-top踹父亲，试图将自己下移 123&lt;div&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt; 结果： 儿子用margin-top踹父亲，父亲没有border，结果父亲也下来了。 儿子用margin-top踹父亲，但父亲有border，达到了我们的目的。 margin这个属性，本质上描述的是兄弟和兄弟之间的距离； 最好不要用这个marign，表达父子之间的距离。 所以，我们一定要善于使用父亲的 padding，而不是儿子的margin。 标准文档流宏观的讲，我们的web页面和photoshop等设计软件有本质的区别：web页面的制作，是个“流”，必须从上而下，像“织毛衣”。而设计软件，想往哪里画个东西，都能画。 我们要看看标准流有哪些微观现象： 1）空白折叠现象： 比如，如果我们想让img标签之间没有空隙，必须紧密连接： 1&lt;img src="images/0.jpg" /&gt;&lt;img src="images/1.jpg" /&gt;&lt;img src="images/2.jpg" /&gt; 2）高矮不齐，底边对齐； 3）自动换行，一行写不满，换行写。 块级元素和行内元素标签分为两种等级： 1）块级元素 霸占一行，不能与其他任何元素并列 能接受宽、高 如果不设置宽度，那么宽度将默认变为父亲的100% 2）行内元素 与其他行内元素并排 不能设置宽、高。默认的宽度，就是文字的宽度。 在HTML中，我们已经将标签分过类，当时分为了：文本级、容器级。 文本级：p、span、a、b、i、u、em 容器级：div、h系列、li、dt、dd CSS的分类和上面的很像，就p不一样： 所有的文本级标签，都是行内元素，除了p，p是个文本级，但是是个块级元素。 所有的容器级标签都是块级元素。 块级元素和行内元素的相互转换块级元素可以设置为行内元素 行内元素可以设置为块级元素 123456div&#123; display:inline; background-color:pink; width:500px; height:500px;&#125; display是“显示模式”的意思，用来改变元素的行内、块级性质 inline就是“行内”。 一旦，给一个标签设置display:inline 那么，这个标签将立即变为行内元素。此时它和一个span无异： 此时这个div不能设置宽度、高度 此时这个div可以和别人并排了 同样的道理 123456span&#123; display:block; width:200px; height:200px; background-color:pink;&#125; block”是“块”的意思 让标签变为块级元素。此时这个标签，和一个div无异： 此时这个span能够设置宽度、高度 此时这个span必须霸占一行了，别人无法和他并排 如果不设置宽度，将撑满父亲 标准流里面限制非常多，标签的性质恶心。比如，我们现在就要并排、并且就要设置宽高。 所以，移民！脱离标准流！ css中一共有三种手段，使一个元素脱离标准文档流： 1） 浮动 2） 绝对定位 3） 固定定位 浮动浮动的性质浮动是css里面布局用的最多的属性。 123456.box1&#123; float:left;&#125;.box2&#123; float:left;&#125; 两个元素并排了，并且两个元素都能够设置宽度、高度了（这在刚才的标准流中，不能实现）。 浮动想学好，一定要知道三个性质。 浮动的元素脱标证明1： 证明2： 一个span标签不需要转成块级元素，就能够设置宽度、高度了。所以能够证明一件事儿，就是所有标签已经不区分行内、块了。也就是说，一旦一个元素浮动了，那么，将能够并排了，并且能够设置宽高了。无论它原来是个div还是个span。 123456span&#123; float:left; width:200px; height:200px; background-color:orange;&#125; 浮动的元素相互贴靠如果有足够空间，那么3号就会靠着2哥。如果没有足够的空间，那么3号会靠着1号大哥。如果没有足够的空间靠着1号大哥，自己去贴左墙。 浮动的元素有“字围”效果1234567891011121314 &lt;style type="text/css"&gt; div&#123; float: left; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div&gt; &lt;img src="chick.jpg" alt=""&gt; &lt;/div&gt; &lt;p&gt; 文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字文字 &lt;/p&gt; 让div浮动，p不浮动，效果如图，div挡住了p，但是p中的文字不会被挡住，形成“字围”效果 关于浮动我们要强调一点，浮动这个东西，我们在初期一定要遵循一个原则： 永远不是一个东西单独浮动，浮动都是一起浮动，要浮动，大家都浮动。 浮动的清除来看一个实验：现在有两个div，div身上没有任何属性。每个div中都有li，这些li都是浮动的。 123456789101112131415161718192021222324 &lt;style type="text/css"&gt; li&#123; float: left; width: 100px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div&gt; &lt;ul&gt; &lt;li&gt;HTML&lt;/li&gt; &lt;li&gt;CSS&lt;/li&gt; &lt;li&gt;JS&lt;/li&gt; &lt;li&gt;设计模式&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div&gt; &lt;ul&gt; &lt;li&gt;学习方法&lt;/li&gt; &lt;li&gt;英语水平&lt;/li&gt; &lt;li&gt;面试技巧&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/body&gt; 我们本以为这些li，会分为两排，但是，第二组中的第1个li，去贴靠第一组中的最后一个li了。 原因就是因为div没有高度，不能给自己浮动的孩子们，一个容器。 给浮动的元素的祖先加高度如果一个元素要浮动，那么它的祖先元素一定要有高度。高度的盒子，才能关住浮动。 clear:both网页制作中，高度height很少出现。为什么？因为能被内容撑高！那也就是说，刚才我们讲解的方法1，工作中用的很少。 1clear:both clear就是清除，both指的是左浮动、右浮动都要清除。意思就是：清除别人对我的影响。 这种方法有一个非常大的、致命的问题，margin失效了。 overflow:hiddenoverflow就是“溢出”的意思， hidden就是“隐藏”的意思。 本意就是清除溢出到盒子外面的文字。但是，前端开发工程师又发现了，它能做偏方 一个父亲不能被自己浮动的儿子，撑出高度。但是，只要给父亲加上overflow:hidden;那么，父亲就能被儿子撑出高了。这是一个偏方。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈CSS(整体感知、常见属性、选择器)]]></title>
    <url>%2F2018%2F07%2F10%2Fcss2%2F</url>
    <content type="text"><![CDATA[今天来接着来学习CSS，go！本节内容有CSS整体感知、常见属性、选择器。 CSS整体感知css是cascading style sheet 层叠式样式表的简写。 我们写css的地方是style标签，就是“样式”的意思，写在head里面。 css也可以写在单独的文件里面，现在我们先写在style标签里面。 12&lt;style type="text/css"&gt;&lt;/style&gt; css对换行不敏感，对空格也不敏感。但是一定要有标准的语法。冒号，分号都不能省略。 语法： 12345678选择器&#123; k:v; k:v;&#125;选择器&#123; k:v; k:v;&#125; 一些常见的属性 字符颜色： 1color:red color属性的值，可以是英语单词，比如red、blue、yellow等等；也可以是rgb、十六进制，不要着急，后几天讲。 sublime中的快捷键是c，然后tab 字号大小： 1font-size:40px font就是“字体”，size就是“尺寸”。px是“像素”。 单位必须加，不加不行。 sublime中的快捷键是fos，然后tab 背景颜色： 1background-color:blue background就是“背景”。 sublime中的快捷键是bgc，然后tab 加粗： 1font-weight:bold font是“字体” weight是“重量”的意思，bold粗。 sublime中的快捷键是fwb，然后tab 不加粗 1font-weight:normal; normal就是正常的意思 sublime中的快捷键是fwn，然后tab 斜体： 1font-style:italic; italic就是“斜体” sublime中的快捷键是fsi，然后tab 不斜体： 1font-style:normal; sublime中的快捷键是fsn，然后tab 下划线： 1text-decoration:underline; decoration就是“装饰”的意思。 sublime中的快捷键是tdu，然后tab 没有下划线 1text-decoration:none; sublime中的快捷键是tdn，然后tab css没啥难的，就是要把属性给记忆准确。 基础选择器css怎么学？很简单有两个知识部分： 1）选择器，怎么选； 2）属性，样式是什么 标签选择器1）所有的标签，都可以是选择器。比如ul、li、label、dt、dl、input 2）无论这个标签藏的多深，一定能够被选择上 3）选择的所有，而不是一个。 标签选择器，选择的是页面上所有这种类型的标签，所以经常描述“共性”，无法描述某一个元素的“个性“ id选择器1&lt;p id="para1"&gt;我是段落1&lt;/p&gt; css: 123#para1&#123; color:green; &#125; id选择器的选择符是“#”。 任何的HTML标签都可以有id属性。表示这个标签的名字。 这个标签的名字，可以任取，但是： 1） 只能有字母、数字、下划线 2） 必须以字母开头 3） 不能和标签同名。比如id不能叫做body、img、a 一个HTML页面，不能出现相同的id，哪怕他们不是一个类型。比如页面上有一个id为pp的p，一个id为pp的div，是非法的！ 一个标签可以被多个css选择器选择，共同作用，这就是“层叠式”的第一层含义。 类选择器.就是类的符号。类的英语叫做class。 所谓的类，就是class属性，class属性和id非常相似，任何的标签都可以携带class属性。class属性可以重复。 123456&lt;h3&gt;我是一个h3啊&lt;/h3&gt;&lt;h3 class="teshu"&gt;我是一个h3啊&lt;/h3&gt;&lt;h3&gt;我是一个h3啊&lt;/h3&gt;&lt;p&gt;我是一个段落啊&lt;/p&gt;&lt;p class="teshu"&gt;我是一个段落啊&lt;/p&gt;&lt;p class="teshu"&gt;我是一个段落啊&lt;/p&gt; CSS 123.teshu&#123; color:red;&#125; 同一个标签，可能同时属于多个类，用空格隔开 1&lt;h3 class="teshu zhongyao"&gt;我是一个h3&lt;/h3&gt; CSS高级选择器后代选择器123.div p&#123; color:red;&#125; 空格就表示后代，.div p 就是.div1的后代所有的p。 强调一下，选择的是后代，不一定是儿子。 交集选择器123h3.special&#123; color:red;&#125; 选择的元素是同时满足两个条件：必须是h3标签，然后必须是special标签。 交集选择器没有空格。 交集选择器，我们一般都是以标签名开头，比如div.haha 比如p.special。 并集选择器（分组选择器）123h3,li&#123; color:red;&#125; 用逗号就表示并集。 通配符**就表示所有元素 123*&#123; color:red;&#125; 效率不高，如果页面上的标签越多，效率越低，所以页面上不能出现这个选择器。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈CSS（列表、div、span）]]></title>
    <url>%2F2018%2F07%2F10%2Fcss1%2F</url>
    <content type="text"><![CDATA[学习CSS！ yeah，今天要开始学习CSS了，本节内容有列表、div和span、表单。 列表列表有3种 无序列表无序列表，用来表示一个列表的语义，并且每个项目和每个项目之间，是不分先后的。 ul就是英语unordered list，“无序列表”的意思。 li 就是英语list item ，“列表项”的意思。 你会发现，这是我们学习的第一个“组标签”，就是要么不写，要么就要写一组。 12345&lt;ul&gt; &lt;li&gt;上海&lt;/li&gt; &lt;li&gt;北京&lt;/li&gt; &lt;li&gt;深圳&lt;/li&gt;&lt;/ul&gt; 也就是说，所有的li不能单独存在，必须包裹在ul里面；反过来说，ul的“儿子”不能是别的东西，只能有li。 但是li是一个容器级标签，li里面什么都能放，比如： 1234567891011121314151617181920&lt;body&gt; &lt;h3&gt;习大大专著&lt;/h3&gt; &lt;ul&gt; &lt;li&gt; &lt;h4&gt;习近平谈治国理政&lt;/h4&gt; &lt;p&gt;¥49.00&lt;/p&gt; &lt;p&gt;《习近平谈治国理政》谈中国、论世界，为各国读者开启了一扇观察和感知中国的窗口。阅读这本书，可以了解以习近平同志为总书记的党中央治国理念和执政方略，品味悠长醇厚的中国历史文化，感受当&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;h4&gt;习近平用典&lt;/h4&gt; &lt;p&gt;¥23.60&lt;/p&gt; &lt;p&gt;人民日报社社长杨振武主持编写并作序，人民日报社副总编辑卢新宁组织撰写解读文字，旨在对习近平总书记重要讲话（文章）引用典故的现实意义进行解读，对典故的背景义&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;h4&gt;摆脱贫困&lt;/h4&gt; &lt;p&gt;26.00&lt;/p&gt; &lt;p&gt;追本溯源 融会贯通 深入学习贯彻习近平总书记系列重要讲话精神 推动学习贯彻向广度深度拓展 习近平总书记**部个人专著 时隔22年后重印发行&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/body&gt; 甚至于可以再放一个ul： 123456789101112131415161718192021222324252627282930313233&lt;body&gt; &lt;h3&gt;去超市要买的东西&lt;/h3&gt; &lt;ul&gt; &lt;li&gt; 吃的 &lt;ul&gt; &lt;li&gt;饼干&lt;/li&gt; &lt;li&gt;面包&lt;/li&gt; &lt;li&gt; 巧克力 &lt;ul&gt; &lt;li&gt;德芙&lt;/li&gt; &lt;li&gt;费列罗&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; 用的 &lt;ul&gt; &lt;li&gt;笔记本&lt;/li&gt; &lt;li&gt;圆珠笔&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; 喝的 &lt;ul&gt; &lt;li&gt;牛奶&lt;/li&gt; &lt;li&gt;可乐&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/body&gt; 有序列表ordered list 有序列表，用ol表示 12345678&lt;body&gt; &lt;h4&gt;中国歌曲排行榜&lt;/h4&gt; &lt;ol&gt; &lt;li&gt;小苹果&lt;/li&gt; &lt;li&gt;月亮之上&lt;/li&gt; &lt;li&gt;最炫民族风&lt;/li&gt; &lt;/ol&gt;&lt;/body&gt; 浏览器会自动增加阿拉伯序号： 也就是说，ol和ul就是语义不一样，怎么使用都是一样的。 ol里面只能有li，li必须被ol包裹。li是容器级。 ol这个东西用的不多，如果想表达顺序，大家一般也用ul： 12345678&lt;body&gt; &lt;h4&gt;中国歌曲排行榜&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;1. 小苹果&lt;/li&gt; &lt;li&gt;2. 月亮之上&lt;/li&gt; &lt;li&gt;3. 最炫民族风&lt;/li&gt; &lt;/ul&gt;&lt;/body&gt; 定义列表定义列表也是一个组标签，不过比较复杂，出现了三个标签： dl表示definition list 定义列表 dt表示definition title 定义标题 dd表示definition description 定义表述词儿 dt、dd只能在dl里面；dl里面只能有dt、dd 123456789101112131415161718body&gt; &lt;h3&gt;中国主要城市&lt;/h3&gt; &lt;dl&gt; &lt;dt&gt;北京&lt;/dt&gt; &lt;dd&gt;国家首都，政治文化中心&lt;/dd&gt; &lt;dd&gt;污染很严重，PM2.0天天报表&lt;/dd&gt; &lt;/dl&gt; &lt;dl&gt; &lt;dt&gt;上海&lt;/dt&gt; &lt;dd&gt;魔都，有外滩、东方明珠塔、黄浦江&lt;/dd&gt; &lt;/dl&gt; &lt;dl&gt; &lt;dt&gt;广州&lt;/dt&gt; &lt;dd&gt;中国南大门，有珠江、小蛮腰&lt;/dd&gt; &lt;/dl&gt;&lt;/body&gt; 表达的语义是两层： 1) 是一个列表，列出了北京、上海、广州三个项目 2）每一个词儿都有自己的描述项。 dd是描述dt的。 div和spandiv和span是非常重要的标签，div的语义是division“分割”； span的语义就是span“范围、跨度”。 1234567891011121314151617181920&lt;body&gt; &lt;div&gt; &lt;h3&gt;中国主要城市&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;北京&lt;/li&gt; &lt;li&gt;上海&lt;/li&gt; &lt;li&gt;广州&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div&gt; &lt;h3&gt;美国主要城市&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;纽约&lt;/li&gt; &lt;li&gt;洛杉矶&lt;/li&gt; &lt;li&gt;华盛顿&lt;/li&gt; &lt;li&gt;西雅图&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/body&gt; div在浏览器中，默认是不会增加任何的效果改变的，但是语义变了，div中的所有元素是一个小区域。 div标签是一个容器级标签，里面什么都能放，甚至可以放div自己。 span也是表达“小区域、小跨度”的标签，但是是一个“文本级”的标签。 就是说，span里面只能放置文字、图片、表单元素。 span里面不能放p、h、ul、dl、ol、div。 span里面是放置小元素的，div里面放置大东西的： 1234567&lt;p&gt; 简介简介简介简介简介简介简介简介 &lt;span&gt; &lt;a href=""&gt;详细信息&lt;/a&gt; &lt;a href=""&gt;购买&lt;/a&gt; &lt;/span&gt;&lt;/p&gt; div标签是最最重要的布局标签。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Document&lt;/title&gt;&lt;head&gt; &lt;style type="text/css"&gt; .header&#123; width: 980px; height:100px; margin: 0 auto; margin-bottom: 10px; &#125; .content&#123; width: 980px; height: 400px; margin: 0 auto; margin-bottom: 10px; &#125; .footer&#123; width: 980px; height:100px; margin: 0 auto; background-color: yellow; margin-bottom: 10px; &#125; .logo&#123; float: left; width: 200px; height: 60px; background-color: red; &#125; .nav&#123; float: right; width: 600px; height: 60px; background-color: blue; &#125; .guanggao&#123; float: left; width: 250px; height: 400px; background-color: darkblue; &#125; .dongxi&#123; float: right; width: 690px; height: 400px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="header"&gt; &lt;div class="logo"&gt;这就是logo&lt;/div&gt; &lt;div class="nav"&gt;导航条&lt;/div&gt; &lt;/div&gt; &lt;div class="content"&gt; &lt;div class="guanggao"&gt;广告&lt;/div&gt; &lt;div class="dongxi"&gt;卖的东西&lt;/div&gt; &lt;/div&gt; &lt;div class="footer"&gt;页脚&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 所以，我们亲切的称呼这种模式叫做“div+css”。div标签负责布局，负责结构，负责分块。css负责样式。 表单表单就是收集用户信息的，就是让用户填写的、选择的。 123456&lt;div&gt; &lt;h3&gt;欢迎注册本网站&lt;/h3&gt; &lt;form&gt; 所有的表单内容，都要写在form标签里面 &lt;/form&gt;&lt;/div&gt; form是英语表单的意思。form标签里面有action属性和method属性，action属性就是表示，表单将提交到哪里。method属性表示用什么HTTP方法提交，有get、post两种。 文本框1&lt;input type="text"&gt; input表示“输入”，指的是这是一个“输入小部件”， type表示“类型”， text表示“文本”，指的是类型是一个文本框的输入小部件。 这是一个自封闭标签。 value表示“值”，value属性就是默认有的值，文本框中已经被填写好了： 1234&lt;p&gt; 请输入您的姓名: &lt;input type="text" value="默认有的值，(*^__^*) 嘻嘻……" /&gt;&lt;/p&gt; 密码框也就是说，input标签很神奇，又是文本框，又是密码框。 到底是啥？看type属性的值是什么，来决定。 如果type=”text” 文本框 如果type=”password” 密码框 1234&lt;p&gt; 请输入您的密码： &lt;input type="password" /&gt;&lt;/p&gt; 单选按钮只能选一个，术语叫做“互斥”。 12345&lt;p&gt; 请选择你的性别： &lt;input type="radio" name="xingbie" /&gt; 男 &lt;input type="radio" name="xingbie" /&gt; 女&lt;/p&gt; radio是“收音机”的意思，input的type的值，如果是radio就是单选按钮。 非常像以前的收音机，按下去一个按钮，其他的就抬起来了。所以叫做radio。 单选按钮，天生是不能互斥的，如果想互斥，必须要有相同的name属性。name就是“名字”。 默认被选择，就应该书写checked=”checked” 1&lt;input type="radio" name="sex" checked="checked"&gt; 复选框也是input标签，type是checkbox。 123456789&lt;p&gt; 请选择你的爱好： &lt;input type="checkbox" name="aihao"/&gt; 睡觉 &lt;input type="checkbox" name="aihao"/&gt; 吃饭 &lt;input type="checkbox" name="aihao"/&gt; 足球 &lt;input type="checkbox" name="aihao"/&gt; 篮球 &lt;input type="checkbox" name="aihao"/&gt; 乒乓球 &lt;input type="checkbox" name="aihao"/&gt; 打豆豆&lt;/p&gt; 复选框，最好也是有相同的name（虽然他不需要互斥，但是也要有相同的name） 下拉列表select就是“选择”，option“选项”。 select标签和ul、ol、dl一样，都是组标签。 123456789101112&lt;p&gt; 请选择你的籍贯： &lt;select&gt; &lt;option&gt;北京&lt;/option&gt; &lt;option&gt;河北&lt;/option&gt; &lt;option&gt;河南&lt;/option&gt; &lt;option&gt;山东&lt;/option&gt; &lt;option&gt;山西&lt;/option&gt; &lt;option&gt;湖北&lt;/option&gt; &lt;option&gt;安徽&lt;/option&gt; &lt;/select&gt;&lt;/p&gt; 多行文本框（文本域）text就是“文本”，area就是“区域”。 1234&lt;p&gt; 签名： &lt;textarea rows="4" cols="80"&gt;&lt;/textarea&gt;&lt;/p&gt; 这个标签，是个标签对儿。对儿里面不用写东西。如果写的话，就是这个框的默认文字。 cols属性表示columns“列”，rows属性表示rows“行”。 值就是一个数，表示多少行，多少列。 三种按钮按钮也是input标签，一共有三种按钮： 普通按钮： 1234&lt;p&gt; 普通按钮： &lt;input type="button" value="我是一个普通按钮" /&gt;&lt;/p&gt; button就是英语“按钮”的意思。value就是“值”的意思，按钮上面显示的文字。 提交按钮： 1234&lt;p&gt; 提交按钮： &lt;input type="submit" /&gt;&lt;/p&gt; ubmit就是英语“提交”的意思。这个按钮不需要写value自动就有“提交”文字。 这个按钮点击，表单真的能提交。这个表单就会被提交到，form标签的action属性中的那个页面中去。 重置按钮 1234&lt;p&gt; 重置按钮 &lt;input type="reset" /&gt;&lt;/p&gt; reset就是“复位”的意思。 label标签12&lt;input type="radio" name="sex" id="nan" /&gt; &lt;label for="nan"&gt;男&lt;/label&gt;&lt;input type="radio" name="sex" id="nv" /&gt; &lt;label for="nv"&gt;女&lt;/label&gt; input元素要有一个id，然后label标签就有一个for属性，和id相同，就表示绑定了，这个label和input就有绑定关系了。 复选框也有label： 12&lt;input type="checkbox" id="kk" /&gt;&lt;label for="kk"&gt;10天内免登陆&lt;/label&gt; 什么表单元素都有label。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML随笔]]></title>
    <url>%2F2018%2F05%2F01%2Fhtml2%2F</url>
    <content type="text"><![CDATA[​ 本节介绍的内容有HTML初始、HTML骨架、HTML的基本语法特性、h和p标签、图片、超级链接。 HTML初识网页的原理用户输入网址之后，对应的服务器就发现有人请求我的网页了，所以这个服务器就会把网页和相关的图片、js文件、css文件、flash文件都通过HTTP协议传输到用户的电脑里面。HTML页面在用户的电脑里面进行渲染。HTTP协议指的是超文本传输协议。每一个网址，都对应了服务器上面的确定的文件。 纯文本文件只有内容，没有样式。常见的纯文本文件有：.txt、.html、.js、.css、.java。没有语义，即使你这个文件中的内容排版再清晰，那也是人认为的，实际上纯文本文件中的所有的文字生而平等，没有任何的语义。 HTML超文本标记语言，HyperText Markup Language。就是通过标签对儿，给纯文本增加语义。也就是说，用文本给文本增加语义，所以这个叫做“超文本”。而有一对儿对儿标签，也成为“标记”，所以就是“超文本标记语言”。 .html.html网页的格式，现在的业界的标准，网页技术严格的三层分离：html就是负责描述页面的语义；css负责描述页面的样式；js负责描述页面的动态效果的。 HTML骨架12345678910&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;/body&gt;&lt;/html&gt; 文档声明头任何一个标准的HTML页面，第一行一定是一个以 1&lt;!DOCTYPE ... 字符集1&lt;meta charset="UTF-8"&gt; 字符集用meta标签定义，meta表示“元”。“元”配置，就是表示基本的配置项目。 charset就是charactor set“字符集”的意思。 中文能够使用的字符集两种： 第一种：UTF-8 1&lt;meta charset="UTF-8"&gt; 第二种：gb2312 1&lt;meta charset="gb2312"&gt; 也可以写成gbk 1&lt;meta charset="gbk"&gt; 规模： UTF-8（字全） &gt; gb2312（只有汉字） 我们用meta标签可以声明当前这个html文档的字库，但是一定要和保存的类型一样，否则乱码！（重点） 当我们不设置的时候，sublime默认类型就是UTF-8。而一旦更改为gb2312的时候，就一定要记得设置一下sublime的保存类型： 文件→ set File Encoding to →Chinese Simplified(GBK) 记住两者匹配： 注意，由于UTF-8里面保存了世界上所有人类语言，所以描述一个汉字需要的码更多。 UTF-8里面存储一个汉字3个字节。而gb2312中存储一个汉字2个字节。 保存大小： UTF-8（更臃肿、加载更慢） &gt; gb2312 （更小巧，加载更快） 总结： UTF-8 字多，有各种国家的语言，但是保存尺寸大，文件臃肿； gb2312字少，只用中文和少数外语和符号，但是尺寸小，文件小巧。 title标签1&lt;title&gt;网页的标题&lt;/titile&gt; title也是有助于SEO搜索引擎优化的 HTML的基本语法特性HTML对换行不敏感，对tab不敏感HTML只在乎标签的嵌套结构，嵌套的关系。谁嵌套了谁，谁被谁嵌套了，和换行、tab无关。 换不换行、tab不tab，都不影响页面的结构。 所以： 1234 &lt;div&gt;&lt;h3&gt;&lt;/h3&gt; &lt;p&gt;&lt;/p&gt; &lt;/div&gt; 完全等价于： 1234&lt;div&gt; &lt;h3&gt;&lt;/h3&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt; 就是说，HTML不是依靠缩进来表示嵌套的，就是看标签的包裹关系。但是，我们发现有良好的缩进，代码更易读。要求大家都正确缩进标签。 空白折叠现象HTML中所有的文字之间，如果有空格、换行、tab都将被折叠为一个空格显示。 代码：里面有空格、缩进、换行 123&lt;p&gt;clay wangzhi&lt;/p&gt; 标签要严格封闭标签不封闭是灾难的： 1&lt;title&gt;欢迎&lt;title&gt; HTML、CSS就是写代码，不能算“编程”，因为这里面没有业务逻辑，加减乘除，与或非。 说白了，就是用代码画画。 h和p标签h系列123456789&lt;h1&gt; 到 &lt;h6&gt; 都是标签：&lt;h1&gt;&lt;/h1&gt;一级标题&lt;h2&gt;&lt;/h2&gt;二级标题……&lt;h6&gt;&lt;/h6&gt;六级标题 h标签没有嵌套关系的。由于h3跟着一个h2，所以，我们自己就知道了这个h3是h2子标题。 h是容器级的标签。理论上里面可以放置p、ul，只是法律上的允许，在语义上，不要这么写。 p标签段落，是英语paragraph“段落”缩写。 HTML标签是分等级的，HTML将所有的标签分为两种：容器级、文本级。 顾名思义，容器级的标签，里面可以放置任何东西；文本级的标签里面，只能放置文字、图片、表单元素。 p标签是一个文本级标签。从学习p的第一天开始，就要死死记住：p里面只能放文字、图片、表单元素。其他的一律不能放。 Chrome浏览器是世界上HTML5支持最好的浏览器。提供了非常好的开发工具，非常适合我们开发人员使用。审查元素功能的快捷键是F12。 Chrome的英语原意“硅”，是谷歌公司的产品，所以也叫作谷歌浏览器。 图片能用的图片类型页面上可以插入图片，能够插入的图片类型是：jpg(jpeg)、gif、png、bmp。类型和类型之间有什么区别。 不能往网页中插入的图片格式是：psd、ai。 语法HTML页面不是直接插入图片，而是插入图片的引用地址，所以也要把图片上传到服务器上。 插入方法： 1&lt;img src="baby.jpg"&gt; img是英语image“图片”的简写 src 是英语source“资源”的简写，千万不要写成scr。 src是img标签的属性，baby.jpg是这个属性的值。 这个标签和我们之前学的，都不一样，因为这个标签不是“对儿”。自封闭标签，也称为单标签。 为什么？原因很简单，如果是对儿，里面就要有内容，表示给这个内容增加语义。比如 1"&lt;h1&gt;哈哈哈哈，我是主标题啦！！！&lt;/h1&gt;" 图片就是一个图片，不需要给什么文字增加语义，所以就是自己一个人活得挺好： 1"&lt;img /&gt;" meta也是自封闭标签： 1&lt;meta charset="UTF-8" /&gt; alt属性alt属性： 1&lt;img src="baby.jpg" alt="巴黎结婚照" /&gt; alt是英语alternate“替代”的意思，就表示不管因为什么原因，当这个图片无法被显示的时候，出现的替代文字（有的浏览器不支持）。 超级链接基本写法一个网站，是由很多html网页组成的，html网页之间能够通过超级链接互相跳转，从而形成了“网”。 语法： 1&lt;a href="1.html"&gt;结婚照&lt;/a&gt; 效果图： a是英语anchor“锚”的意思，就好像这个页面往另一个页面扔出了一个锚。是一个文本级的标签。 href是英语hypertext reference超文本地址的缩写。读作“喝瑞夫”。 a标签的另外两个属性title 悬停文本 1&lt;a href="09_img.html" title="很好看哦"&gt;结婚照&lt;/a&gt; target 是否在新窗口中打开 target实际上是“目标”的意思。 1&lt;a href="09_img.html" title="很好看哦" target="_blank"&gt;结婚照&lt;/a&gt; blank就是“空白”的意思，就表示新建一个空白窗口。 也就是说，如果不写target=”_blank”那么就是在相同的标签页打开，如果写了，就是在新的空白标签页中打开。 完整的超级链接： 1&lt;a href="1.html" title="悬停文本" target="_blank"&gt;链接的内容&lt;/a&gt; 页面内的锚点页面当中可以有锚点，所谓的锚点，就是一个小标记，这个小标记是用户不可察觉的，用户不知道这里有一个标记。 锚点用name属性来设置，一个a标签如果name属性（或者id属性），那么就是页面的一个锚点。 123&lt;a name="wdzp"&gt;我的作品&lt;/a&gt;或者：&lt;a id="wdzp"&gt;我的作品&lt;/a&gt; 页面内锚点 12345678910111213&lt;body&gt; &lt;a href="#gzjy"&gt;点击我就查看工作经验&lt;/a&gt; &lt;a href="#wdzp"&gt;点击我就查看我的作品&lt;/a&gt; &lt;h1&gt;个人简介&lt;/h1&gt; &lt;h2&gt;基本信息&lt;/h2&gt; &lt;p&gt;基本信息&lt;/p&gt; &lt;h2&gt;&lt;a id="gzjy"&gt;工作经验&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;工作经验&lt;/p&gt; &lt;h2&gt;&lt;a name="wdzp"&gt;我的作品&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;作品展示&lt;/p&gt;&lt;/body&gt; 跨页面锚点 123&lt;body&gt; &lt;a href="11_页面内锚点.html#gzjy"&gt;查看小明同学的工作经验&lt;/a&gt;&lt;/body&gt; a是一个文本级的标签比如一个段落中的所有文字都能够被点击，那么应该： p包裹a： 123&lt;p&gt; &lt;a href=""&gt;段落&lt;/a&gt;&lt;/p&gt; 而不是a包裹p： 12345&lt;a href=""&gt; &lt;p&gt; 段落 &lt;/p&gt;&lt;/a&gt; a的语义要小于p，a就是可以当做文本来处理，所以p里面相当于放的就是纯文字。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>HTML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一键安装mysql脚本]]></title>
    <url>%2F2017%2F11%2F30%2Fmysql4%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202#!/bin/bash# Notes: install mysql5.6 on centos#mysql_install_dir=/opt/mysql #程序目录mysql_data_dir=/opt/mysql/data #数据目录mysql_6_version=5.6.36 #更改文件名dbrootpwd=1qazxsw2 #mysql密码Mem=`free -m | awk '/Mem:/&#123;print $2&#125;'`Swap=`free -m | awk '/Swap:/&#123;print $2&#125;'`Install_MySQL()&#123;yum -y install make gcc-c++ cmake bison-devel ncurses-devel autoconfwget http://mirrors.sohu.com/mysql/MySQL-5.6/mysql-$&#123;mysql_6_version&#125;.tar.gzid -u mysql &gt;/dev/null 2&gt;&amp;1[ $? -ne 0 ] &amp;&amp; useradd -M -s /sbin/nologin mysqlmkdir -p $mysql_data_dir;chown mysql.mysql -R $mysql_data_dirtar zxf mysql-$&#123;mysql_6_version&#125;.tar.gzcd mysql-$mysql_6_versionmake clean[ ! -d "$mysql_install_dir" ] &amp;&amp; mkdir -p $mysql_install_dircmake . -DCMAKE_INSTALL_PREFIX=$mysql_install_dir \-DMYSQL_DATADIR=$mysql_data_dir \-DSYSCONFDIR=/etc \-DWITH_INNOBASE_STORAGE_ENGINE=1 \-DWITH_PARTITION_STORAGE_ENGINE=1 \-DWITH_FEDERATED_STORAGE_ENGINE=1 \-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \-DWITH_MYISAM_STORAGE_ENGINE=1 \-DWITH_INNOBASE_STORAGE_ENGINE=1 \-DENABLED_LOCAL_INFILE=1 \-DENABLE_DTRACE=0 \-DEXTRA_CHARSETS=all \-DDEFAULT_CHARSET=utf8mb4 \-DDEFAULT_COLLATION=utf8mb4_general_ci \-DWITH_EMBEDDED_SERVER=1 \make -j `grep processor /proc/cpuinfo | wc -l`make installif [ -d "$mysql_install_dir/support-files" ];then echo "$&#123;CSUCCESS&#125;MySQL install successfully! $&#123;CEND&#125;" cd .. rm -rf mysql-$mysql_6_versionelse rm -rf $mysql_install_dir echo "$&#123;CFAILURE&#125;MySQL install failed, Please contact the author! $&#123;CEND&#125;" kill -9 $$fi/bin/cp $mysql_install_dir/support-files/mysql.server /etc/init.d/mysqldchmod +x /etc/init.d/mysqldchkconfig mysqld oncd ..# my.cf[ -d "/etc/mysql" ] &amp;&amp; /bin/mv /etc/mysql&#123;,_bk&#125;cat &gt; /etc/my.cnf &lt;&lt; EOF[client]port = 3306socket = /tmp/mysql.sockdefault-character-set = utf8[mysqld]port = 3306socket = /tmp/mysql.sockbasedir = $mysql_install_dirdatadir = $mysql_data_dirpid-file = $mysql_data_dir/mysql.piduser = mysqlbind-address = 0.0.0.0server-id = 1init-connect = 'SET NAMES utf8mb4'character-set-server = utf8mb4skip-name-resolveskip-external-locking#skip-networkingback_log = 300max_connections = 1000max_connect_errors = 6000open_files_limit = 65535table_open_cache = 128max_allowed_packet = 4Mbinlog_cache_size = 1Mmax_heap_table_size = 8Mtmp_table_size = 16Mread_buffer_size = 2Mread_rnd_buffer_size = 8Msort_buffer_size = 8Mjoin_buffer_size = 8Mkey_buffer_size = 4Mthread_cache_size = 8query_cache_type = 1query_cache_size = 8Mquery_cache_limit = 2Mft_min_word_len = 4log_bin = mysql-binbinlog_format = mixedexpire_logs_days = 10log_error = $mysql_data_dir/mysql-error.logslow_query_log = 1long_query_time = 1#slow_query_log_file = $mysql_data_dir/mysql-slow.logperformance_schema = 0explicit_defaults_for_timestamp#lower_case_table_names = 1default_storage_engine = InnoDB#default-storage-engine = MyISAMinnodb_file_per_table = 1innodb_open_files = 500innodb_buffer_pool_size = 64Minnodb_write_io_threads = 4innodb_read_io_threads = 4innodb_thread_concurrency = 0innodb_purge_threads = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 2Minnodb_log_file_size = 32Minnodb_log_files_in_group = 3innodb_max_dirty_pages_pct = 90innodb_lock_wait_timeout = 120bulk_insert_buffer_size = 8Mmyisam_sort_buffer_size = 8Mmyisam_max_sort_file_size = 10Gmyisam_repair_threads = 1interactive_timeout = 28800wait_timeout = 28800[mysqldump]quickmax_allowed_packet = 16M[myisamchk]key_buffer_size = 8Msort_buffer_size = 8Mread_buffer = 4Mwrite_buffer = 4MEOFif [ $Mem -gt 1500 -a $Mem -le 2500 ];then sed -i 's@^thread_cache_size.*@thread_cache_size = 16@' /etc/my.cnf sed -i 's@^query_cache_size.*@query_cache_size = 16M@' /etc/my.cnf sed -i 's@^myisam_sort_buffer_size.*@myisam_sort_buffer_size = 16M@' /etc/my.cnf sed -i 's@^key_buffer_size.*@key_buffer_size = 16M@' /etc/my.cnf sed -i 's@^innodb_buffer_pool_size.*@innodb_buffer_pool_size = 128M@' /etc/my.cnf sed -i 's@^tmp_table_size.*@tmp_table_size = 32M@' /etc/my.cnf sed -i 's@^table_open_cache.*@table_open_cache = 256@' /etc/my.cnfelif [ $Mem -gt 2500 -a $Mem -le 3500 ];then sed -i 's@^thread_cache_size.*@thread_cache_size = 32@' /etc/my.cnf sed -i 's@^query_cache_size.*@query_cache_size = 32M@' /etc/my.cnf sed -i 's@^myisam_sort_buffer_size.*@myisam_sort_buffer_size = 32M@' /etc/my.cnf sed -i 's@^key_buffer_size.*@key_buffer_size = 64M@' /etc/my.cnf sed -i 's@^innodb_buffer_pool_size.*@innodb_buffer_pool_size = 512M@' /etc/my.cnf sed -i 's@^tmp_table_size.*@tmp_table_size = 64M@' /etc/my.cnf sed -i 's@^table_open_cache.*@table_open_cache = 512@' /etc/my.cnfelif [ $Mem -gt 3500 ];then sed -i 's@^thread_cache_size.*@thread_cache_size = 64@' /etc/my.cnf sed -i 's@^query_cache_size.*@query_cache_size = 64M@' /etc/my.cnf sed -i 's@^myisam_sort_buffer_size.*@myisam_sort_buffer_size = 64M@' /etc/my.cnf sed -i 's@^key_buffer_size.*@key_buffer_size = 256M@' /etc/my.cnf sed -i 's@^innodb_buffer_pool_size.*@innodb_buffer_pool_size = 1024M@' /etc/my.cnf sed -i 's@^tmp_table_size.*@tmp_table_size = 128M@' /etc/my.cnf sed -i 's@^table_open_cache.*@table_open_cache = 1024@' /etc/my.cnffi$mysql_install_dir/scripts/mysql_install_db --user=mysql --basedir=$mysql_install_dir --datadir=$mysql_data_dirchown mysql.mysql -R $mysql_data_dirservice mysqld start[ -z "`grep ^'export PATH=' /etc/profile`" ] &amp;&amp; echo "export PATH=$mysql_install_dir/bin:\$PATH" &gt;&gt; /etc/profile[ -n "`grep ^'export PATH=' /etc/profile`" -a -z "`grep $mysql_install_dir /etc/profile`" ] &amp;&amp; sed -i "s@^export PATH=\(.*\)@export PATH=$mysql_install_dir/bin:\1@" /etc/profile. /etc/profile$mysql_install_dir/bin/mysql -e "grant all privileges on *.* to root@'127.0.0.1' identified by \"$dbrootpwd\" with grant option;"$mysql_install_dir/bin/mysql -e "grant all privileges on *.* to root@'localhost' identified by \"$dbrootpwd\" with grant option;"$mysql_install_dir/bin/mysql -uroot -p$dbrootpwd -e "delete from mysql.user where Password='';"$mysql_install_dir/bin/mysql -uroot -p$dbrootpwd -e "delete from mysql.db where User='';"$mysql_install_dir/bin/mysql -uroot -p$dbrootpwd -e "delete from mysql.proxies_priv where Host!='localhost';"$mysql_install_dir/bin/mysql -uroot -p$dbrootpwd -e "drop database test;"$mysql_install_dir/bin/mysql -uroot -p$dbrootpwd -e "reset master;"rm -rf /etc/ld.so.conf.d/&#123;mysql,mariadb,percona&#125;*.confecho "$mysql_install_dir/lib" &gt; mysql.conf/sbin/ldconfigservice mysqld stop&#125;Install_MySQL]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql常见的几个错误问题及解决方法]]></title>
    <url>%2F2017%2F11%2F29%2Fmysql3%2F</url>
    <content type="text"><![CDATA[Mysql常见的几个错误问题及解决方法： mysql DNS反解：skip-name-resolve错误日志有类似警告： 120119 16:26:04 [Warning] IP address ‘192.168.1.10’ could not be resolved: Name or service not known 120119 16:26:04 [Warning] IP address ‘192.168.1.14’ could not be resolved: Name or service not known 120119 16:26:04 [Warning] IP address ‘192.168.1.17’ could not be resolved: Name or service not known 通过show processlist发现大量类似如下的连接： |592|unauthenticated user|192.168.1.10:35320|NULL|Connect| |login|NULL| |593|unauthenticated user|192.168.1.14:35321|NULL|Connect| |login|NULL| |594|unauthenticated user|192.168.1.17:35322|NULL|Connect| |login|NULL| skip-name-resolve 参数的作用：不再进行反解析（ip不反解成域名），这样可以加快数据库的反应时间。 修改配置文件添加并需要重启：点击(此处)折叠或打开 [mysqld] skip-name-resolve 问题错误日志：Error: Can’t create a new thread (errno 12)数据库服务器问题，数据库操作无法创建新线程。一般是有以下3个方面的原因：1）、MySQL 线程开得太多。2）、服务器系统内存溢出。3）、环境软件损坏或系统损坏。 【问题解决】 1）进入 phpmyadmin 的 mysql 数据库中的 user 表，对数据库的用户进行编辑，修改 max_connections 的值。适当的改小一点。 2）联系服务器管理员检查服务器的内存和系统是否正常，如果服务器内存紧张，请检查一下哪些进程消耗了服务器的内存，同时考虑是否增加服务器的内存来提高整个系统的负载能力。 3）mysql版本更改为稳定版本 4）优化网站程序的sql等等 操作报错：ERROR 1010 (HY000): Error dropping database mysql&gt; drop database xjtrace; ERROR 1010 (HY000): Error dropping database (can’t rmdir ‘./xjtrace/ 在做数据库删除时出现这种提示，其原因是在database下面含有自己放进去的文件，譬如.txt文件或.sql文件等，只要进去把这个文件删了在执行。 mysql&gt;drop database xjtrace; Query OK, 0 rows affected (0.00 sec) 果断删除即可！！ 4.导出数据很快，导入到新库时却很慢： MySQL导出的SQL语句在导入时有可能会非常非常慢，经历过导入仅400万条记录，竟用了近2个小时。在导出时合理使用几个参数，可以大大加快导入的速度。 -e 使用包括几个VALUES列表的多行INSERT语法;–max_allowed_packet=XXX 客户端/服务器之间通信的缓存区的最大大小;–net_buffer_length=XXX TCP/IP和套接字通信缓冲区大小,创建长度达net_buffer_length的行注意：max_allowed_packet和net_buffer_length不能比目标数据库的配置数值大，否则可能出错。 首先确定目标库的参数值 mysql&gt; show variables like ‘max_allowed_packet’; +——————–+———+ | Variable_name | Value | +——————–+———+ | max_allowed_packet | 1048576 | +——————–+———+ 1 row in set (0.00 sec) mysql&gt; show variables like ‘net_buffer_length’; +——————-+——-+ | Variable_name | Value | +——————-+——-+ | net_buffer_length | 16384 | +——————-+——-+ 1 row in set (0.00 sec) 根据参数值书写mysqldump命令，如：mysql&gt;mysqldump -uroot -p 数据库名 -e –max_allowed_packet=1048576 -net_buffer_length=16384 &gt; SQL文件 例如：点击(此处)折叠或打开 mysql&gt; mysqldump -uroot -p xjtrace -e –max_allowed_packet=1048576 –net_buffer_length=16384 &gt;xjtrace_data_date +%F.sql 之前2小时才能导入的sql现在几十秒就可以完成了。k]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主库异常，从库手动切换为主库方案]]></title>
    <url>%2F2017%2F11%2F28%2Fmysql2%2F</url>
    <content type="text"><![CDATA[1.登录从服务器，确认从服务器已经完成所有同步操作：123mysql&gt; stop slave io_thread mysql&gt; show processlist 直到看到状态都为：xxx has read all relay log 表示更新都执行完毕 2.停止从服务器slave服务：1mysql&gt; stop slave 3.将从服务器切换为主服务器：1mysql&gt; reset master 完成切换 4.授权内网其他机器有写入等权限(如果没有权限的话)123mysql&gt; SELECT Host,User FROM mysql.user;mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;192.168.1.%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;mysql&gt; FLUSH PRIVILEGES; 5.修改其他机器hosts或应用内连接12# vi /etc/hosts192.168.1.106 db-001]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysqldump数据导出和source数据导入]]></title>
    <url>%2F2017%2F11%2F27%2Fmysql1%2F</url>
    <content type="text"><![CDATA[一般情况下我用Navicat软件进行数据的导入导出比较多，一下是用mysqldump进行实现 导出数据启动dos运行里面运行：cmd（1）导出数据库 ①导出所有数据库（包括系统数据库）到指定文件下 12mysqldump -u$ -p*** --all-databases &gt; D:/all.sql 注：$为用户***为用户密码（最好回车后填写）命令后面没有任何标点符号 ②导出db1和db2两个数据库的所有数据 1mysqldump -uroot -p123456 --databases db1 db2 &gt; D:/db1_2.sql ③只导出表结构不导出数据，–no-data 1mysqldum -uroot -p123456 --no-data --database db1 &gt;D:/db1_data.sql ④跨服务器导出导入数据 1mysqldump --host=192.168.80.136 -uroot -p123456 -C --database db1 |mysql --host=192.168.80.156 -uroot -p123456 db1 注意：加上-C参数可以启用压缩传递 （2）导出表​ 注意导出指定表只能针对一个数据库进行导出，且导出的内容中和导出数据库也不一样，导出指定表的导出文本中没有创建 数据库的判断语句，只有删除表-创建表-导入数据 ①导出db1中的a1、a2表 1mysqldump -uroot -p123456 --databases db1 --tables a1 a2 &gt;D:/db1_a1_a2.sql ②条件导出，导出db1表a1中字段id=1的表 1mysqldump -uroot -p123456 --databases db1 --tables a1 --where=&quot;id=&apos;1&apos;&quot; &gt; D:db1_id1.sql 注意：如果想导出“远程数据库服务器”的数据，加一个-h参数指定服务器地址即可: 1mysqldump -h125.123.125.220 -uroot -p123456 --databases db1 &gt;D:/remote_data.sql 导入数据 （1）导入数据库 登录进入mysql界面使用CREATE命令创建数据库后使用USE命令进入该数据库，然后再使用SOURCE命令即可导入数据库 注：如果sql文件中已经有数据库创建语句，则不用使用CREATE和USE命令 例如：导入数据库db1 123CREATE DATABASE db1; USE db1; SOURCE d:/dn1.sql; （2）导入数据表 登录进入mysql界面选择数据库使用SOURCE命令即可 参考博客： https://blog.csdn.net/wdg1994/article/details/51089245]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令(一)]]></title>
    <url>%2F2016%2F06%2F07%2F%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAlinux%E5%91%BD%E4%BB%A4%EF%BC%881%EF%BC%89-ls%2F</url>
    <content type="text"><![CDATA[linux常用命令： ls、 lsls命令时linux下最常用的命令。ls命令就是list的缩写，缺省下ls用来打印出当前目录的清单，如果ls指定其他目录，那么就会显示指定目录里的文件及文件夹清单。 通过ls命令不仅可以查看linux文件夹包含的文件，而且可以查看文件权限（包括目录、文件夹、文件权限），查看目录信息等等。 选项常用选项 123456789-a：显示所有档案及目录（ls内定将档案名或目录名称为“.”的视为影藏，不会列出）；-l：以长格式显示目录下的内容列表。输出的信息从左到右依次包括文件名，文件类型、权限模式、硬连接数、所有者、组、文件大小和文件的最后修改时间等；-h:–human-readable 以容易理解的格式列出文件大小 (例如 1K 234M 2G);-s：显示文件和目录的大小，以区块为单位；-t：用文件和目录的更改时间排序；-r：以文件名反序排列并输出目录内容列表；-d：仅显示目录名，而不显示目录下的内容列表。显示符号链接文件本身，而不显示其所指向的目录列表；-R：递归处理，将指定目录下的所有文件及子目录一并处理；--color[=WHEN]：使用不同的颜色高亮显示不同类型的。 常用范例 列出/opt文件夹下的所有文件和目录的详细资料 1ls -lR /opt/ 输出结果为： 123456789/opt/:total 124188-rw-r--r-- 1 root root 127163815 Aug 25 06:10 go1.11.linux-amd64.tar.gz-rw-r--r-- 1 root root 1003 Sep 11 14:16 setup.shdrwxr-xr-x 1 root root 512 Jan 28 22:17 test/opt/test:total 0-rw-r--r-- 1 root root 0 Jan 28 22:17 test.txt 列出当前目录中所有以“t”开头的目录的详细内容，可以使用如下命令： 1ls -l t* 输出结果为： 12total 0-rw-r--r-- 1 root root 0 Jan 28 22:17 test.txt 列出目前工作目录下所有名称是s 开头的档案，越新的排越后面，可以使用如下命令： 1ls -ltr s* 输出结果为： 12-rw-r--r-- 1 root root 1003 Sep 11 14:16 setup.sh-rw-r--r-- 1 root root 0 Jan 28 22:21 s.txt 颜色蓝色—–目录绿色—–可执行文件白色—–一般性文件，如文本文件，配置文件等红色—–压缩文件或归档文件浅蓝色—-链接文件红色闪烁—-链接文件存在问题黄色—–设备文件青黄色—-管道文件 参考链接： http://man.linuxde.net/ls http://www.9usb.net/201005/linux-ls.html http://www.cnblogs.com/peida/archive/2012/10/23/2734829.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux常用命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo(next)+github博客的搭建]]></title>
    <url>%2F2016%2F06%2F06%2F%E6%96%B0%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ 本文主要讲解博客的搭建过程，next主题优化，next配置文件详解等。 hexo+github的搭建过程准备安装软件依次安装 Node.js Git 配置和使用github注册github访问github.com右上角singnup uername 最好都用小写，因为最后建立的博客地址是：http://username.github.io；邮箱十分重要，GitHub 上很多通知都是通过邮箱的。 创建RepositoryRepository 名字应该是http://username.github.io。 其他的可以选择添加一些描述也可以选择默认什么也不添加 ，点击creat repository 配置SSH keys运行git bash 123$ ssh-keygen -t rsa -C "邮件地址@youremail.com"Generating public/private rsa key pair.Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):&lt;回车就好&gt; 【提示1】这里的邮箱地址，输入注册 Github 的邮箱地址； 【提示2】「-C」的是大写的「C」 然后系统会要你输入密码： 12Enter passphrase (empty for no passphrase):&lt;设置密码&gt;Enter same passphrase again:&lt;再次输入密码&gt; 在回车中会提示你输入一个密码，这个密码会在你提交项目时使用，如果为空的话提交项目时则不用输入。这个设置是防止别人往你的项目里提交内容。 注意：输入密码的时候没有输入痕迹的，不要以为什么也没有输入。 添加SSH Key到GitHub在本地文件夹找到id_rsa.pub文件，.ssh文件夹里记事本打开这个文件复制全部内容到github相应位。 测试 打开git bash，输入以下代码 1$ ssh -T git@github.com 如何返回值中有successfully字段代表配置成功了。 设置用户信息打开git bash，完善个人信息 12$ git config --global user.name "user" //输入注册时的username$ git config --global user.email "email" //填写注册邮箱 搭建hexo博客利用npm命令安装hexo 1$ npm install -g hexo 创建独立博客项目文件夹 安装完成后，关掉前面那个 Git Bash 窗口。在本地创建一个与 Repository 中博客项目同名的文件夹（如E:\username.github.io）在文件夹上点击鼠标右键，选择 Git bash here； 【提示】在进行博客搭建工作时，每次使用命令都要在E:\username.github.io目录下。 执行下面的指令，Hexo 就会自动在 E:\username.github.io文件夹建立独立博客所需要的所有文件啦！ 1$ hexo init 安装依赖包 1$ npm install 确保git部署 1$ npm install hexo-deployer-git --save 本地查看 现在已经搭建好本地的 Hexo 博客了，执行完下面的命令就可以到浏览器输入 localhost:4000 查看到啦 12$ hexo g$ hexo s hexo g 每次进行相应改动都要hexo g 生成一下 hexo s 启动服务预览 执行完 hexo init 命令后会给一个默认的主题：landscape 你可以到官网找你喜欢的主题进行下载 hexo themes 知乎：有哪些好看的 Hexo 主题？ 找到它所在的 Github Repository （怎么找，我喜欢的那个，恰好博主放了他的github地址） 找到之后通过git命令下载 在主题的repository点击clone 复制一下那个地址 1$ git clone https://github.com/theme-next/hexo-theme-next themes/next next主题使用及优化启用主题与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 启用 NexT 主题 1theme: next 到此，NexT 主题安装完成。下一步我们将验证主题是否正确启用。在切换主题之后、验证之前， 我们最好使用 hexo clean 来清除 Hexo 的缓存。 选择 SchemeScheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是： Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist - Muse 的紧凑版本，整洁有序的单栏外观 Pisces - 双栏 Scheme，小家碧玉似的清新 Scheme 的切换通过更改 主题配置文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除即可。 选择 Pisces Scheme 123#scheme: Muse#scheme: Mistscheme: Pisces 设置 语言编辑 站点配置文件， 将 language 设置成你所需要的语言。建议明确设置你所需要的语言，例如选用简体中文，配置如下： 1language: zh-CN Local Search添加百度/谷歌/本地 自定义站点内容搜索 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： 1$ npm install hexo-generator-searchdb --save 编辑 站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，启用本地搜索功能： 123# Local searchlocal_search: enable: true 文章模块的美化文章内代码美化 行内代码在主题目录下，将source/css/_custom/custom.styl文件修改如下： 123456789//行内代码样式code &#123; color: #ff7600; background: #fbf7f8; border: 1px solid #d6d6d6; padding:1px 4px; word-break: break-all; border-radius:4px;&#125; 区块代码在主题目录下，修改config.yml文件： 12# 样式可选： normal | night | night eighties | night blue | night brighthighlight_theme: night 文章结束语 添加模块文件 在主题目录下layout/_macro中新建 passage-end-tag.swig文件,并添加以下内容： 1234567&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt; -------------本文结束&lt;i class="fa fa-paw"&gt;&lt;/i&gt;感谢您的阅读------------- &lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 导入模板文件 在layout/_macro/post.swig文件中，找到： 123&#123;#####################&#125;&#123;### END POST BODY ###&#125;&#123;#####################&#125; 在上面代码之前添加： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 配置在主题配置文件中添加： 123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 增强文章底部版权信息 增加文章md文件的头部信息中添加copyright: true时，添加版权声明 增加文章标题、发布时间、更新时间等信息 在目录 next/layout/_macro/下添加 my-copyright.swig： 123456789101112131415161718192021222324252627282930&#123;% if page.copyright %&#125;&lt;div class="my_post_copyright"&gt; &lt;script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"&gt;&lt;/script&gt; &lt;script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"&gt;&lt;/script&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;"&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href="/" title="访问 &#123;&#123; theme.author &#125;&#125; 的个人博客"&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format("YYYY年MM月DD日 - HH:MM") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format("YYYY年MM月DD日 - HH:MM") &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;" title="&#123;&#123; page.title &#125;&#125;"&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class="copy-path" title="点击复制文章链接"&gt;&lt;i class="fa fa-clipboard" data-clipboard-text="&#123;&#123; page.permalink &#125;&#125;" aria-label="复制成功！"&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class="fa fa-creative-commons"&gt;&lt;/i&gt; &lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard('.fa-clipboard'); $(".fa-clipboard").click(function()&#123; clipboard.on('success', function()&#123; swal(&#123; title: "", text: '复制成功', icon: "success", showConfirmButton: true &#125;); &#125;); &#125;); &lt;/script&gt;&#123;% endif %&#125; 在目录next/source/css/_common/components/post/下添加my-post-copyright.styl： 123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改next/layout/_macro/post.swig，在代码 123&#123;#####################&#125;&#123;### END POST BODY ###&#125;&#123;#####################&#125; 之前添加增加如下代码： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'my-copyright.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码： 1@import "my-post-copyright" 保存重新生成即可。 参考 技术小白搭建hexo+github博客 next最新版主题下载使用 next主题官方文档 next主题个性化教程 next主题配置文件详解 NexT v6.0+ 背景动画Canvas_nest设置无效的解决方案 给Hexo搭建的博客增加百度谷歌搜索引擎验证 添加文章字数和读取文章的时间 hexo + next主题高级配置]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
</search>
